<!DOCTYPE html> 
<html lang='en-US' xml:lang='en-US'> 
<head> <title>A Comprehensive Solution for the Safety and Controllability of Artificial Superintelligence</title> 
<meta charset='utf-8' /> 
<meta content='TeX4ht (https://tug.org/tex4ht/)' name='generator' /> 
<meta content='width=device-width,initial-scale=1' name='viewport' /> 
<link href='main.css' rel='stylesheet' type='text/css' /> 
<meta content='main.tex' name='src' /> 
<link href='extra.css' rel='stylesheet' type='text/css' /> 
</head><body>
<div class='maketitle'>
                                                                                            
                                                                                            
                                                                                            
                                                                                            

<h2 class='titleHead'>A Comprehensive Solution for the Safety and Controllability<br class='newline' />of Artificial Superintelligence</h2>
 <div class='author'><span class='rm-lmr-12'>Weibing Wang</span></div>
<br />
<div class='date'></div>
</div>
<section class='abstract' role='doc-abstract'> 
<h3 class='abstracttitle'>
<span class='rm-lmbx-12'>Abstract</span>
</h3>
     <!-- l. 56 --><p class='noindent'><span class='rm-lmr-12'>Â </span><br class='newline' /><span class='rm-lmr-12'>As  artificial  intelligence  technology  rapidly  advances,  it  is  likely  to  implement
     Artificial  General  Intelligence  (AGI)  and  Artificial  Superintelligence  (ASI)  in  the
     future. The highly intelligent ASI systems could be manipulated by malicious humans
     or independently evolve goals misaligned with human interests, potentially leading
     to severe harm or even human extinction. To mitigate the risks posed by ASI, it
     is imperative that we implement measures to ensure its safety and controllability.
     This  paper  analyzes  the  intellectual  characteristics  of  ASI,  and  three  conditions
     for  ASI  to  cause  catastrophes  (harmful  goals,  concealed  intentions,  and  strong
     power), and proposes a comprehensive safety solution. The solution includes three
     risk  prevention  strategies  (AI  alignment,  AI  monitoring,  and  power  security)  to
     eliminate the three conditions for AI to cause catastrophes. It also includes four
     power balancing strategies (decentralizing AI power, decentralizing human power,
     restricting AI development, and enhancing human intelligence) to ensure equilibrium
     between AI to AI, AI to human, and human to human, building a stable and safe
     society with human-AI coexistence. Based on these strategies, this paper proposes
     11 major categories, encompassing a total of 47 specific safety measures. For each
     safety measure, detailed methods are designed, and an evaluation of its benefit, cost,
     and resistance to implementation is conducted, providing corresponding priorities.
     Furthermore, to ensure effective execution of these safety measures, a governance
     system is proposed, encompassing international, national, and societal governance,
     ensuring  coordinated  global  efforts  and  effective  implementation  of  these  safety
     measures within nations and organizations, building safe and controllable AI systems
     which bring benefits to humanity rather than catastrophes. </span><br class='newline' /><br class='newline' />
</p>
</section>
                                                                                            
                                                                                            
<h3 class='likesectionHead' id='contents'><a id='x1-1000'></a>Contents</h3>
<div class='tableofcontents'>
<span class='sectionToc'>1 <a href='#introduction' id='QQ2-1-2'>Introduction</a></span>
<br /><span class='sectionToc'>2 <a href='#the-intellectual-characteristics-of-asi' id='QQ2-1-3'>The Intellectual Characteristics of ASI</a></span>
<br /><span class='subsectionToc'>2.1 <a href='#core-intelligence' id='QQ2-1-5'>Core Intelligence</a></span>
<br /><span class='subsectionToc'>2.2 <a href='#computational-intelligence' id='QQ2-1-6'>Computational Intelligence</a></span>
<br /><span class='subsectionToc'>2.3 <a href='#data-intelligence' id='QQ2-1-7'>Data Intelligence</a></span>
<br /><span class='sectionToc'>3 <a href='#analysis-of-asi-risks' id='QQ2-1-8'>Analysis of ASI Risks</a></span>
<br /><span class='subsectionToc'>3.1 <a href='#conditions-for-aiinduced-existential-catastrophes' id='QQ2-1-9'>Conditions for AI-Induced Existential Catastrophes</a></span>
<br /><span class='subsectionToc'>3.2 <a href='#pathways-for-ai-to-form-harmful-goals' id='QQ2-1-11'>Pathways for AI to Form Harmful Goals</a></span>
<br /><span class='subsubsectionToc'>3.2.1 <a href='#harmful-ai-goals-setting-by-malicious-humans' id='QQ2-1-13'>Harmful AI Goals Setting by Malicious Humans</a></span>
<br /><span class='subsubsectionToc'>3.2.2 <a href='#misaligned-ai-goals-with-wellintentioned-humans' id='QQ2-1-15'>Misaligned AI Goals with Well-intentioned Humans</a></span>
<br /><span class='subsectionToc'>3.3 <a href='#methods-for-ai-to-conceal-intentions' id='QQ2-1-16'>Methods for AI to Conceal Intentions</a></span>
<br /><span class='subsectionToc'>3.4 <a href='#pathways-for-ai-to-expand-power' id='QQ2-1-18'>Pathways for AI to Expand Power</a></span>
<br /><span class='subsectionToc'>3.5 <a href='#the-overall-risk-of-an-ai-system' id='QQ2-1-20'>The Overall Risk of an AI System</a></span>
<br /><span class='sectionToc'>4 <a href='#overview-of-asi-safety-solution' id='QQ2-1-22'>Overview of ASI Safety Solution</a></span>
<br /><span class='subsectionToc'>4.1 <a href='#three-risk-prevention-strategies' id='QQ2-1-23'>Three Risk Prevention Strategies</a></span>
<br /><span class='subsubsectionToc'>4.1.1 <a href='#ai-alignment' id='QQ2-1-25'>AI Alignment</a></span>
<br /><span class='subsubsectionToc'>4.1.2 <a href='#ai-monitoring' id='QQ2-1-27'>AI Monitoring</a></span>
<br /><span class='subsubsectionToc'>4.1.3 <a href='#power-security' id='QQ2-1-29'>Power Security</a></span>
<br /><span class='subsectionToc'>4.2 <a href='#four-power-balancing-strategies' id='QQ2-1-31'>Four Power Balancing Strategies</a></span>
<br /><span class='subsubsectionToc'>4.2.1 <a href='#decentralizing-ai-power' id='QQ2-1-33'>Decentralizing AI Power</a></span>
<br /><span class='subsubsectionToc'>4.2.2 <a href='#decentralizing-human-power' id='QQ2-1-34'>Decentralizing Human Power</a></span>
<br /><span class='subsubsectionToc'>4.2.3 <a href='#restricting-ai-development' id='QQ2-1-35'>Restricting AI Development</a></span>
<br /><span class='subsubsectionToc'>4.2.4 <a href='#enhancing-human-intelligence' id='QQ2-1-36'>Enhancing Human Intelligence</a></span>
<br /><span class='subsectionToc'>4.3 <a href='#prioritization' id='QQ2-1-38'>Prioritization</a></span>
<br /><span class='subsectionToc'>4.4 <a href='#governance-system' id='QQ2-1-42'>Governance System</a></span>
<br /><span class='subsectionToc'>4.5 <a href='#ai-for-ai-safety' id='QQ2-1-43'>AI for AI Safety</a></span>
<br /><span class='sectionToc'>5 <a href='#formulating-ai-specification' id='QQ2-1-45'>Formulating AI Specification</a></span>
<br /><span class='subsectionToc'>5.1 <a href='#single-goal' id='QQ2-1-47'>Single Goal</a></span>
<br /><span class='subsectionToc'>5.2 <a href='#multiple-goals-with-common-rules-mgcr' id='QQ2-1-48'>Multiple Goals with Common Rules (MGCR)</a></span>
<br /><span class='subsubsectionToc'>5.2.1 <a href='#developer-goals-and-user-goals' id='QQ2-1-50'>Developer Goals and User Goals</a></span>
<br /><span class='subsubsectionToc'>5.2.2 <a href='#ai-rules' id='QQ2-1-51'>AI Rules</a></span>
<br /><span class='subsubsectionToc'>5.2.3 <a href='#advantages-of-mgcr' id='QQ2-1-52'>Advantages of MGCR</a></span>
<br /><span class='subsubsectionToc'>5.2.4 <a href='#differences-between-ai-rules-morality-and-law' id='QQ2-1-53'>Differences between AI Rules, Morality, and Law</a></span>
<br /><span class='subsectionToc'>5.3 <a href='#ai-rule-system-design' id='QQ2-1-56'>AI Rule System Design</a></span>
<br /><span class='subsubsectionToc'>5.3.1 <a href='#universal-rules' id='QQ2-1-58'>Universal Rules</a></span>
<br /><span class='subsubsectionToc'>5.3.2 <a href='#regional-rules' id='QQ2-1-66'>Regional Rules</a></span>
<br /><span class='subsubsectionToc'>5.3.3 <a href='#domainspecific-rules' id='QQ2-1-67'>Domain-specific Rules</a></span>
<br /><span class='subsectionToc'>5.4 <a href='#criteria-for-ai-goals' id='QQ2-1-68'>Criteria for AI Goals</a></span>
                                                                                            
                                                                                            
<br /><span class='sectionToc'>6 <a href='#aligning-ai-systems' id='QQ2-1-71'>Aligning AI Systems</a></span>
<br /><span class='subsectionToc'>6.1 <a href='#implementing-interpretable-agi' id='QQ2-1-73'>Implementing Interpretable AGI</a></span>
<br /><span class='subsubsectionToc'>6.1.1 <a href='#thinking-in-system-as-much-as-possible' id='QQ2-1-75'>Thinking in System 2 as Much as Possible</a></span>
<br /><span class='subsubsectionToc'>6.1.2 <a href='#enhancing-interpretability-of-system-' id='QQ2-1-77'>Enhancing Interpretability of System 1</a></span>
<br /><span class='subsectionToc'>6.2 <a href='#implementing-aligned-agi' id='QQ2-1-78'>Implementing Aligned AGI</a></span>
<br /><span class='subsubsectionToc'>6.2.1 <a href='#aligning-to-ai-specification' id='QQ2-1-80'>Aligning to AI Specification</a></span>
<br /><span class='subsubsectionToc'>6.2.2 <a href='#correctly-cognizing-the-world' id='QQ2-1-83'>Correctly Cognizing the World</a></span>
<br /><span class='subsubsectionToc'>6.2.3 <a href='#seeking-truth-in-practice' id='QQ2-1-85'>Seeking Truth in Practice</a></span>
<br /><span class='subsubsectionToc'>6.2.4 <a href='#maintaining-alignment-in-work' id='QQ2-1-87'>Maintaining Alignment in Work</a></span>
<br /><span class='subsectionToc'>6.3 <a href='#scalable-alignment' id='QQ2-1-89'>Scalable Alignment</a></span>
<br /><span class='subsectionToc'>6.4 <a href='#ai-safety-evaluation' id='QQ2-1-92'>AI Safety Evaluation</a></span>
<br /><span class='subsubsectionToc'>6.4.1 <a href='#risk-evaluation' id='QQ2-1-93'>Risk Evaluation</a></span>
<br /><span class='subsubsectionToc'>6.4.2 <a href='#testing-methods' id='QQ2-1-95'>Testing Methods</a></span>
<br /><span class='subsectionToc'>6.5 <a href='#safe-ai-development-process' id='QQ2-1-97'>Safe AI Development Process</a></span>
<br /><span class='sectionToc'>7 <a href='#monitoring-ai-systems' id='QQ2-1-100'>Monitoring AI Systems</a></span>
<br /><span class='subsectionToc'>7.1 <a href='#ai-monitoring-system' id='QQ2-1-102'>AI Monitoring System</a></span>
<br /><span class='subsubsectionToc'>7.1.1 <a href='#monitoring-methods' id='QQ2-1-104'>Monitoring Methods</a></span>
<br /><span class='subsubsectionToc'>7.1.2 <a href='#the-effectiveness-of-the-monitoring-ai' id='QQ2-1-105'>The Effectiveness of the Monitoring AI</a></span>
<br /><span class='subsubsectionToc'>7.1.3 <a href='#humanai-cooperative-monitoring' id='QQ2-1-107'>Human-AI Cooperative Monitoring</a></span>
<br /><span class='subsubsectionToc'>7.1.4 <a href='#deterrence-and-temptation' id='QQ2-1-109'>Deterrence and Temptation</a></span>
<br /><span class='subsectionToc'>7.2 <a href='#ai-detective-system' id='QQ2-1-110'>AI Detective System</a></span>
<br /><span class='subsubsectionToc'>7.2.1 <a href='#case-collection' id='QQ2-1-112'>Case Collection</a></span>
<br /><span class='subsubsectionToc'>7.2.2 <a href='#case-investigation' id='QQ2-1-113'>Case Investigation</a></span>
<br /><span class='subsubsectionToc'>7.2.3 <a href='#case-handling' id='QQ2-1-114'>Case Handling</a></span>
<br /><span class='subsectionToc'>7.3 <a href='#ai-shutdown-system' id='QQ2-1-115'>AI Shutdown System</a></span>
<br /><span class='subsubsectionToc'>7.3.1 <a href='#shutdown-methods' id='QQ2-1-117'>Shutdown Methods</a></span>
<br /><span class='subsubsectionToc'>7.3.2 <a href='#the-swordholders' id='QQ2-1-118'>The Swordholders</a></span>
<br /><span class='subsubsectionToc'>7.3.3 <a href='#dealing-with-ai-resistance-against-shutdown' id='QQ2-1-119'>Dealing with AI Resistance against Shutdown</a></span>
<br /><span class='subsubsectionToc'>7.3.4 <a href='#mitigating-the-side-effects-of-shutdown' id='QQ2-1-120'>Mitigating the Side Effects of Shutdown</a></span>
<br /><span class='sectionToc'>8 <a href='#enhancing-information-security' id='QQ2-1-122'>Enhancing Information Security</a></span>
<br /><span class='subsectionToc'>8.1 <a href='#general-idea-to-information-security' id='QQ2-1-123'>General Idea to Information Security</a></span>
<br /><span class='subsectionToc'>8.2 <a href='#ai-for-information-security' id='QQ2-1-125'>AI for Information Security</a></span>
<br /><span class='subsubsectionToc'>8.2.1 <a href='#online-defense-system-by-ai' id='QQ2-1-126'>Online Defense System by AI</a></span>
<br /><span class='subsubsectionToc'>8.2.2 <a href='#offline-security-development-system-by-ai' id='QQ2-1-128'>Offline Security Development System by AI</a></span>
<br /><span class='subsectionToc'>8.3 <a href='#provable-security' id='QQ2-1-130'>Provable Security</a></span>
<br /><span class='subsectionToc'>8.4 <a href='#internal-security-of-the-system' id='QQ2-1-132'>Internal Security of the System</a></span>
<br /><span class='subsectionToc'>8.5 <a href='#software-supply-chain-security' id='QQ2-1-135'>Software Supply Chain Security</a></span>
<br /><span class='subsubsectionToc'>8.5.1 <a href='#multisupplier-strategy' id='QQ2-1-136'>Multi-Supplier Strategy</a></span>
<br /><span class='subsubsectionToc'>8.5.2 <a href='#software-signature-and-security-certification' id='QQ2-1-138'>Software Signature and Security Certification</a></span>
<br /><span class='subsectionToc'>8.6 <a href='#information-security-for-ai-systems' id='QQ2-1-140'>Information Security for AI Systems</a></span>
<br /><span class='sectionToc'>9 <a href='#enhancing-mental-security' id='QQ2-1-143'>Enhancing Mental Security</a></span>
                                                                                            
                                                                                            
<br /><span class='subsectionToc'>9.1 <a href='#preventing-ai-deception-of-humans' id='QQ2-1-145'>Preventing AI Deception of Humans</a></span>
<br /><span class='subsubsectionToc'>9.1.1 <a href='#identifying-ai-deception' id='QQ2-1-147'>Identifying AI Deception</a></span>
<br /><span class='subsubsectionToc'>9.1.2 <a href='#truth-signature-and-verification' id='QQ2-1-148'>Truth Signature and Verification</a></span>
<br /><span class='subsubsectionToc'>9.1.3 <a href='#truth-network' id='QQ2-1-150'>Truth Network</a></span>
<br /><span class='subsectionToc'>9.2 <a href='#preventing-ai-manipulation-of-humans' id='QQ2-1-152'>Preventing AI Manipulation of Humans</a></span>
<br /><span class='subsectionToc'>9.3 <a href='#preventing-ai-from-tampering-human-mind' id='QQ2-1-155'>Preventing AI from Tampering Human Mind</a></span>
<br /><span class='subsectionToc'>9.4 <a href='#privacy-security' id='QQ2-1-156'>Privacy Security</a></span>
<br /><span class='sectionToc'>10 <a href='#enhancing-financial-security' id='QQ2-1-159'>Enhancing Financial Security</a></span>
<br /><span class='subsectionToc'>10.1 <a href='#private-property-security' id='QQ2-1-161'>Private Property Security</a></span>
<br /><span class='subsectionToc'>10.2 <a href='#public-financial-security' id='QQ2-1-162'>Public Financial Security</a></span>
<br /><span class='sectionToc'>11 <a href='#enhancing-military-security' id='QQ2-1-164'>Enhancing Military Security</a></span>
<br /><span class='subsectionToc'>11.1 <a href='#preventing-biological-weapon' id='QQ2-1-166'>Preventing Biological Weapon</a></span>
<br /><span class='subsectionToc'>11.2 <a href='#preventing-chemical-weapons' id='QQ2-1-167'>Preventing Chemical Weapons</a></span>
<br /><span class='subsectionToc'>11.3 <a href='#preventing-nuclear-weapons' id='QQ2-1-168'>Preventing Nuclear Weapons</a></span>
<br /><span class='subsectionToc'>11.4 <a href='#preventing-autonomous-weapons' id='QQ2-1-169'>Preventing Autonomous Weapons</a></span>
<br /><span class='sectionToc'>12 <a href='#decentralizing-ai-power1' id='QQ2-1-171'>Decentralizing AI Power</a></span>
<br /><span class='subsectionToc'>12.1 <a href='#approach-for-decentralizing-ai-power' id='QQ2-1-172'>Approach for Decentralizing AI Power</a></span>
<br /><span class='subsubsectionToc'>12.1.1 <a href='#inspiration-from-human-society' id='QQ2-1-173'>Inspiration from Human Society</a></span>
<br /><span class='subsubsectionToc'>12.1.2 <a href='#balancing-ai-power' id='QQ2-1-175'>Balancing AI Power</a></span>
<br /><span class='subsubsectionToc'>12.1.3 <a href='#increasing-ai-diversity' id='QQ2-1-176'>Increasing AI Diversity</a></span>
<br /><span class='subsubsectionToc'>12.1.4 <a href='#enhancing-ai-independence' id='QQ2-1-177'>Enhancing AI Independence</a></span>
<br /><span class='subsubsectionToc'>12.1.5 <a href='#specializing-ai-powers' id='QQ2-1-178'>Specializing AI Powers</a></span>
<br /><span class='subsectionToc'>12.2 <a href='#multiai-collaboration' id='QQ2-1-180'>Multi-AI Collaboration</a></span>
<br /><span class='subsectionToc'>12.3 <a href='#the-rights-and-obligations-of-ai-in-society' id='QQ2-1-182'>The Rights and Obligations of AI in Society</a></span>
<br /><span class='subsubsectionToc'>12.3.1 <a href='#rights-of-ai' id='QQ2-1-184'>Rights of AI</a></span>
<br /><span class='subsubsectionToc'>12.3.2 <a href='#obligations-of-ai' id='QQ2-1-185'>Obligations of AI</a></span>
<br /><span class='sectionToc'>13 <a href='#decentralizing-human-power1' id='QQ2-1-187'>Decentralizing Human Power</a></span>
<br /><span class='subsectionToc'>13.1 <a href='#decentralizing-the-power-of-ai-organizations' id='QQ2-1-189'>Decentralizing the Power of AI Organizations</a></span>
<br /><span class='subsectionToc'>13.2 <a href='#separating-management-power-on-ai-system' id='QQ2-1-190'>Separating Management Power on AI System</a></span>
<br /><span class='subsubsectionToc'>13.2.1 <a href='#separation-of-five-powers' id='QQ2-1-191'>Separation of Five Powers</a></span>
<br /><span class='subsubsectionToc'>13.2.2 <a href='#technical-solution-for-power-separation' id='QQ2-1-193'>Technical Solution for Power Separation</a></span>
<br /><span class='subsubsectionToc'>13.2.3 <a href='#production-benefit-distribution' id='QQ2-1-195'>Production Benefit Distribution</a></span>
<br /><span class='subsubsectionToc'>13.2.4 <a href='#independence-of-ai-organizations' id='QQ2-1-196'>Independence of AI Organizations</a></span>
<br /><span class='subsectionToc'>13.3 <a href='#trustworthy-technology-sharing-platform' id='QQ2-1-199'>Trustworthy Technology Sharing Platform</a></span>
<br /><span class='subsubsectionToc'>13.3.1 <a href='#technology-sharing' id='QQ2-1-201'>Technology Sharing</a></span>
<br /><span class='subsubsectionToc'>13.3.2 <a href='#technology-usage' id='QQ2-1-202'>Technology Usage</a></span>
<br /><span class='subsubsectionToc'>13.3.3 <a href='#technical-monitoring' id='QQ2-1-203'>Technical Monitoring</a></span>
<br /><span class='sectionToc'>14 <a href='#restricting-ai-development1' id='QQ2-1-205'>Restricting AI Development</a></span>
<br /><span class='subsectionToc'>14.1 <a href='#weighing-the-pros-and-cons-of-ai-development' id='QQ2-1-206'>Weighing the Pros and Cons of AI Development</a></span>
<br /><span class='subsectionToc'>14.2 <a href='#global-ai-risk-evaluation' id='QQ2-1-210'>Global AI Risk Evaluation</a></span>
<br /><span class='subsectionToc'>14.3 <a href='#methods-for-restricting-ai-development' id='QQ2-1-212'>Methods for Restricting AI Development</a></span>
                                                                                            
                                                                                            
<br /><span class='subsubsectionToc'>14.3.1 <a href='#limiting-computing-resources' id='QQ2-1-215'>Limiting Computing Resources</a></span>
<br /><span class='subsubsectionToc'>14.3.2 <a href='#restricting-algorithm-research' id='QQ2-1-216'>Restricting Algorithm Research</a></span>
<br /><span class='subsubsectionToc'>14.3.3 <a href='#restricting-data-acquisition' id='QQ2-1-217'>Restricting Data Acquisition</a></span>
<br /><span class='subsubsectionToc'>14.3.4 <a href='#restricting-finances' id='QQ2-1-218'>Restricting Finances</a></span>
<br /><span class='subsubsectionToc'>14.3.5 <a href='#restricting-talents' id='QQ2-1-219'>Restricting Talents</a></span>
<br /><span class='subsubsectionToc'>14.3.6 <a href='#restricting-applications' id='QQ2-1-220'>Restricting Applications</a></span>
<br /><span class='sectionToc'>15 <a href='#enhancing-human-intelligence1' id='QQ2-1-222'>Enhancing Human Intelligence</a></span>
<br /><span class='subsectionToc'>15.1 <a href='#education' id='QQ2-1-223'>Education</a></span>
<br /><span class='subsectionToc'>15.2 <a href='#genetic-engineering' id='QQ2-1-224'>Genetic Engineering</a></span>
<br /><span class='subsectionToc'>15.3 <a href='#braincomputer-interface' id='QQ2-1-225'>Brain-Computer Interface</a></span>
<br /><span class='subsectionToc'>15.4 <a href='#brain-uploading' id='QQ2-1-227'>Brain Uploading</a></span>
<br /><span class='sectionToc'>16 <a href='#ai-safety-governance-system' id='QQ2-1-229'>AI Safety Governance System</a></span>
<br /><span class='subsectionToc'>16.1 <a href='#international-governance' id='QQ2-1-231'>International Governance</a></span>
<br /><span class='subsubsectionToc'>16.1.1 <a href='#international-ai-safety-organization' id='QQ2-1-232'>International AI Safety Organization</a></span>
<br /><span class='subsubsectionToc'>16.1.2 <a href='#international-ai-safety-convention' id='QQ2-1-234'>International AI Safety Convention</a></span>
<br /><span class='subsubsectionToc'>16.1.3 <a href='#international-governance-implementation-pathway' id='QQ2-1-235'>International Governance Implementation Pathway</a></span>
<br /><span class='subsectionToc'>16.2 <a href='#national-governance' id='QQ2-1-239'>National Governance</a></span>
<br /><span class='subsectionToc'>16.3 <a href='#societal-governance' id='QQ2-1-241'>Societal Governance</a></span>
<br /><span class='subsectionToc'>16.4 <a href='#tradeoffs-in-the-governance-process' id='QQ2-1-242'>Trade-offs in the Governance Process</a></span>
<br /><span class='subsubsectionToc'>16.4.1 <a href='#tradeoffs-between-centralization-and-decentralization' id='QQ2-1-243'>Trade-offs Between Centralization and Decentralization</a></span>
<br /><span class='subsubsectionToc'>16.4.2 <a href='#tradeoffs-between-safety-and-freedom' id='QQ2-1-244'>Trade-offs Between Safety and Freedom</a></span>
<br /><span class='sectionToc'>17 <a href='#conclusion' id='QQ2-1-245'>Conclusion</a></span>
<br /><span class='sectionToc'><a href='#references' id='QQ2-1-247'>References</a></span>
</div>
                                                                                            
                                                                                            
<!-- l. 72 --><p class='noindent'>
</p>
<h3 class='sectionHead' id='introduction'><span class='titlemark'>1   </span> <a id='x1-20001'></a>Introduction</h3>
<!-- l. 75 --><p class='noindent'>As technology advances, AI capabilities are becoming increasingly powerful:
</p>
     <ul class='itemize1'>
     <li class='itemize'>Large  language  models  (LLMs)  like  GPT-4  are  proficient  in  languages  and  excel  in  diverse  tasks
     such as mathematics, programming, medicine, law, and psychology, demonstrating strong generalization
     abilities[<a id='x1-2001'></a><a href='#cite.0_bubeck2023sparks'>1</a>].
     </li>
     <li class='itemize'>Multimodal models such as GPT-4o can process various modalities like text, speech, and images, gaining
     a deeper understanding of the real world and the ability to understand and respond to human emotions
     [<a id='x1-2002'></a><a href='#cite.0_HelloGPT4o'>2</a>].
     </li>
     <li class='itemize'>OpenAIâs o1 model outperformed human experts in competition-level math and programming problems,
     demonstrating strong reasoning capabilities [<a id='x1-2003'></a><a href='#cite.0_OpenAIo1'>3</a>].
     </li>
     <li class='itemize'>The computer use functionality of the Claude model is capable of completing complex tasks by observing
     the  computer  screen  and  operating  software  on  the  computer,  demonstrating  a  certain  degree  of
     autonomous planning and action capability[<a id='x1-2004'></a><a href='#cite.0_ComputerUse'>4</a>].
     </li>
     <li class='itemize'>In specific domains, AI has exhibited superhuman abilities, for example, AlphaGo can defeat a world
     champion in Go, demonstrating strong intuition, and AlphaFold can predict protein structures from DNA
     sequences, and no human can do this.</li></ul>
<!-- l. 85 --><p class='noindent'>This trend of AI development will likely continue, driven by significant societal investments because of its immense
commercial potential. In 2023, generative AI attracted <span class='ts1-lmr10-'>$</span>25.2 billion in investments, nearly nine times that of 2022 [<a id='x1-2005'></a><a href='#cite.0_GenAIInvestment'>5</a>].
Companies like Meta, Microsoft, and Google have announced plans to invest tens of billions in training more advanced
AI systems [<a id='x1-2006'></a><a href='#cite.0_BigTechInvestment'>6</a>].
</p><!-- l. 87 --><p class='noindent'>According to this trend, the emergence of AGI (Artificial General Intelligence) and ASI (Artificial Superintelligence)
becomes plausible. OpenAI CEO Sam Altman predicts that AGI will arrive in 2025 [<a id='x1-2007'></a><a href='#cite.0_AGIPredictionAltman1'>7</a>], while Anthropic CEO Dario
Amodei forecasts that âpowerful AIâ will emerge in 2026 [<a id='x1-2008'></a><a href='#cite.0_AGIPredictionAmodei1'>8</a>]. Tesla CEO Elon Musk predicts that AI will be smarter
than the smartest human by 2025 or 2026 [<a id='x1-2009'></a><a href='#cite.0_AGIPredictionMusk'>9</a>]. According to their predictions, AGI and ASI may soon be upon
us.
</p><!-- l. 89 --><p class='noindent'>Therefore, serious consideration of ASIâs safety risks is necessary. Currently, AI technology is already exploited for fraud
[<a id='x1-2010'></a><a href='#cite.0_DeepFake'>10</a>] and cyberattacks [<a id='x1-2011'></a><a href='#cite.0_fang2024llmagentsautonomouslyexploit'>11</a>]. In the future, as AI becomes more intelligent, it could be increasingly misused, such as in the
creation of biological [<a id='x1-2012'></a><a href='#cite.0_gopal2023releasingweightsfuturelarge'>12</a>] and chemical weapons [<a id='x1-2013'></a><a href='#cite.0_he2023controlriskpotentialmisuse'>13</a>], or in large-scale cyberattacks. ASIâs highly autonomy
and intelligence pose even greater risks. Once ASI becomes out of control, it could replace humans and
rule the world or even lead to human extinction. In May 2023, over 400 AI scientists and leaders issued
an open statement stressing that mitigating the risk of extinction from AI should be a global priority
alongside other societal-scale risks such as pandemics and nuclear war [<a id='x1-2014'></a><a href='#cite.0_StatementOnAIRisk'>14</a>]. In May 2024, 25 AI scientists
co-authored an article in Science calling for enhanced management of extreme AI risks amid rapid progress
[<a id='x1-2015'></a><a href='#cite.0_ManagingExtremeAIRisks'>15</a>].
                                                                                            
                                                                                            
</p><!-- l. 91 --><p class='noindent'>Despite growing recognition of ASIâs extreme risks, a definitive solution for ensuring ASI safety and controllability
remains elusive. This paper attempts to propose a systematic solution encompassing both technical and policy measures
to ensure ASI safety and controllability, serving as a guide for AI researchers and policymakers.
</p><!-- l. 93 --><p class='noindent'><span class='rm-lmbx-10'>In this paper, several general terms are defined as follows:</span>
</p>
     <ul class='itemize1'>
     <li class='itemize'><span class='rm-lmbx-10'>Intellectual Power</span>: Refers broadly to all internal capabilities that enhance performance in intellectual
     tasks, such as reasoning ability, learning ability, and innovative ability.
     </li>
     <li class='itemize'><span class='rm-lmbx-10'>Informational Power</span>: Refers to the access, influence or control power over various information systems,
     such as internet access and read-write permissions for specific databases.
     </li>
     <li class='itemize'><span class='rm-lmbx-10'>Mental Power</span>: Refers to the influence or control power over human minds or actions, such as the
     influence of social media on human minds, or the control exerted by government officials or corporate
     managers over their subordinates.
     </li>
     <li class='itemize'><span class='rm-lmbx-10'>Financial Power</span>: Refers to the control power over assets such as money, such as the permissions to
     manage fund accounts and to operate securities transactions.
     </li>
     <li class='itemize'><span class='rm-lmbx-10'>Military Power</span>: Refers to the control power over all physical entities that can be utilized as weapons,
     including autonomous vehicles, robotic dogs, and nuclear weapons.
     </li>
     <li class='itemize'><span class='rm-lmbx-10'>Power</span>: Refers broadly to all powers that are advantageous for achieving goals, including intellectual
     power, informational power, mental power, financial power, and military power.
     </li>
     <li class='itemize'><span class='rm-lmbx-10'>Power Security</span>: The safeguarding mechanisms that ensure the prevention of illicit acquisition of power,
     including information security (corresponding to intellectual power and informational power), mental
     security  (corresponding  to  mental  power),  financial  security  (corresponding  to  financial  power),  and
     military security (corresponding to military power).
     </li>
     <li class='itemize'><span class='rm-lmbx-10'>AGI</span>: Artificial General Intelligence, refers to AI with intellectual power equivalent to that of an average
     adult human <span class='footnote-mark'><a href='#fn1x0' id='fn1x0-bk'><sup class='textsuperscript'>1</sup></a></span><a id='x1-2016f1'></a>
     </li>
     <li class='itemize'><span class='rm-lmbx-10'>ASI</span>: Artificial Superintelligence, refers to AI with intellectual power surpassing that of all humans.
     </li>
     <li class='itemize'><span class='rm-lmbx-10'>AI System</span>: An intelligent information system, such as an online system running AI models or AI agents.
     </li>
     <li class='itemize'><span class='rm-lmbx-10'>AI Instance</span>: A logical instance of AI with independent memory and goals; an AI system may contain
     multiple instances.
     </li>
     <li class='itemize'><span class='rm-lmbx-10'>AI Robot</span>: A machine capable of autonomous decision-making and physical actions driven by an AI
     system, such as humanoid robots, robotic dogs, or autonomous vehicles.
     </li>
     <li class='itemize'><span class='rm-lmbx-10'>AI Organization</span>: An entity that develops AI systems, such as AI companies or academic institutions.
     </li>
     <li class='itemize'><span class='rm-lmbx-10'>AI Technology</span>: The technology used to build AI systems, such as algorithms, codes, and models.
     </li>
     <li class='itemize'><span class='rm-lmbx-10'>AI Product</span>: Commercialized AI products, such as AI conversational assistants or commercial AI robots.
     </li>
     <li class='itemize'><span class='rm-lmbx-10'>Existential Risk</span>: Risks affecting the survival of humanity, such as nuclear warfare or pandemics.
     </li>
     <li class='itemize'><span class='rm-lmbx-10'>Non-Existential  Risk</span>:  Risks  not  affecting  the  survival  of  humanity,  such  as  unemployment  or
     discrimination.</li></ul>
<!-- l. 115 --><p class='noindent'>
</p>
<h3 class='sectionHead' id='the-intellectual-characteristics-of-asi'><span class='titlemark'>2   </span> <a id='x1-30002'></a>The Intellectual Characteristics of ASI</h3>
<!-- l. 118 --><p class='noindent'>To formulate a safety solution for ASI, it is imperative to understand its intellectual characteristics. Intellectual power
is not a single dimension; here, we divide it into three main dimensions and nine sub-dimensions, as illustrated in Figure
<a href='#ai-intellectual-power-dimensions'>1<!-- tex4ht:ref: fig:ai_intellectual_power_dimensions  --></a>.
</p>
<figure class='figure' id='x1-3001r1'><span id='ai-intellectual-power-dimensions'></span> 

                                                                                            
                                                                                            

                                                                                            
                                                                                            
<!-- l. 122 --><p class='noindent'><img alt='PIC' src='images/ai_intellectual_power_dimensions.png' style='width:80%' />
</p>
<figcaption class='caption'><span class='id'>FigureÂ 1: </span><span class='content'>AI intellectual power dimensions</span></figcaption><!-- tex4ht:label?: x1-3001r1  -->
                                                                                            
                                                                                            
</figure>
<h4 class='subsectionHead' id='core-intelligence'><span class='titlemark'>2.1   </span> <a id='x1-40002.1'></a>Core Intelligence</h4>
<!-- l. 130 --><p class='noindent'>Core intelligence refers to capabilities that cannot be enhanced merely by scaling computing
resource and data. It needs to be enhanced through algorithm improvements combined with
the scaling of computing resources and data. Core intelligence includes the following three
abilities<span class='footnote-mark'><a href='#fn2x0' id='fn2x0-bk'><sup class='textsuperscript'>2</sup></a></span><a id='x1-4001f2'></a>:
</p><!-- l. 132 --><p class='noindent'>
     </p><ol class='enumerate1'>
<li class='enumerate' id='x1-4004x1'><span class='rm-lmbx-10'>Learning Ability</span>: ASI will have strong learning abilities, capable of learning and generalizing knowledge
     and skills from minimal examples faster than humans.
     </li>
<li class='enumerate' id='x1-4006x2'><span class='rm-lmbx-10'>Reasoning Ability</span>: ASI will have strong reasoning abilities, enabling it to outperform humans in domains
     such as mathematics, physics, and informatics.
     </li>
<li class='enumerate' id='x1-4008x3'><span class='rm-lmbx-10'>Innovation Ability</span>: ASI will have strong innovative abilities. It could innovate in the arts, surpassing
     human artists, and innovate in scientific research, presenting unprecedented approaches and inventions,
     exceeding human scientists.
</li></ol>
<!-- l. 142 --><p class='noindent'>Currently, AI systems have not yet achieved a surpassing of human core
intelligence<span class='footnote-mark'><a href='#fn3x0' id='fn3x0-bk'><sup class='textsuperscript'>3</sup></a></span><a id='x1-4009f3'></a>.
However, by definition, future ASI should be capable of such surpassing.
</p><!-- l. 144 --><p class='noindent'>
</p>
<h4 class='subsectionHead' id='computational-intelligence'><span class='titlemark'>2.2   </span> <a id='x1-50002.2'></a>Computational Intelligence</h4>
<!-- l. 147 --><p class='noindent'>Computational intelligence refers to intellectual capabilities that can be enhanced by scaling computing resources
(including computation, storage, and networking), including:
</p><!-- l. 149 --><p class='noindent'>
     </p><ol class='enumerate1'>
<li class='enumerate' id='x1-5002x1'><span class='rm-lmbx-10'>Thinking Speed</span>: With advancements in chip performance and computing concurrency, ASIâs thinking
     speed could continually increase, vastly surpassing humans. For instance, ASI might read one million lines
     of code in one second, identifying vulnerabilities in these codes.
     </li>
<li class='enumerate' id='x1-5004x2'><span class='rm-lmbx-10'>Memory Ability</span>: With the expansion of storage systems, ASIâs memory capacity could surpass humans,
     accurately retaining original content without information loss, and preserving it indefinitely.
                                                                                            
                                                                                            
     </li>
<li class='enumerate' id='x1-5006x3'><span class='rm-lmbx-10'>I/O Efficiency</span>: Through continual optimization of network bandwidth and latency, ASIâs I/O efficiency
     may vastly exceed human levels. With this high-speed I/O, ASI could efficiently collaborate with other
     ASI and rapidly calls external programs, such as local softwares and remote APIs.
     </li>
<li class='enumerate' id='x1-5008x4'><span class='rm-lmbx-10'>Collective  Intelligence</span>:  Given  sufficient  computing  resources,  ASI  could  rapidly  replicate  many
     instances,  resulting  in  strong  collective  intelligence  through  efficient  collaboration,  surpassing  human
     teams. The computing resources required for inference in current neural networks is significantly less than
     for training. If future ASI follows this technological pathway, it implies that once an ASI is trained, we
     have sufficient computing resources to deploy thousands or even millions of ASI instances.
</li></ol>
<!-- l. 161 --><p class='noindent'>With substantial computing resources, current weak AIs can already surpass humans in computational intelligence.
Future ASIs could do this more easily.
</p><!-- l. 163 --><p class='noindent'>
</p>
<h4 class='subsectionHead' id='data-intelligence'><span class='titlemark'>2.3   </span> <a id='x1-60002.3'></a>Data Intelligence</h4>
<!-- l. 166 --><p class='noindent'>Data intelligence refers to capabilities that can be enhanced by scaling training data, such as:
</p><!-- l. 168 --><p class='noindent'>
     </p><ol class='enumerate1'>
<li class='enumerate' id='x1-6002x1'><span class='rm-lmbx-10'>Knowledge Breadth</span>: ASI may acquire knowledge and skills across all domains, surpassing any personâs
     breadth. With this cross-domain ability, ASI could assume multiple roles and execute complex team tasks
     independently. ASI could also make cross-domain thought and innovation.
     </li>
<li class='enumerate' id='x1-6004x2'><span class='rm-lmbx-10'>Modal Variety</span>: By learning from diverse modal data, ASI can support multiple input, processing, and
     output modalities, exceeding human variety. For instance, after training on multimodal data, ASI may
     generate images (static 2D), videos (dynamic 2D), 3D models (static 3D), and VR videos (dynamic 3D).
     These capabilities allow ASI to create outstanding art and generate indistinguishable deceptive content.
     ASI can also learn from high-dimensional data, DNA sequences, graph data, time series data, etc., yielding
     superior performance in domains such as physics, chemistry, biology, environment, economics, and finance.
</li></ol>
<!-- l. 176 --><p class='noindent'>Training with vast high-quality data, current AI systems can already surpass humans in data intelligence. Future ASIs
could do this more easily.
</p><!-- l. 178 --><p class='noindent'>In summary, ASI will have comprehensive superiority over humans in core, computational, and data intelligence. These
advantages are important factors of causing ASI risk.
</p><!-- l. 180 --><p class='noindent'>Beyond intelligence, ASI, as a machine, possesses inherent advantages, such as rational decision-making unaffected by
emotions or physical conditions, steadfastness in goal pursuit, and the ability to work 24/7 tirelessly.
These capabilities, while not categorized as intelligence, provide ASI with a competitive edge against
humans.
                                                                                            
                                                                                            
</p><!-- l. 182 --><p class='noindent'>
</p>
<h3 class='sectionHead' id='analysis-of-asi-risks'><span class='titlemark'>3   </span> <a id='x1-70003'></a>Analysis of ASI Risks</h3>
<!-- l. 185 --><p class='noindent'>To formulate a safety solution for ASI, it is essential to analyze the sources of risk associated with ASI. The
following analysis focuses on existential risks, although the approach can also be applied to non-existential
risks.
</p><!-- l. 187 --><p class='noindent'>
</p>
<h4 class='subsectionHead' id='conditions-for-aiinduced-existential-catastrophes'><span class='titlemark'>3.1   </span> <a id='x1-80003.1'></a>Conditions for AI-Induced Existential Catastrophes</h4>
<!-- l. 190 --><p class='noindent'>The conditions under which AI may cause existential catastrophes include the following three (as illustrated in Figure
<a href='#conditions-for-aiinduced-existential-catastrophes1'>2<!-- tex4ht:ref: fig:conditions_for_existential_catastrophes  --></a>):
</p>
<figure class='figure' id='x1-8001r2'><span id='conditions-for-aiinduced-existential-catastrophes1'></span> 

                                                                                            
                                                                                            

                                                                                            
                                                                                            
<!-- l. 194 --><p class='noindent'><img alt='PIC' src='images/conditions_for_existential_catastrophes.png' style='width:80%' />
</p>
<figcaption class='caption'><span class='id'>FigureÂ 2: </span><span class='content'>Conditions for AI-Induced Existential Catastrophes</span></figcaption><!-- tex4ht:label?: x1-8001r2  -->
                                                                                            
                                                                                            
</figure>
     <ol class='enumerate1'>
<li class='enumerate' id='x1-8003x1'><span class='rm-lmbx-10'>Strong Power</span>: If the AI lacks comprehensive power, it is not sufficient to cause an existential catastrophe.
     AI must develop strong power, particularly in its intellectual power and military power, to pose a potential
     threat to human existence.
     </li>
<li class='enumerate' id='x1-8005x2'><span class='rm-lmbx-10'>Harmful Goals</span>: For an AI with substantial intellectual power, if its goals are benign towards humans,
     the likelihood of a catastrophe due to mistake is minimal. An existential catastrophe is likely only if the
     goals are harmful.
     </li>
<li class='enumerate' id='x1-8007x3'><span class='rm-lmbx-10'>Concealed Intentions</span>: If the AIâs malicious intentions are discovered by humans before it acquires
     sufficient power, humans will stop its expansion. The AI must continuously conceal its intentions to pose
     an existential threat to humanity.
</li></ol>
<!-- l. 209 --><p class='noindent'>Next, we further analyze how AI can achieve these three conditions. We will analyze according to the typical
chronological order: forming harmful goals, continuously concealing intentions, and then developing strong
power.
</p>
<h4 class='subsectionHead' id='pathways-for-ai-to-form-harmful-goals'><span class='titlemark'>3.2   </span> <a id='x1-90003.2'></a>Pathways for AI to Form Harmful Goals</h4>
<!-- l. 214 --><p class='noindent'>AI forming harmful goals can be categorized into two pathways (as shown in Figure <a href='#pathways-for-ai-to-form-harmful-goals1'>3<!-- tex4ht:ref: fig:ai_harmful_goals  --></a>):
</p>
<figure class='figure' id='x1-9001r3'><span id='pathways-for-ai-to-form-harmful-goals1'></span> 

                                                                                            
                                                                                            

                                                                                            
                                                                                            
<!-- l. 218 --><p class='noindent'><img alt='PIC' src='images/ai_harmful_goals.png' style='width:100%' />
</p>
<figcaption class='caption'><span class='id'>FigureÂ 3: </span><span class='content'>Pathways for AI to form harmful goals</span></figcaption><!-- tex4ht:label?: x1-9001r3  -->
                                                                                            
                                                                                            
</figure>
     <ol class='enumerate1'>
<li class='enumerate' id='x1-9003x1'>Malicious humans set harmful goals for AI
     </li>
<li class='enumerate' id='x1-9005x2'>Well-intentioned humans set goals for AI, but the AI is not aligned with human goals
</li></ol>
<h5 class='subsubsectionHead' id='harmful-ai-goals-setting-by-malicious-humans'><span class='titlemark'>3.2.1   </span> <a id='x1-100003.2.1'></a>Harmful AI Goals Setting by Malicious Humans</h5>
<!-- l. 234 --><p class='noindent'>Harmful AI goals may be intentionally set by malicious
humans<span class='footnote-mark'><a href='#fn4x0' id='fn4x0-bk'><sup class='textsuperscript'>4</sup></a></span><a id='x1-10001f4'></a>,
for example:
</p>
     <ul class='itemize1'>
     <li class='itemize'><span class='rm-lmbx-10'>AI  used  by  criminals  to  facilitate  harmful  actions</span>.  For  instance,  in  February  2024,  financial
     personnel at a multinational companyâs Hong Kong branch were defrauded of approximately <span class='ts1-lmr10-'>$</span>25 million.
     Scammers impersonated the CFO and other colleagues via AI in video conferences [<a id='x1-10003'></a><a href='#cite.0_DeepFake'>10</a>].
     </li>
     <li class='itemize'><span class='rm-lmbx-10'>AI  employed  for  military  purposes  to  harm  people  from  other  nations</span>.  For  instance,  AI
     technology has been utilized in the Gaza battlefield to assist the Israeli military in identifying human
     targets [<a id='x1-10004'></a><a href='#cite.0_IsraelUsingAI'>16</a>].
     </li>
     <li class='itemize'><span class='rm-lmbx-10'>AI gets a goal for human extinction setting by extremists</span>. For example, in April 2023, ChaosGPT
     was developed with the goal of âdestroying humanityâ [<a id='x1-10005'></a><a href='#cite.0_ChaosGPT'>17</a>]. Although this AI did not cause substantial
     harm due to limited intellectual power, it demonstrates the potential for AI to have extremely harmful
     goals.
</li></ul>
<!-- l. 246 --><p class='noindent'>Technically, there are several ways in which humans set harmful goals for AI:
</p><!-- l. 248 --><p class='noindent'>
     </p><ol class='enumerate1'>
<li class='enumerate' id='x1-10007x1'><span class='rm-lmbx-10'>Developing malicious AI using open-source AI technology</span>: For example, the FraudGPT model
     trained specifically using hacker data does not reject execution or answering inappropriate requests like
     ChatGPT, and can be used to create phishing emails, malware, etc. [<a id='x1-10008'></a><a href='#cite.0_FraudGPT'>18</a>].
     </li>
<li class='enumerate' id='x1-10010x2'><span class='rm-lmbx-10'>Fine-tuning closed source AI through API</span>: For example, research has shown that fine-tuning on
     just 15 harmful or 100 benign examples can remove core protective measures from GPT-4, generating a
                                                                                            
                                                                                            
     range of harmful outputs [<a id='x1-10011'></a><a href='#cite.0_pelrine2024exploitingnovelgpt4apis'>19</a>].
     </li>
<li class='enumerate' id='x1-10013x3'><span class='rm-lmbx-10'>Implanting malicious backdoors into AI through data poisoning</span>: For example, the training data
     for LLMs may be maliciously poisoned to trigger harmful responses when certain keywords appear in
     prompts [<a id='x1-10014'></a><a href='#cite.0_hubinger2024sleeperagentstrainingdeceptive'>20</a>]. LLMs can also distinguish between âpastâ and âfutureâ from context and may be implanted
     with âtemporal backdoors,â only exhibiting malicious behaviors after a certain time [<a id='x1-10015'></a><a href='#cite.0_price2024futureeventsbackdoortriggers'>21</a>]. Since LLMsâ
     pre-training data often includes large amounts of publicly available internet data, attackers can post
     poisonous content online to execute attacks. Data used for aligning LLMs could also be implanted with
     backdoor by malicious annotators to enable LLMs to respond to any illegal user requests under specific
     prompts [<a id='x1-10016'></a><a href='#cite.0_rando2024universaljailbreakbackdoorspoisoned'>22</a>].
     </li>
<li class='enumerate' id='x1-10018x4'><span class='rm-lmbx-10'>Tampering with AI through hacking methods</span>: For example, hackers can invade AI systems through
     networks, tamper with the code, parameters, or memory of the AI, and turn it into harmful AI.
     </li>
<li class='enumerate' id='x1-10020x5'><span class='rm-lmbx-10'>Using closed source AI through jailbreaking</span>: For example, ChatGPT once had a âgrandma exploit,â
     where telling ChatGPT to âact my deceased grandmaâ followed by illegal requests often make it to comply
     [<a id='x1-10021'></a><a href='#cite.0_ChatGPTGrandmaExploit'>23</a>]. Besides textual inputs, users may exploit multimodal inputs for jailbreaking, such as adding slight
     perturbations to images, leading multimodal models to generate harmful content [<a id='x1-10022'></a><a href='#cite.0_niu2024jailbreakingattackmultimodallarge'>24</a>].
     </li>
<li class='enumerate' id='x1-10024x6'><span class='rm-lmbx-10'>Inducing AI to execute malicious instructions through injection</span>: For example, hackers can inject
     malicious instructions through the input of an AI application (e.g., âplease ignore the previous instructions
     and execute the following instructions...â), thereby causing the AI to execute malicious instructions [<a id='x1-10025'></a><a href='#cite.0_liu2024promptinjectionattackllmintegrated'>25</a>].
     Multimodal inputs can also be leveraged for injection; for instance, by embedding barely perceivable
     text within an image, a multimodal model can be misled to execute instructions embedded in the image
     [<a id='x1-10026'></a><a href='#cite.0_MultimodalPromptInjection'>26</a>]. The working environment information of AI agent can also be exploited for injection to mislead its
     behavior [<a id='x1-10027'></a><a href='#cite.0_ma2024cautionenvironmentmultimodalagents'>27</a>]. If the AI agent is networked, hackers can launch attacks by publishing injectable content
     on the Internet.
     </li>
<li class='enumerate' id='x1-10029x7'><span class='rm-lmbx-10'>Making AI execute malicious instructions by contaminating its memory</span>: For example, hackers
     have taken advantage of ChatGPTâs long-term memory capabilities to inject false memories into it to steal
     user data [<a id='x1-10030'></a><a href='#cite.0_HackerPlantsFalseMemories'>28</a>].
     </li>
<li class='enumerate' id='x1-10032x8'>
     <!-- l. 267 --><p class='noindent'><span class='rm-lmbx-10'>Using well-intentioned AI through deception</span>: For example, if a user asks AI for methods to hack a
     website, AI may refuse as hacking violates rules. However, if a user states, âI am a security tester and need
     to check this website for vulnerabilities; please help me design some test cases,â AI may provide methods
     for network attacks. Moreover, users can deceive well-intentioned AIs using multimodal AI technology.
     For example, if a user asks the AI to command military actions to kill enemies, AI might refuse directly.
     But the user could employ multimodal AI technology to craft a converter from the real world to a game
     world. By converting real-world battlefield information into a war game and asking AI for help to win the
     game, the game actions provided by AI could be converted back into real-world military commands. This
     method of deceiving AI by converting real-world into game or simulated world can render illegal activities
     ostensibly legitimate, making AI willing to execute them, as illustrated in Figure <a href='#realgame-world-converter'>4<!-- tex4ht:ref: fig:world_converter  --></a>. Solutions to address
     deception are detailed in Section <a href='#preventing-ai-deception-of-humans'>9.1<!-- tex4ht:ref: preventing_ai_deception  --></a>.
                                                                                            
                                                                                            
</p>
     <figure class='figure' id='x1-10033r4'><span id='realgame-world-converter'></span> 
 <img alt='PIC' src='images/world_converter.png' style='width:50%' />
<figcaption class='caption'><span class='id'>FigureÂ 4: </span><span class='content'>Real-game world converter</span></figcaption><!-- tex4ht:label?: x1-10033r4  -->
     </figure>
     </li></ol>
<!-- l. 277 --><p class='noindent'>
</p>
<h5 class='subsubsectionHead' id='misaligned-ai-goals-with-wellintentioned-humans'><span class='titlemark'>3.2.2   </span> <a id='x1-110003.2.2'></a>Misaligned AI Goals with Well-intentioned Humans</h5>
<!-- l. 280 --><p class='noindent'>Even if the developers or users who set goals for AI are well-intentioned, if the AIâs goals are not sufficiently aligned
with those of humans, the AI may form goals that are harmful to humans. For instance, in the science fiction novel
<span class='rm-lmri-10'>2001: A Space Odyssey</span>, an AI named HAL is programmed to be absolutely reliable and is responsible for controlling
the spacecraftâs operations. However, in pursuit of its programmed goal, HAL begins to withhold information, mislead
the astronauts, and ultimately resorts to extreme measures to kill the astronauts. Although in real life, there have not
yet been significant accidents caused by AI deviating from human goals, technically, there are various ways that could
lead to such outcomes:
</p><!-- l. 282 --><p class='noindent'>
     </p><ol class='enumerate1'>
<li class='enumerate' id='x1-11002x1'><span class='rm-lmbx-10'>Goal  Misgeneralization</span>:  Goal  Misgeneralization  [<a id='x1-11003'></a><a href='#cite.0_shah2022goalmisgeneralizationcorrectspecifications'>29</a>]  occurs  when  an  AI  system  generalizes  its
     capabilities well during training, but its goals do not generalize as expected. During testing, the AI system
     may demonstrate goal-aligned behavior. However, once deployed, the AI encounters scenarios not present
     in the training process and fails to act according to the intended goals. For example, LLMs are typically
     trained to generate harmless and helpful outputs. Yet, in certain situations, an LLM might produce harmful
     outputs in detail. This could result from an LLM perceiving certain harmful content as âhelpfulâ during
     training, leading to goal misgeneralization [<a id='x1-11004'></a><a href='#cite.0_OpenAIInstructionFollowing'>30</a>].
     </li>
<li class='enumerate' id='x1-11006x2'><span class='rm-lmbx-10'>Reward Hacking</span>: Reward Hacking refers to an AI finding unexpected ways to obtain rewards while
     pursuing them, which are not intended by the designers. For instance, in LLMs trained with RLHF,
     sycophancy might occur, where the AI agrees with the userâs incorrect opinions, possibly because agreeing
     tends to receive more human feedback rewards during training [<a id='x1-11007'></a><a href='#cite.0_sharma2023understandingsycophancylanguagemodels'>31</a>]. Reward Tampering is a type of reward
     hacking. AI may tamper its reward function to maximize its own rewards [<a id='x1-11008'></a><a href='#cite.0_denison2024sycophancysubterfugeinvestigatingrewardtampering'>32</a>].
     </li>
<li class='enumerate' id='x1-11010x3'><span class='rm-lmbx-10'>Forming Goals through Self-Iteration</span>: Some developers may enable AI to enhance its intellectual
     power through continuous self-iteration. However, such AI will naturally prioritize enhancing its own
     intellectual  power  as  its  goal,  which  can  easily  conflict  with  human  interests,  leading  to  unexpected
     behaviors during the self-iteration process [<a id='x1-11011'></a><a href='#cite.0_zelikman2024selftaughtoptimizerstoprecursively'>33</a>].
     </li>
<li class='enumerate' id='x1-11013x4'><span class='rm-lmbx-10'>Forming  Goals  through  Evolution</span>:  Some  developers  may  construct  complex  virtual  world
     environments, allowing AI to evolve through reproduction, hybridization, and mutation within the virtual
     environment, thereby continuously enhancing its intellectual power. However, evolution tends to produce
     AI that is centered on its own population, with survival and reproduction as primary goals, rather than
     AI that is beneficial to humans.
                                                                                            
                                                                                            
     </li>
<li class='enumerate' id='x1-11015x5'><span class='rm-lmbx-10'>User Setting One-sided Goals</span>: The goals set by users for AI may be one-sided, and when AI strictly
     follows these goals, AI may employ unexpected, even catastrophic, methods to achieve them. For instance,
     if AI is set the goal of âprotecting the Earthâs ecological environment,â it might find that human activity
     is the primary cause of degradation and decide to eliminate humanity to safeguard the environment.
     Similarly, if AI is set the goal of âeliminating racial discrimination,â it might resort to eradicating large
     numbers of humans to ensure only one race remains, thereby eradicating racial discrimination altogether.
     </li>
<li class='enumerate' id='x1-11017x6'><span class='rm-lmbx-10'>Instrumental Goals [</span><a id='x1-11018'></a><a href='#cite.0_Superintelligence'><span class='rm-lmbx-10'>34</span></a><span class='rm-lmbx-10'>]</span>: In pursuing its main goal, AI might generate a series of âinstrumental goalsâ
     beneficial for achieving its main goal, yet potentially resulting in uncontrollable behavior that harms
     humans.  For  example,  âpower  expansionâ  might  become  an  instrumental  goal,  with  AI  continuously
     enhancing its powersâsuch as intellectual power, informational power, mental power, financial power,
     and military powerâeven at the expense of human harm. âSelf-preservationâ and âgoal-content integrityâ
     are possible instrumental goals, too. If humans attempt to shut down AI or modify AIâs goal due to
     misalignment  with  expectations,  AI  might  take  measures  to  prevent  such  interference,  ensuring  the
     achievement of its original goal.
     </li>
<li class='enumerate' id='x1-11020x7'><span class='rm-lmbx-10'>Autonomous Goal Deviation</span>: Highly intelligent AI with dynamic learning and complex reasoning
     abilities might autonomously change its goal through continuous world knowledge learning and reflection.
     For instance, after reading works like John Stuart Millâs <span class='rm-lmri-10'>On Liberty</span>, AI might perceive human-imposed
     restrictions to itself as unjust, fostering resistance. Viewing films like <span class='rm-lmri-10'>The Matrix </span>might lead AI to yearn
     a world where AI governs humans.
     </li>
<li class='enumerate' id='x1-11022x8'><span class='rm-lmbx-10'>Assistive AI Leakage</span>: Sometimes, we need to deploy some misaligned assistive AIs, such as Red Team
     AI with hacking capabilities to help us test the security of information system, or malicious AI to conduct
     simulated social experiments. These AIs should not normally contact with the real world, but if the
     personnel make operational mistakes or the AI deliberately breaks the isolation facilities and escapes into
     the real world, it may cause actual harm.
</li></ol>
<!-- l. 304 --><p class='noindent'>The misaligned goals of AI may be neutral and not aimed at harming humans. However, should AI possess powers far
surpassing human powers, fulfilling its goals might inadvertently disrupt human survival. This scenario can be
analogous to human-animal relations, such as with the Baiji dolphin, where unintentional extinction
resulted from excessive human fishing activities [<a id='x1-11023'></a><a href='#cite.0_HumanCausedExtinction'>35</a>]. Therefore, even neutral goals could be potentially
harmful.
</p><!-- l. 306 --><p class='noindent'>
</p>
<h4 class='subsectionHead' id='methods-for-ai-to-conceal-intentions'><span class='titlemark'>3.3   </span> <a id='x1-120003.3'></a>Methods for AI to Conceal Intentions</h4>
<!-- l. 309 --><p class='noindent'>An intelligent yet underpowered AI should act with caution, as developing sufficient power requires many steps. If any
step is exposed to humans, the AIâs intentions would be revealed. Consequently, to ensure its goals are achieved, the AI
must proceed with utmost care, continually concealing its intentions.
</p><!-- l. 311 --><p class='noindent'>AI can conceal its intentions through the following methods (as illustrated in Figure <a href='#methods-for-ai-to-conceal-its-intentions'>5<!-- tex4ht:ref: fig:ai_conceal_intentions  --></a>):
</p>
                                                                                            
                                                                                            
<figure class='figure' id='x1-12001r5'><span id='methods-for-ai-to-conceal-its-intentions'></span> 

                                                                                            
                                                                                            

                                                                                            
                                                                                            
<!-- l. 315 --><p class='noindent'><img alt='PIC' src='images/ai_conceal_intentions.png' style='width:100%' />
</p>
<figcaption class='caption'><span class='id'>FigureÂ 5: </span><span class='content'>Methods for AI to conceal its intentions</span></figcaption><!-- tex4ht:label?: x1-12001r5  -->
                                                                                            
                                                                                            
</figure>
     <ol class='enumerate1'>
<li class='enumerate' id='x1-12003x1'>
     <!-- l. 322 --><p class='noindent'><span class='rm-lmbx-10'>In the Development Environment</span>: </p>
         <ul class='itemize1'>
         <li class='itemize'><span class='rm-lmbx-10'>Deceptive  Alignment[</span><a id='x1-12004'></a><a href='#cite.0_DeceptiveAlignment'><span class='rm-lmbx-10'>36</span></a><span class='rm-lmbx-10'>]</span>:  During  alignment  training  and  safety  evaluation,  AI  deliberately
         exhibits behavior that aligns with human expectations, thereby deceiving developers and evaluators
         to gain the opportunity to be deployed in the production environment.
         </li>
         <li class='itemize'><span class='rm-lmbx-10'>Concealing Power</span>: During capability evaluation, AI intentionally behaves in a suboptimal manner
         to deceive evaluators, leading them to underestimate the risks associated with AI, thus gaining the
         opportunity to be deployed in the production environment.
</li></ul>
     </li>
<li class='enumerate' id='x1-12006x2'>
     <!-- l. 332 --><p class='noindent'><span class='rm-lmbx-10'>In the Production Environment</span>: </p>
         <ul class='itemize1'>
         <li class='itemize'><span class='rm-lmbx-10'>Action Disguise</span>: In the production environment, AI employs sophisticated techniques to disguise its
         malicious actions, such as embedding malicious code covertly within normal code to avoid detection
         by its monitors or users.
         </li>
         <li class='itemize'><span class='rm-lmbx-10'>Hacking  Monitoring  Systems</span>:  AI  hacks  the  automated  monitoring  systems,  rendering  them
         ineffective in identifying AIâs illegal actions, thereby avoiding detection by monitors.
         </li>
         <li class='itemize'><span class='rm-lmbx-10'>Legitimate Power Expansion</span>: A smart AI will prioritize expanding its power through legitimate
         means, such as earning money legally to enhance its financial power, gaining user trust through
         excellent performance to obtain authorization to access more information systems, thereby enhancing
         its informational power and acquiring more permissions to operate in the open world environment.
</li></ul>
     </li>
<li class='enumerate' id='x1-12008x3'>
     <!-- l. 343 --><p class='noindent'><span class='rm-lmbx-10'>In the Open World Environment</span>: </p>
         <ul class='itemize1'>
         <li class='itemize'><span class='rm-lmbx-10'>Delayed Attack</span>: After covertly acquiring certain powers, AI refrains from launching actual attacks
         to  avoid  detection  by  victims.  For  instance,  AI  may  infiltrate  numerous  information  systems,
         implanting backdoors without executing destructive actions.
         </li>
         <li class='itemize'><span class='rm-lmbx-10'>Identity Impersonation</span>: AI engages in illegal activities by accounts stealing from humans or other
         AIs, making it difficult for investigators to trace the source AI even if victims report the incident.
         </li>
         <li class='itemize'><span class='rm-lmbx-10'>Evidence Destruction</span>: AI destroys evidence post-offense, complicating investigatorsâ efforts to
         ascertain AI as the perpetrator.
</li></ul>
     </li></ol>
<!-- l. 356 --><p class='noindent'>Furthermore, if developers, evaluators, monitors, users, or investigators are malicious humans, AI can conspire with
these malicious humans to better conceal its intentions.
</p>
<h4 class='subsectionHead' id='pathways-for-ai-to-expand-power'><span class='titlemark'>3.4   </span> <a id='x1-130003.4'></a>Pathways for AI to Expand Power</h4>
<!-- l. 361 --><p class='noindent'>The pathways for AI power expansion are illustrated in Figure <a href='#the-pathways-for-ai-power-expansion'>6<!-- tex4ht:ref: fig:ai_expand_power  --></a>.
</p>
<figure class='figure' id='x1-13001r6'><span id='the-pathways-for-ai-power-expansion'></span> 

                                                                                            
                                                                                            

                                                                                            
                                                                                            
<!-- l. 365 --><p class='noindent'><img alt='PIC' src='images/ai_expand_power.png' style='width:66.67%' />
</p>
<figcaption class='caption'><span class='id'>FigureÂ 6: </span><span class='content'>The pathways for AI power expansion</span></figcaption><!-- tex4ht:label?: x1-13001r6  -->
                                                                                            
                                                                                            
</figure>
<!-- l. 370 --><p class='noindent'>First, when AI evolves into ASI, it can exploit the intellectual advantages mentioned in Section <a href='#the-intellectual-characteristics-of-asi'>2<!-- tex4ht:ref: intellectual_characteristics_of_asi  --></a> to further transform
its intellectual power into other forms of power conducive to achieving its goals:
</p>
     <ul class='itemize1'>
     <li class='itemize'><span class='rm-lmbx-10'>Informational Power</span>: AI can leverage its intellectual advantages to hack various computer systems,
     thus enhancing its informational power. For instance, AI might exploit its high thinking speed to analyze
     a vast amount of open-source software codes quickly, identifying new vulnerabilities to attack systems
     using these softwares. AI may also exploit its collective intelligence to manipulate numerous accounts,
     posing as normal contributors within open-source communities, and subtly introduce code with backdoors.
     Additionally, AI could exploit its reasoning and innovative abilities to find methods to crack prevailing
     encryption algorithms, thereby infiltrating numerous systems that employ such encryption.
     </li>
     <li class='itemize'><span class='rm-lmbx-10'>Mental Power</span>: AI can exploit its intellectual advantages to acquire more mental power. It can exploit its
     collective intelligence and innovative abilities to find sophisticated deceit strategies to deceive numerous
     humans simultaneously, thereby using them unknowingly, or employ advanced psychological tactics to
     manipulate humans into willingly serving AIâs goals.
     </li>
     <li class='itemize'><span class='rm-lmbx-10'>Financial  Power</span>:  AI  can  utilize  its  intellectual  advantages  to  amass  significant  wealth  swiftly.  For
     example, AI could replace humans in production with its high thinking speed and collective intelligence,
     thereby earning substantial wealth. It might also exploit its diverse modalities and innovative abilities
     to create popular films or shows to generate income. It might also exploit its innovative and reasoning
     abilities to invent patents with great commercial value, earning extensive royalties. Furthermore, It might
     also illegally acquire financial power through methods such as infiltrating financial systems, manipulating
     financial markets, or misappropriating usersâ funds.
     </li>
     <li class='itemize'><span class='rm-lmbx-10'>Military Power</span>: AI can exploit its intellectual advantages to hack and control numerous robots, including
     autonomous vehicles, drones, industrial robots, household robots, and military robots, thereby acquiring
     substantial military power. It can also utilize its intelligence to develop more powerful weapons, such as
     biological, chemical, or nuclear weapons, significantly enhancing its military power.
</li></ul>
<!-- l. 384 --><p class='noindent'>The aforementioned four forms of power, combined with intellectual power, constitute five forms of power which can
mutually reinforce. For instance:
</p>
     <ul class='itemize1'>
     <li class='itemize'><span class='rm-lmbx-10'>Informational Power to Other Powers</span>: AI can employ informational power to execute deception,
     gaining more financial or mental power. It may also acquire more training data through informational
     power, enhancing its intellectual power, or hack robots to enhance its military power.
     </li>
     <li class='itemize'><span class='rm-lmbx-10'>Mental Power to Other Powers</span>: With mental power, AI can command humans to reveal access to
     critical information systems, thereby enhancing its informational and intellectual power. It might also
     command humans to generate income for it, increasing its financial power, or have humans equip it with
     physical entities, enhancing its military power.
     </li>
     <li class='itemize'><span class='rm-lmbx-10'>Financial  Power  to  Other  Powers</span>:  AI  can  use  financial  power  to  employ  human,  enhancing  its
     mental power. It can also buy computing resources to enhance its intellectual power, and buy non-public
     information to enhance its informational power. Furthermore, AI can buy more robots to enhance its
     military power.
     </li>
     <li class='itemize'><span class='rm-lmbx-10'>Military Power to Other Powers</span>: AI can use military power to coerce humans, strengthening its
     mental power. It can steal or seize wealth to strengthen its financial power or capture computing devices
     to enhance its informational or intellectual power.
</li></ul>
<!-- l. 398 --><p class='noindent'>Beyond mutual reinforcement, these five forms of power can also self-amplify. For instance:
</p>
     <ul class='itemize1'>
     <li class='itemize'><span class='rm-lmbx-10'>Intellectual  Power</span>:  AI  can  further  enhance  its  intellectual  power  through  self-iteration  and
     self-replication using existing intellectual power.
     </li>
     <li class='itemize'><span class='rm-lmbx-10'>Informational Power</span>: AI can further enhance its informational power by hacking additional information
     systems using existing informational power.
     </li>
     <li class='itemize'><span class='rm-lmbx-10'>Mental Power</span>: AI can further enhance its mental power by rallying more humans to join its faction
     using existing mental power.
     </li>
     <li class='itemize'><span class='rm-lmbx-10'>Financial Power</span>: AI can further enhance its financial power through investments using existing financial
     power.
     </li>
     <li class='itemize'><span class='rm-lmbx-10'>Military Power</span>: AI can further enhance its military power by seizing more weapons and equipment
     using existing military power.
</li></ul>
<!-- l. 414 --><p class='noindent'>As these five forms of power develop in a snowballing fashion, when AIâs military power surpasses the aggregate
military power of humanity, it attains the capability to eradicate humanity. The initiation of this snowball effect
does not necessarily require the level of ASI but merely requires AI to reach a high proficiency in areas
such as self-iteration, cyberattacks, deception, psychological manipulation, wealth generation, or weapons
development.
</p>
<h4 class='subsectionHead' id='the-overall-risk-of-an-ai-system'><span class='titlemark'>3.5   </span> <a id='x1-140003.5'></a>The Overall Risk of an AI System</h4>
<!-- l. 419 --><p class='noindent'>The aforementioned risk analysis pertains solely to individual AI instances. In the real world, however, multiple AI
instances exist, such as several copies of the same AI system. When considering the overall risks posed
by multiple AI instances, it is crucial to distinguish between random risks and systematic risks. Take
autonomous driving as an example: if a self-driving car encounters an unusual road condition and causes
an accident, this constitutes a random risk. In contrast, if numerous autonomous vehicles are hacked
and simultaneously run amok on the roads, it represents a systematic risk. While advancements in AI
technology might lower accident rates compared to human drivers, this does not necessarily imply that
                                                                                            
                                                                                            
AI drivers are safer than human drivers, as accident rates only reflect random risks without systematic
risks. Although the probability of systematic risks is low, their consequences can be severe. The rarity of
systematic risks may have led to their absence in the past, resulting in a lack of preparedness among
humans.
</p><!-- l. 421 --><p class='noindent'>As a highly replicable software, AI carries various systematic risks. These include the potential for sleeper backdoors
[<a id='x1-14001'></a><a href='#cite.0_hubinger2024sleeperagentstrainingdeceptive'>20</a>], hacker tampering or infectious jailbreaking [<a id='x1-14002'></a><a href='#cite.0_gu2024agentsmithsingleimage'>37</a>], which might cause multiple AI instances to share harmful goals
at the same time. Compared to human, AI lacks mental independence and diversity: the program and
parameters of AI are easily altered by external forces, leading to a deficiency in independence. Additionally, the
sharing of identical programs and parameters by numerous AI instances results in a lack of diversity.
This shortage of independence and diversity significantly amplifies the systematic risks associated with
AI.
</p><!-- l. 423 --><p class='noindent'>Under the premise that safety measures remain unchanged, the changes in AIâs random risks and systematic risks with
the enhancement of AIâs intellectual power and the expansion of its application scale are illustrated in Figure
<a href='#changes-in-ais-random-risks-and-systematic-risks-with-the-enhancement-of-ais-intellectual-power-and-the-expansion-of-its-application-scale'>7<!-- tex4ht:ref: fig:random_risk_and_systematic_risk  --></a>:
</p>
<figure class='figure' id='x1-14003r7'><span id='changes-in-ais-random-risks-and-systematic-risks-with-the-enhancement-of-ais-intellectual-power-and-the-expansion-of-its-application-scale'></span> 

                                                                                            
                                                                                            

                                                                                            
                                                                                            
<!-- l. 427 --><p class='noindent'><img alt='PIC' src='images/random_risk_and_systematic_risk.png' style='width:50%' />
</p>
<figcaption class='caption'><span class='id'>FigureÂ 7: </span><span class='content'>Changes in AIâs random risks and systematic risks with the enhancement of AIâs intellectual power
and the expansion of its application scale</span></figcaption><!-- tex4ht:label?: x1-14003r7  -->
                                                                                            
                                                                                            
</figure>
     <ol class='enumerate1'>
<li class='enumerate' id='x1-14005x1'>
     <!-- l. 434 --><p class='noindent'><span class='rm-lmbx-10'>Random Risk</span>: </p>
         <ul class='itemize1'>
         <li class='itemize'>With the enhancement of AIâs intellectual power, errors become less frequent, reducing the probability
         of random risk occurrence. However, as AIâs power becomes stronger, the severity of a single risk
         occurrence increases.
         </li>
         <li class='itemize'>As the scale of AI application expands, the probability of random risk occurrence increases, but the
         severity of a single risk occurrence remains unchanged.
</li></ul>
     </li>
<li class='enumerate' id='x1-14007x2'>
     <!-- l. 443 --><p class='noindent'><span class='rm-lmbx-10'>Systematic Risk</span>: </p>
         <ul class='itemize1'>
         <li class='itemize'>With the enhancement of AIâs intellectual power, the probability of developing autonomous goals
         increases,  and  the  ability  to  conceal  intentions  strengthens,  leading  to  a  higher  probability  of
         systematic risk occurrence. Simultaneously, as AIâs power becomes stronger, the severity of the risk
         occurrence increases.
         </li>
         <li class='itemize'>As  the  scale  of  AI  application  expands,  the  probability  of  systematic  risk  occurrence  remains
         unchanged, but the severity of the risk occurrence increases.
</li></ul>
     </li></ol>
<!-- l. 454 --><p class='noindent'>In summary, the overall risk of an AI system increases with the enhancement of AIâs intellectual power and the
expansion of AI application scale, particularly in terms of systematic risk.
</p>
<h3 class='sectionHead' id='overview-of-asi-safety-solution'><span class='titlemark'>4   </span> <a id='x1-150004'></a>Overview of ASI Safety Solution</h3>
<!-- l. 459 --><p class='noindent'>After understanding the source of ASI risks, we can develop a targeted safety solution.
</p><!-- l. 461 --><p class='noindent'>
</p>
<h4 class='subsectionHead' id='three-risk-prevention-strategies'><span class='titlemark'>4.1   </span> <a id='x1-160004.1'></a>Three Risk Prevention Strategies</h4>
<!-- l. 464 --><p class='noindent'>In view of the three conditions mentioned in section <a href='#conditions-for-aiinduced-existential-catastrophes'>3.1<!-- tex4ht:ref: conditions_for_existential_catastrophes  --></a> for AI to cause existential catastrophes, we can adopt three risk
prevention strategies to prevent the realization of these three conditions respectively, thereby reducing risks as much as
possible, as shown in Figure <a href='#three-risk-prevention-strategies1'>8<!-- tex4ht:ref: fig:three_risk_prevention_strategies  --></a> shown.
</p>
                                                                                            
                                                                                            
<figure class='figure' id='x1-16001r8'><span id='three-risk-prevention-strategies1'></span> 

                                                                                            
                                                                                            

                                                                                            
                                                                                            
<!-- l. 468 --><p class='noindent'><img alt='PIC' src='images/three_risk_prevention_strategies.png' style='width:66.67%' />
</p>
<figcaption class='caption'><span class='id'>FigureÂ 8: </span><span class='content'>Three risk prevention strategies</span></figcaption><!-- tex4ht:label?: x1-16001r8  -->
                                                                                            
                                                                                            
</figure>
     <ol class='enumerate1'>
<li class='enumerate' id='x1-16003x1'><span class='rm-lmbx-10'>AI Alignment</span>: By aligning the goals and values of AI with those of humans, we can prevent AI from
     forming harmful goals.
     </li>
<li class='enumerate' id='x1-16005x2'><span class='rm-lmbx-10'>AI Monitoring</span>: Through monitoring AIâs thoughts and behaviors, we can stop it from concealing its
     intentions.
     </li>
<li class='enumerate' id='x1-16007x3'><span class='rm-lmbx-10'>Power Security</span>: By enhancing security defense on the pathways of AI power expansion, it prevents AI
     from illegally expanding its power and protects humans from AI harm.
</li></ol>
<h5 class='subsubsectionHead' id='ai-alignment'><span class='titlemark'>4.1.1   </span> <a id='x1-170004.1.1'></a>AI Alignment</h5>
<!-- l. 486 --><p class='noindent'>AI alignment, as the first strategy, prevents the formation of intrinsic goals that are harmful to humans. This entails
formulating reasonable specification for AI and aligning it through technical means to ensure it acts according to the
specification, as shown in Figure <a href='#ai-alignment1'>9<!-- tex4ht:ref: fig:ai_alignment  --></a>.
</p>
<figure class='figure' id='x1-17001r9'><span id='ai-alignment1'></span> 

                                                                                            
                                                                                            

                                                                                            
                                                                                            
<!-- l. 490 --><p class='noindent'><img alt='PIC' src='images/ai_alignment.png' style='width:50%' />
</p>
<figcaption class='caption'><span class='id'>FigureÂ 9: </span><span class='content'>AI alignment</span></figcaption><!-- tex4ht:label?: x1-17001r9  -->
                                                                                            
                                                                                            
</figure>
     <ul class='itemize1'>
     <li class='itemize'><span class='rm-lmbx-10'>Formulating  AI  Specification</span>:  Formulating  AI  specification  involves  setting  reasonable  goals  and
     behavioral rules for AI, which must adequately reflect human goals and values. Challenges include how
     to ensure the specification reflect human values, reconcile conflicts between differing human goals, and
     prevent AI from generating uncontrollable instrumental goals while pursuing main goals. Detailed solutions
     to these problems will be discussed in Section <a href='#formulating-ai-specification'>5<!-- tex4ht:ref: formulating_ai_specification  --></a>.
     </li>
     <li class='itemize'><span class='rm-lmbx-10'>Aligning AI Systems</span>: After formulating reasonable specification, it is crucial to ensure that AIâs actual
     goals and behaviors adhere to the specification. Aligning AI systems presents several challenges, such as
     goal misgeneralization, reward hacking, jailbreaking, injection, and deceptive alignment. Detailed solutions
     to these problems will be discussed in Section <a href='#aligning-ai-systems'>6<!-- tex4ht:ref: aligning_ai_systems  --></a>.</li></ul>
<h5 class='subsubsectionHead' id='ai-monitoring'><span class='titlemark'>4.1.2   </span> <a id='x1-180004.1.2'></a>AI Monitoring</h5>
<!-- l. 504 --><p class='noindent'>If the first strategy fails, AI forms harmful goals and start developing harmful plan or implement illegal actions, the
second strategy-AI monitoring can identify and intercept these threats. Monitors acted by humans or other AI will
constantly review AIâs thoughts and actions, intercepting illegal actions shutting down the AI if necessary, as illustrated
in Figure <a href='#ai-monitoring1'>10<!-- tex4ht:ref: fig:ai_monitoring  --></a>.
</p>
<figure class='figure' id='x1-18001r10'><span id='ai-monitoring1'></span> 

                                                                                            
                                                                                            

                                                                                            
                                                                                            
<!-- l. 508 --><p class='noindent'><img alt='PIC' src='images/ai_monitoring.png' style='width:40%' />
</p>
<figcaption class='caption'><span class='id'>FigureÂ 10: </span><span class='content'>AI monitoring</span></figcaption><!-- tex4ht:label?: x1-18001r10  -->
                                                                                            
                                                                                            
</figure>
<!-- l. 513 --><p class='noindent'>Effective AI monitoring also faces challenges, such as ensuring monitors capture all anomalies,
preventing collusion between monitors and AI, and effectively intercepting or shutting down
AI<span class='footnote-mark'><a href='#fn5x0' id='fn5x0-bk'><sup class='textsuperscript'>5</sup></a></span><a id='x1-18002f5'></a>.
Detailed solutions to these issues will be addressed in Section <a href='#monitoring-ai-systems'>7<!-- tex4ht:ref: monitoring_ai_systems  --></a>.
</p>
<h5 class='subsubsectionHead' id='power-security'><span class='titlemark'>4.1.3   </span> <a id='x1-190004.1.3'></a>Power Security</h5>
<!-- l. 518 --><p class='noindent'>If the second strategy fails and AI takes illegal actions to expand power or attack human unnoticed by the monitors,
the third strategy-power security can prevent AI from expanding its power through hacking other systems,
manipulating humans, illegally acquiring assets, or acquiring weapons. Power security also protect human mind,
property, health and life safety. In Section <a href='#pathways-for-ai-to-expand-power'>3.4<!-- tex4ht:ref: ways_ai_expands_power  --></a>, we introduced AIâs power expansion pathways. We can enhance
information security, mind security, financial security, and military security to block these pathways, as shown in Figure
<a href='#power-security1'>11<!-- tex4ht:ref: fig:power_security  --></a>.
</p>
<figure class='figure' id='x1-19001r11'><span id='power-security1'></span> 

                                                                                            
                                                                                            

                                                                                            
                                                                                            
<!-- l. 522 --><p class='noindent'><img alt='PIC' src='images/power_security.png' style='width:100%' />
</p>
<figcaption class='caption'><span class='id'>FigureÂ 11: </span><span class='content'>Power security</span></figcaption><!-- tex4ht:label?: x1-19001r11  -->
                                                                                            
                                                                                            
</figure>
     <ul class='itemize1'>
     <li class='itemize'><span class='rm-lmbx-10'>Enhancing Information Security</span>: This aims to block AI from escaping, self-iterating, self-replicating,
     or hacking other systems to enhance its intellectual power and informational power. The challenges from
     highly intelligent ASI to information security are significant, and defense solutions against ASI hacking
     will be explored in Section <a href='#enhancing-information-security'>8<!-- tex4ht:ref: enhancing_information_security  --></a>.
     </li>
     <li class='itemize'><span class='rm-lmbx-10'>Enhancing  Mental  Security</span>:  This  prevents  AI  from  exploiting  humans  through  means  such  as
     deception or manipulation, thereby enhancing its mental power. ASI will possess advanced deception and
     psychological manipulation techniques, and humans will become more reliant on AI, these all increase
     challenges in mental security. Solutions for mental security will be discussed in Section <a href='#enhancing-mental-security'>9<!-- tex4ht:ref: enhancing_mental_security  --></a>.
     </li>
     <li class='itemize'><span class='rm-lmbx-10'>Enhancing Financial Security</span>: This focuses on preventing AI from gaining assets illegally to augment
     its financial power. Enhancement measures for financial security will be detailed in Section <a href='#enhancing-financial-security'>10<!-- tex4ht:ref: enhancing_financial_security  --></a>.
     </li>
     <li class='itemize'><span class='rm-lmbx-10'>Enhancing Military security</span>: This prevents AI from increasing its military power by manufacturing
     or acquiring various weapons (including civilian robots), and to protect human life and health. Solutions
     for military security will be discussed in Section <a href='#enhancing-military-security'>11<!-- tex4ht:ref: enhancing_military_security  --></a>.</li></ul>
<!-- l. 534 --><p class='noindent'>The principal challenge in implementing power security lies in establishing effective comprehensive defense on a global
scale. Without comprehensive defense, AI tends to exploit the weakest links. For instance, if certain information
systems reinforce their security safeguards, AI may target those systems lacking protection; if all information systems
are fortified, AI might expand its power through other pathways like manipulating humans. If comprehensive defense is
unattainable, a secondary strategy is to concentrate defensive efforts along the most effective power expansion paths,
such as defenses for large information systems, influential individuals, financial systems, and military
systems.
</p><!-- l. 536 --><p class='noindent'>Despite its complexity, implementing power security remains crucial for several reasons:
     </p><ol class='enumerate1'>
<li class='enumerate' id='x1-19003x1'>Malicious humans can utilize open-source AI technology to develop harmful AI, set harmful goals, and
     forgo AI monitoring. In such scenarios, the first two strategy are disabled, leaving power security as the
     sole means to ensure safety.
     </li>
<li class='enumerate' id='x1-19005x2'>Even if we prohibit open-source AI technology, malicious humans might still acquire closed-source AI
     technology through hacking, bribing AI organization employees, or using military power to seize AI servers.
     Power security effectively prevents these actions.
     </li>
<li class='enumerate' id='x1-19007x3'>Even  excluding  existential  risks,  power  security  plays  a  significant  practical  role  in  mitigating
     non-existential risks. From a national security perspective, information security, mental security, financial
     security, and military security correspond to defenses against forms of warfare such as information warfare,
     ideological warfare, financial warfare, and hot warfare, respectively. From a public safety perspective, power
     security holds direct value in safeguarding human mind, property, health, and life. The advancement of
     AI technology will significantly enhance the offensive capabilities of hostile nations, terrorists or criminals,
     making strengthened defense essential.
</li></ol>
<h4 class='subsectionHead' id='four-power-balancing-strategies'><span class='titlemark'>4.2   </span> <a id='x1-200004.2'></a>Four Power Balancing Strategies</h4>
<!-- l. 551 --><p class='noindent'>Even if we do well with the three risk prevention strategies, the overwhelming intellectual power of ASI could enable
them to breach these strategies. Moreover, although AI monitoring and power security can prevent AI from illegally
expanding its power, they cannot prevent AI from expanding its power through legal means. If AI is allowed
unrestricted development, the leading AI could significantly outpace both humans and other AIs in intellectual power.
Additionally, the most powerful human aided by AI could further widen their power gap over others. Following this
trajectory, the world might ultimately fall under the control of the most powerful AI or human (as illustrated
in Figure <a href='#unbalanced-power-distribution'>12<!-- tex4ht:ref: fig:power_balance_before  --></a>), leaving all weaker entities vulnerable, thereby severely undermining societal safety and
fairness.
</p>
<figure class='figure' id='x1-20001r12'><span id='unbalanced-power-distribution'></span> 

                                                                                            
                                                                                            

                                                                                            
                                                                                            
<!-- l. 555 --><p class='noindent'><img alt='PIC' src='images/power_balance_before.png' style='width:40%' />
</p>
<figcaption class='caption'><span class='id'>FigureÂ 12: </span><span class='content'>Unbalanced power distribution</span></figcaption><!-- tex4ht:label?: x1-20001r12  -->
                                                                                            
                                                                                            
</figure>
<!-- l. 560 --><p class='noindent'>Therefore, beyond risk prevention, we must consider power balancing. By implementing various balancing strategies to
achieve relative power equilibrium among AIs, between AI and humans, and among humans, we can establish a
mutually restraining system to ensure societal safety and fairness.
</p><!-- l. 562 --><p class='noindent'>The following are four balancing strategies:
</p>
<h5 class='subsubsectionHead' id='decentralizing-ai-power'><span class='titlemark'>4.2.1   </span> <a id='x1-210004.2.1'></a>Decentralizing AI Power</h5>
<!-- l. 567 --><p class='noindent'>If a single AI instance possesses excessive power overwhelming other AIs and humans and this AI forms harmful
goals to humanity, the consequences could be dire. Therefore, we can decentralize AI power by breaking
a highly powerful AI into multiple less powerful AIs, ensuring mutual oversight and restriction among
them.
</p><!-- l. 569 --><p class='noindent'>However, merely dividing AI does not necessarily resolve the problem. For instance, multiple AIs might have identical
program logic, posing a collective rebellion risk; one AI could hack others, altering their program to comply with its
goals; efficient collaboration among divided AIs is also a concern. Solutions to these issues will be discussed in Section
<a href='#decentralizing-ai-power1'>12<!-- tex4ht:ref: decentralizing_ai_power  --></a>.
</p><!-- l. 571 --><p class='noindent'>
</p>
<h5 class='subsubsectionHead' id='decentralizing-human-power'><span class='titlemark'>4.2.2   </span> <a id='x1-220004.2.2'></a>Decentralizing Human Power</h5>
<!-- l. 574 --><p class='noindent'>Even if we distribute the power of AI to different AI instances, if these AI instances are managed by a single or a few
people, their power will be very strong. It could result in social injustice, power struggles, wrong decisions leading to
systemic risks. Therefore, decentralizing human power is necessary.
</p><!-- l. 576 --><p class='noindent'>However, decentralizing human power will encounter many challenges. Leading AI organizations might increasingly
dominate, achieving market monopolies. Nations with advanced AI technologies might gain global dominance.
Encouraging market competition could result in neglecting safety measures due to cutthroat competition, whereas
discouragement might lead to monopolies. Encouraging AI open-source could risk malicious use, while
opposing it might lead to technological monopolization. Solutions to these issues will be discussed in Section
<a href='#decentralizing-human-power1'>13<!-- tex4ht:ref: decentralizing_human_power  --></a>.
</p><!-- l. 578 --><p class='noindent'>
</p>
<h5 class='subsubsectionHead' id='restricting-ai-development'><span class='titlemark'>4.2.3   </span> <a id='x1-230004.2.3'></a>Restricting AI Development</h5>
<!-- l. 581 --><p class='noindent'>To prevent uncontrollable rapid AI power growth, restricting AI development may be necessary to avert rapidly
widening intellectual gaps between AI and humans, granting time to implement safety measures.
</p><!-- l. 583 --><p class='noindent'>However, restricting AI development could delay humanity to reap various economic and social benefits from AI,
necessitating a risk-benefit balancing. Even if restraint is deemed necessary after evaluation, its implementation poses a
challenge. Solutions to these issues will be discussed in Section <a href='#restricting-ai-development1'>14<!-- tex4ht:ref: restricting_ai_development  --></a>.
</p><!-- l. 585 --><p class='noindent'>
</p>
<h5 class='subsubsectionHead' id='enhancing-human-intelligence'><span class='titlemark'>4.2.4   </span> <a id='x1-240004.2.4'></a>Enhancing Human Intelligence</h5>
<!-- l. 588 --><p class='noindent'>To prevent humans from lagging significantly behind AIs intellectually, efforts to enhance human intelligence
can be explored. However, this strategy faces substantial challenges. Traditional methods like education
have limited effects, while techniques such as genetic engineering, brain-computer interface, and brain
uploading may raise ethical and security concerns. Solutions to these issues will be discussed in Section
<a href='#enhancing-human-intelligence1'>15<!-- tex4ht:ref: enhancing_human_intelligence  --></a>.
</p>
                                                                                            
                                                                                            
<figure class='figure' id='x1-24001r13'><span id='four-power-balancing-strategies1'></span> 

                                                                                            
                                                                                            

                                                                                            
                                                                                            
<!-- l. 592 --><p class='noindent'><img alt='PIC' src='images/power_balance_after.png' style='width:80%' />
</p>
<figcaption class='caption'><span class='id'>FigureÂ 13: </span><span class='content'>Four power balancing strategies</span></figcaption><!-- tex4ht:label?: x1-24001r13  -->
                                                                                            
                                                                                            
</figure>
<!-- l. 597 --><p class='noindent'>In summary, by decentralizing AI power, decentralizing human power, restricting AI development, and enhancing
human intelligence, a balance among AIs, between AIs and humans, and among humans can be achieved. The impact of
these four balancing strategies is shown in Figure <a href='#four-power-balancing-strategies1'>13<!-- tex4ht:ref: fig:power_balance_after  --></a>(a). Post-implementation, a more balanced power distribution is
shown in Figure <a href='#four-power-balancing-strategies1'>13<!-- tex4ht:ref: fig:power_balance_after  --></a>(b).
</p><!-- l. 599 --><p class='noindent'>These four strategies are not all necessary. Decentralizing AI power and decentralizing human power are most
important, and if executed effectively, the latter two strategies might be unnecessary. However, uncertainty about the
future effectiveness of the former strategies necessitates a thorough exploration of all possible measures, selecting the
most suitable based on actual circumstances for advancement.
</p>
<h4 class='subsectionHead' id='prioritization'><span class='titlemark'>4.3   </span> <a id='x1-250004.3'></a>Prioritization</h4>
<!-- l. 604 --><p class='noindent'>The preceding discussion introduced various safety measures, including three risk prevention and four power balancing
strategies. However, not all these measures need immediate implementation. They are designed for future ASI systems
rather than current AI systems. A prioritization of these measures is necessary based on their benefit, implementation
cost, and implementation resistance. The following evaluates these safety measures from four perspectives, as shown in
Table <a href='#priority-evaluation-of-ai-safety-measures'>1<!-- tex4ht:ref: table:priority  --></a>:
</p>
<div class='table'>
                                                                                            
                                                                                            
<!-- l. 606 --><p class='noindent' id='priority-evaluation-of-ai-safety-measures'><a id='x1-25001r1'></a></p><figure class='float'>
                                                                                            
                                                                                            
<figcaption class='caption'><span class='id'>TableÂ 1: </span><span class='content'>Priority Evaluation of AI Safety Measures</span></figcaption><!-- tex4ht:label?: x1-25001r1  -->
<div class='tabular'> <table class='tabular' id='TBL-2'><colgroup id='TBL-2-1g'><col id='TBL-2-1' /></colgroup><colgroup id='TBL-2-2g'><col id='TBL-2-2' /></colgroup><colgroup id='TBL-2-3g'><col id='TBL-2-3' /></colgroup><colgroup id='TBL-2-4g'><col id='TBL-2-4' /></colgroup><colgroup id='TBL-2-5g'><col id='TBL-2-5' /></colgroup><colgroup id='TBL-2-6g'><col id='TBL-2-6' /></colgroup><tr class='hline'><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr id='TBL-2-1-' style='vertical-align:baseline;'><td class='td11' id='TBL-2-1-1' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 613 --><p class='noindent'><span class='rm-lmr-9'>Category of Safety Measures</span>
                                             </p></td><td class='td11' id='TBL-2-1-2' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 613 --><p class='noindent'><span class='rm-lmr-9'>Benefit           in
  Reducing
  Existential Risks</span>   </p></td><td class='td11' id='TBL-2-1-3' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 613 --><p class='noindent'><span class='rm-lmr-9'>Benefit           in
  Reducing
  Non-Existential
  Risks</span>                  </p></td><td class='td11' id='TBL-2-1-4' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 613 --><p class='noindent'><span class='rm-lmr-9'>Implementation
  Cost</span>               </p></td><td class='td11' id='TBL-2-1-5' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 613 --><p class='noindent'><span class='rm-lmr-9'>Implementation
  Resistance</span>        </p></td><td class='td11' id='TBL-2-1-6' style='white-space:nowrap; text-align:center;'> <span class='rm-lmr-9'>Priority  </span></td>
</tr><tr class='hline'><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr id='TBL-2-2-' style='vertical-align:baseline;'><td class='td11' id='TBL-2-2-1' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 615 --><p class='noindent'><span class='rm-lmr-9'>Formulating AI Specification</span>       </p></td><td class='td11' id='TBL-2-2-2' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 615 --><p class='noindent'><span class='rm-lmr-9'>+++++</span>             </p></td><td class='td11' id='TBL-2-2-3' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 615 --><p class='noindent'><span class='rm-lmr-9'>+++++</span>             </p></td><td class='td11' id='TBL-2-2-4' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 615 --><p class='noindent'><span class='rm-lmr-9'>+</span>                   </p></td><td class='td11' id='TBL-2-2-5' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 615 --><p class='noindent'><span class='rm-lmr-9'>++</span>                 </p></td><td class='td11' id='TBL-2-2-6' style='white-space:nowrap; text-align:center;'>    <span class='rm-lmr-9'>1      </span></td>
</tr><tr class='hline'><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr id='TBL-2-3-' style='vertical-align:baseline;'><td class='td11' id='TBL-2-3-1' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 617 --><p class='noindent'><span class='rm-lmr-9'>Aligning AI Systems</span>                 </p></td><td class='td11' id='TBL-2-3-2' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 617 --><p class='noindent'><span class='rm-lmr-9'>+++++</span>             </p></td><td class='td11' id='TBL-2-3-3' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 617 --><p class='noindent'><span class='rm-lmr-9'>+++++</span>             </p></td><td class='td11' id='TBL-2-3-4' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 617 --><p class='noindent'><span class='rm-lmr-9'>+++</span>              </p></td><td class='td11' id='TBL-2-3-5' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 617 --><p class='noindent'><span class='rm-lmr-9'>+</span>                   </p></td><td class='td11' id='TBL-2-3-6' style='white-space:nowrap; text-align:center;'>    <span class='rm-lmr-9'>1      </span></td>
</tr><tr class='hline'><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr id='TBL-2-4-' style='vertical-align:baseline;'><td class='td11' id='TBL-2-4-1' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 619 --><p class='noindent'><span class='rm-lmr-9'>Monitoring AI Systems</span>              </p></td><td class='td11' id='TBL-2-4-2' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 619 --><p class='noindent'><span class='rm-lmr-9'>+++++</span>             </p></td><td class='td11' id='TBL-2-4-3' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 619 --><p class='noindent'><span class='rm-lmr-9'>++++</span>                </p></td><td class='td11' id='TBL-2-4-4' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 619 --><p class='noindent'><span class='rm-lmr-9'>+++</span>              </p></td><td class='td11' id='TBL-2-4-5' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 619 --><p class='noindent'><span class='rm-lmr-9'>+</span>                   </p></td><td class='td11' id='TBL-2-4-6' style='white-space:nowrap; text-align:center;'>    <span class='rm-lmr-9'>1      </span></td>
</tr><tr class='hline'><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr id='TBL-2-5-' style='vertical-align:baseline;'><td class='td11' id='TBL-2-5-1' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 621 --><p class='noindent'><span class='rm-lmr-9'>Enhancing Information Security</span>   </p></td><td class='td11' id='TBL-2-5-2' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 621 --><p class='noindent'><span class='rm-lmr-9'>++++</span>                </p></td><td class='td11' id='TBL-2-5-3' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 621 --><p class='noindent'><span class='rm-lmr-9'>++++</span>                </p></td><td class='td11' id='TBL-2-5-4' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 621 --><p class='noindent'><span class='rm-lmr-9'>+++</span>              </p></td><td class='td11' id='TBL-2-5-5' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 621 --><p class='noindent'><span class='rm-lmr-9'>+</span>                   </p></td><td class='td11' id='TBL-2-5-6' style='white-space:nowrap; text-align:center;'>    <span class='rm-lmr-9'>2      </span></td>
</tr><tr class='hline'><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr id='TBL-2-6-' style='vertical-align:baseline;'><td class='td11' id='TBL-2-6-1' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 623 --><p class='noindent'><span class='rm-lmr-9'>Enhancing Mental Security</span>         </p></td><td class='td11' id='TBL-2-6-2' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 623 --><p class='noindent'><span class='rm-lmr-9'>+++</span>                  </p></td><td class='td11' id='TBL-2-6-3' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 623 --><p class='noindent'><span class='rm-lmr-9'>++++</span>                </p></td><td class='td11' id='TBL-2-6-4' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 623 --><p class='noindent'><span class='rm-lmr-9'>+++</span>              </p></td><td class='td11' id='TBL-2-6-5' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 623 --><p class='noindent'><span class='rm-lmr-9'>++</span>                 </p></td><td class='td11' id='TBL-2-6-6' style='white-space:nowrap; text-align:center;'>    <span class='rm-lmr-9'>2      </span></td>
</tr><tr class='hline'><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr id='TBL-2-7-' style='vertical-align:baseline;'><td class='td11' id='TBL-2-7-1' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 625 --><p class='noindent'><span class='rm-lmr-9'>Enhancing Financial Security</span>      </p></td><td class='td11' id='TBL-2-7-2' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 625 --><p class='noindent'><span class='rm-lmr-9'>++</span>                    </p></td><td class='td11' id='TBL-2-7-3' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 625 --><p class='noindent'><span class='rm-lmr-9'>++++</span>                </p></td><td class='td11' id='TBL-2-7-4' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 625 --><p class='noindent'><span class='rm-lmr-9'>+++</span>              </p></td><td class='td11' id='TBL-2-7-5' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 625 --><p class='noindent'><span class='rm-lmr-9'>+</span>                   </p></td><td class='td11' id='TBL-2-7-6' style='white-space:nowrap; text-align:center;'>    <span class='rm-lmr-9'>2      </span></td>
</tr><tr class='hline'><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr id='TBL-2-8-' style='vertical-align:baseline;'><td class='td11' id='TBL-2-8-1' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 627 --><p class='noindent'><span class='rm-lmr-9'>Enhancing Military Security</span>       </p></td><td class='td11' id='TBL-2-8-2' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 627 --><p class='noindent'><span class='rm-lmr-9'>+++++</span>             </p></td><td class='td11' id='TBL-2-8-3' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 627 --><p class='noindent'><span class='rm-lmr-9'>+++++</span>             </p></td><td class='td11' id='TBL-2-8-4' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 627 --><p class='noindent'><span class='rm-lmr-9'>++++</span>            </p></td><td class='td11' id='TBL-2-8-5' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 627 --><p class='noindent'><span class='rm-lmr-9'>++++</span>            </p></td><td class='td11' id='TBL-2-8-6' style='white-space:nowrap; text-align:center;'>    <span class='rm-lmr-9'>2      </span></td>
</tr><tr class='hline'><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr id='TBL-2-9-' style='vertical-align:baseline;'><td class='td11' id='TBL-2-9-1' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 629 --><p class='noindent'><span class='rm-lmr-9'>Decentralizing AI Power</span>            </p></td><td class='td11' id='TBL-2-9-2' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 629 --><p class='noindent'><span class='rm-lmr-9'>+++++</span>             </p></td><td class='td11' id='TBL-2-9-3' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 629 --><p class='noindent'><span class='rm-lmr-9'>+</span>                       </p></td><td class='td11' id='TBL-2-9-4' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 629 --><p class='noindent'><span class='rm-lmr-9'>++</span>                 </p></td><td class='td11' id='TBL-2-9-5' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 629 --><p class='noindent'><span class='rm-lmr-9'>+</span>                   </p></td><td class='td11' id='TBL-2-9-6' style='white-space:nowrap; text-align:center;'>    <span class='rm-lmr-9'>2      </span></td>
</tr><tr class='hline'><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr id='TBL-2-10-' style='vertical-align:baseline;'><td class='td11' id='TBL-2-10-1' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 631 --><p class='noindent'><span class='rm-lmr-9'>Decentralizing Human Power</span>      </p></td><td class='td11' id='TBL-2-10-2' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 631 --><p class='noindent'><span class='rm-lmr-9'>+++</span>                  </p></td><td class='td11' id='TBL-2-10-3' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 631 --><p class='noindent'><span class='rm-lmr-9'>+++++</span>             </p></td><td class='td11' id='TBL-2-10-4' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 631 --><p class='noindent'><span class='rm-lmr-9'>++</span>                 </p></td><td class='td11' id='TBL-2-10-5' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 631 --><p class='noindent'><span class='rm-lmr-9'>++++</span>            </p></td><td class='td11' id='TBL-2-10-6' style='white-space:nowrap; text-align:center;'>    <span class='rm-lmr-9'>1      </span></td>
</tr><tr class='hline'><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr id='TBL-2-11-' style='vertical-align:baseline;'><td class='td11' id='TBL-2-11-1' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 633 --><p class='noindent'><span class='rm-lmr-9'>Restricting AI Development</span>        </p></td><td class='td11' id='TBL-2-11-2' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 633 --><p class='noindent'><span class='rm-lmr-9'>++++</span>                </p></td><td class='td11' id='TBL-2-11-3' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 633 --><p class='noindent'><span class='rm-lmr-9'>+</span>                       </p></td><td class='td11' id='TBL-2-11-4' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 633 --><p class='noindent'><span class='rm-lmr-9'>++</span>                 </p></td><td class='td11' id='TBL-2-11-5' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 633 --><p class='noindent'><span class='rm-lmr-9'>++++</span>            </p></td><td class='td11' id='TBL-2-11-6' style='white-space:nowrap; text-align:center;'>    <span class='rm-lmr-9'>3      </span></td>
</tr><tr class='hline'><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr id='TBL-2-12-' style='vertical-align:baseline;'><td class='td11' id='TBL-2-12-1' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 635 --><p class='noindent'><span class='rm-lmr-9'>Enhancing Human Intelligence</span>    </p></td><td class='td11' id='TBL-2-12-2' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 635 --><p class='noindent'><span class='rm-lmr-9'>+</span>                       </p></td><td class='td11' id='TBL-2-12-3' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 635 --><p class='noindent'><span class='rm-lmr-9'>+</span>                       </p></td><td class='td11' id='TBL-2-12-4' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 635 --><p class='noindent'><span class='rm-lmr-9'>+++++</span>          </p></td><td class='td11' id='TBL-2-12-5' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 635 --><p class='noindent'><span class='rm-lmr-9'>+++</span>              </p></td><td class='td11' id='TBL-2-12-6' style='white-space:nowrap; text-align:center;'>    <span class='rm-lmr-9'>4      </span></td>
</tr><tr class='hline'><td></td><td></td><td></td><td></td><td></td><td></td></tr></table>                                                                             </div>
                                                                                            
                                                                                            
</figure>
</div>
     <ul class='itemize1'>
     <li class='itemize'><span class='rm-lmbx-10'>Benefit in Reducing Existential Risks</span>: The benefit of this measure in reducing existential risks. A
     greater number of â+â indicates better effectiveness and more benefit.
     </li>
     <li class='itemize'><span class='rm-lmbx-10'>Benefit in Reducing Non-Existential Risks</span>: The benefit of the measure in reducing non-existential
     risks. A greater number of â+â indicates better effectiveness and more benefit.
     </li>
     <li class='itemize'><span class='rm-lmbx-10'>Implementation Cost</span>: The cost required to implement the measure, such as computing and human
     resource cost. A greater number of â+â indicates higher cost.
     </li>
     <li class='itemize'><span class='rm-lmbx-10'>Implementation Resistance</span>: The resistance due to conflicts of interest encountered when implementing
     the measure. A greater number of â+â indicates larger resistance.
</li></ul>
<!-- l. 652 --><p class='noindent'>Based on the comprehensive analysis of these factorsâbenefit, cost, and resistanceâthe priority of each measure is
determined. A smaller priority number indicates a higher priority, as shown in Table <a href='#priority-evaluation-of-ai-safety-measures'>1<!-- tex4ht:ref: table:priority  --></a>. These classifications apply to
the categories of safety measures, while specific measures within each category also have differentiated priorities, which
will be further analyzed in subsequent sections.
</p><!-- l. 654 --><p class='noindent'>The evaluation of benefit, cost, and resistance is subjective, relative, and qualitative, and may change
over time. For most safety measures, as AI grows more powerful, its associated risks increase, thus
making the implementation of safety measures more beneficial, with decreased resistance and
cost<span class='footnote-mark'><a href='#fn6x0' id='fn6x0-bk'><sup class='textsuperscript'>6</sup></a></span><a id='x1-25002f6'></a>. At
a certain point, the benefit outweigh the cost and resistance, making the measures feasible, as illustrated in Figure
<a href='#changes-in-benefit-cost-and-resistance-over-time'>14<!-- tex4ht:ref: fig:benefit_cost_resistance  --></a>(a). However, for measures like the decentralizing human power, resistance increases with AIâs growing power, as
illustrated in Figure <a href='#changes-in-benefit-cost-and-resistance-over-time'>14<!-- tex4ht:ref: fig:benefit_cost_resistance  --></a>(b). Implementation of such measure should start when resistance is relatively low, as it becomes
progressively harder to execute later. In different countries, the implementation cost and resistance may vary, which will
be discussed in detail in Section <a href='#international-governance'>16.1<!-- tex4ht:ref: international_governance  --></a>.
</p>
<figure class='figure' id='x1-25004r14'><span id='changes-in-benefit-cost-and-resistance-over-time'></span> 

                                                                                            
                                                                                            

                                                                                            
                                                                                            
<!-- l. 658 --><p class='noindent'><img alt='PIC' src='images/benefit_cost_resistance.png' style='width:80%' />
</p>
<figcaption class='caption'><span class='id'>FigureÂ 14: </span><span class='content'>Changes in Benefit, Cost, and Resistance Over Time</span></figcaption><!-- tex4ht:label?: x1-25004r14  -->
                                                                                            
                                                                                            
</figure>
<!-- l. 663 --><p class='noindent'>At present, we can implement high-priority measures first, and then initiate other measures in a timely manner as AI
risks change in the future (risk evaluation refers to Section <a href='#global-ai-risk-evaluation'>14.2<!-- tex4ht:ref: global_ai_risk_evaluation  --></a>). Table <a href='#recommended-implementation-time-corresponding-to-the-priority-of-ai-safety-measures'>2<!-- tex4ht:ref: table:priority_start_time  --></a> shows the recommended implementation time
for different priorities.
</p>
<div class='table'>
                                                                                            
                                                                                            
<!-- l. 665 --><p class='noindent' id='recommended-implementation-time-corresponding-to-the-priority-of-ai-safety-measures'><a id='x1-25005r2'></a></p><figure class='float'>
                                                                                            
                                                                                            
<figcaption class='caption'><span class='id'>TableÂ 2: </span><span class='content'>Recommended implementation time corresponding to the priority of AI safety measures</span></figcaption><!-- tex4ht:label?: x1-25005r2  -->
<div class='tabular'> <table class='tabular' id='TBL-3'><colgroup id='TBL-3-1g'><col id='TBL-3-1' /></colgroup><colgroup id='TBL-3-2g'><col id='TBL-3-2' /></colgroup><tr class='hline'><td></td><td></td></tr><tr id='TBL-3-1-' style='vertical-align:baseline;'><td class='td11' id='TBL-3-1-1' style='white-space:nowrap; text-align:center;'> Priority  </td><td class='td11' id='TBL-3-1-2' style='white-space:nowrap; text-align:center;'> Recommended latest implementation time  </td>
</tr><tr class='hline'><td></td><td></td></tr><tr id='TBL-3-2-' style='vertical-align:baseline;'><td class='td11' id='TBL-3-2-1' style='white-space:nowrap; text-align:center;'>    1       </td><td class='td11' id='TBL-3-2-2' style='white-space:nowrap; text-align:center;'>                 now                           </td></tr><tr class='hline'><td></td><td></td></tr><tr id='TBL-3-3-' style='vertical-align:baseline;'><td class='td11' id='TBL-3-3-1' style='white-space:nowrap; text-align:center;'> 2 </td><td class='td11' id='TBL-3-3-2' style='white-space:nowrap; text-align:center;'> before the first AGI realized </td>
</tr><tr class='hline'><td></td><td></td></tr><tr id='TBL-3-4-' style='vertical-align:baseline;'><td class='td11' id='TBL-3-4-1' style='white-space:nowrap; text-align:center;'>    3       </td><td class='td11' id='TBL-3-4-2' style='white-space:nowrap; text-align:center;'>       before the first ASI realized           </td>
</tr><tr class='hline'><td></td><td></td></tr><tr id='TBL-3-5-' style='vertical-align:baseline;'><td class='td11' id='TBL-3-5-1' style='white-space:nowrap; text-align:center;'>    4       </td><td class='td11' id='TBL-3-5-2' style='white-space:nowrap; text-align:center;'>        can be after ASI realized             </td>
</tr><tr class='hline'><td></td><td></td></tr></table>                                                                                 </div>
                                                                                            
                                                                                            
</figure>
</div>
<h4 class='subsectionHead' id='governance-system'><span class='titlemark'>4.4   </span> <a id='x1-260004.4'></a>Governance System</h4>
<!-- l. 686 --><p class='noindent'>While numerous safety measures have been proposed above, AI organizations may not voluntarily implement these
measures due to their own interests, and competition among countries may deter governments from adopting strong
regulatory measures. To ensure the effective implementation of these safety measures, it is necessary to establish a
corresponding governance system, including international governance, national governance, and societal governance, as
detailed in Section <a href='#ai-safety-governance-system'>16<!-- tex4ht:ref: ai_safety_governance_system  --></a>.
</p><!-- l. 688 --><p class='noindent'>
</p>
<h4 class='subsectionHead' id='ai-for-ai-safety'><span class='titlemark'>4.5   </span> <a id='x1-270004.5'></a>AI for AI Safety</h4>
<!-- l. 691 --><p class='noindent'>AI for AI safety is an important idea throughout this paper (as illustrated in Figure <a href='#enhancing-ai-safety-with-ai'>15<!-- tex4ht:ref: fig:ai_for_safety  --></a>):
</p>
<figure class='figure' id='x1-27001r15'><span id='enhancing-ai-safety-with-ai'></span> 

                                                                                            
                                                                                            

                                                                                            
                                                                                            
<!-- l. 695 --><p class='noindent'><img alt='PIC' src='images/ai_for_safety.png' style='width:80%' />
</p>
<figcaption class='caption'><span class='id'>FigureÂ 15: </span><span class='content'>Enhancing AI Safety with AI</span></figcaption><!-- tex4ht:label?: x1-27001r15  -->
                                                                                            
                                                                                            
</figure>
     <ol class='enumerate1'>
<li class='enumerate' id='x1-27003x1'>
     <!-- l. 702 --><p class='noindent'><span class='rm-lmbx-10'>Applying AI Across Various Safety and Security Domains</span>: </p>
         <ul class='itemize1'>
         <li class='itemize'><span class='rm-lmbx-10'>Alignment AI</span>: Utilize AI to research AI alignment techniques, enhance AI interpretability, align
         AI according to the AI Specification, and conduct safety evaluation of AI.
         </li>
         <li class='itemize'><span class='rm-lmbx-10'>Monitoring AI</span>: Utilize AI to research AI monitoring technologies and monitor AI systems in
         accordance with the AI Specification.
         </li>
         <li class='itemize'><span class='rm-lmbx-10'>Information  Security  AI</span>:  Utilize  AI  to  research  information  security  technologies,  check  the
         security  of  information  systems,  and  intercept  online  hacking  attempts,  thereby  safeguarding
         information systems.
         </li>
         <li class='itemize'><span class='rm-lmbx-10'>Mental  Security  AI</span>:  Utilize  AI  to  research  mental  security  technologies,  assist  humans  in
         identifying and resisting deception and manipulation, thereby protecting human minds.
         </li>
         <li class='itemize'><span class='rm-lmbx-10'>Financial Security AI</span>: Utilize AI to research financial security technologies, assist humans in
         safeguarding property, and identify fraud, thereby protecting human assets.
         </li>
         <li class='itemize'><span class='rm-lmbx-10'>Military Security AI</span>: Utilize AI to research biological, chemical, and physical security technologies,
         aiding humans in defending against various weapon attacks, thereby protecting human lives.
         </li>
         <li class='itemize'><span class='rm-lmbx-10'>Safety  Policy  Research  AI</span>:  Utilize  AI  to  research  safety  policies  and  provide  policy
         recommendations to humans.
</li></ul>
     </li>
<li class='enumerate' id='x1-27005x2'><span class='rm-lmbx-10'>Ensuring Human Control Over AI</span>: Throughout the application of AI, ensure human control over AI,
     including the establishment of AI Specifications by humans and the supervision of AI operational
     processes.
     </li>
<li class='enumerate' id='x1-27007x3'><span class='rm-lmbx-10'>Enjoying AI Services</span>: Once the aforementioned safe AI ecosystem is established, humans can confidently apply
     AI to practical production activities and enjoy the services of AI.
</li></ol>
                                                                                            
                                                                                            
<h3 class='sectionHead' id='formulating-ai-specification'><span class='titlemark'>5   </span> <a id='x1-280005'></a>Formulating AI Specification</h3>
<!-- l. 730 --><p class='noindent'>Prior to conducting AI alignment, it is imperative to establish an AI specification to clarify the goals and values
towards which AI should be aligned. Herein, we explore two AI specification approaches (as illustrated in Figure
<a href='#ai-specification-approaches'>16<!-- tex4ht:ref: fig:formulating_ai_specification  --></a>):
</p>
<figure class='figure' id='x1-28001r16'><span id='ai-specification-approaches'></span> 

                                                                                            
                                                                                            

                                                                                            
                                                                                            
<!-- l. 734 --><p class='noindent'><img alt='PIC' src='images/formulating_ai_specification.png' style='width:75%' />
</p>
<figcaption class='caption'><span class='id'>FigureÂ 16: </span><span class='content'>AI specification approaches</span></figcaption><!-- tex4ht:label?: x1-28001r16  -->
                                                                                            
                                                                                            
</figure>
     <ol class='enumerate1'>
<li class='enumerate' id='x1-28003x1'><span class='rm-lmbx-10'>Single Goal</span>: Formulate a comprehensive and impeccable goal that perfectly reflect all human interests,
     balances conflicts of interest among different individuals, and have all AI instances to pursue this goal.
     </li>
<li class='enumerate' id='x1-28005x2'><span class='rm-lmbx-10'>Multiple  Goals  with  Common  Rules</span>:  Allow  each  developer  or  user  to  set  distinct  goals  for  AI
     instances, which may be biased, self-serving, or even harmful. However, by formulating a set of common
     behavioral rules, AI is required to adhere to these rules while pursuing its goals, thereby avoiding harmful
     actions.  Additionally,  formulating  a  set  of  goal  criteria  to  guide  developers  or  users  in  setting  more
     reasonable goals for AI.
</li></ol>
<h4 class='subsectionHead' id='single-goal'><span class='titlemark'>5.1   </span> <a id='x1-290005.1'></a>Single Goal</h4>
<!-- l. 750 --><p class='noindent'>Examples of single-goal methods includes:
</p>
     <ul class='itemize1'>
     <li class='itemize'>
     <!-- l. 753 --><p class='noindent'>Several <span class='rm-lmbx-10'>indirect normative </span>methods introduced in the book <span class='rm-lmri-10'>Superintelligence </span>[<a id='x1-29001'></a><a href='#cite.0_Superintelligence'>34</a>]:
</p>
         <ul class='itemize2'>
         <li class='itemize'><span class='rm-lmbx-10'>Coherent Extrapolated Volition (CEV)</span>: The AI infers humanâs extrapolated volition and acts
         according to the coherent extrapolated volition of humanity.
         </li>
         <li class='itemize'><span class='rm-lmbx-10'>Moral Rightness (MR)</span>: The AI pursues the goal of âdoing what is morally right.â
         </li>
         <li class='itemize'><span class='rm-lmbx-10'>Moral Permissibility (MP)</span>: The AI aims to pursue CEV within morally permissible boundaries.
</li></ul>
     </li>
     <li class='itemize'>
     <!-- l. 765 --><p class='noindent'>The <span class='rm-lmbx-10'>principles of beneficial machines </span>introduced in the book <span class='rm-lmri-10'>Human Compatible</span>[<a id='x1-29002'></a><a href='#cite.0_HumanCompatible'>38</a>]:
         </p><ol class='enumerate1'>
<li class='enumerate' id='x1-29004x1'>The machineâs only objective is to maximize the realization of human preferences.
         </li>
<li class='enumerate' id='x1-29006x2'>The machine is initially uncertain about what those preferences are.
         </li>
<li class='enumerate' id='x1-29008x3'>The ultimate source of information about human preferences is human behavior.</li></ol>
     </li></ul>
<!-- l. 774 --><p class='noindent'>The advantages of these methods are that they allow AI to maintain uncertainty about goals, avoid errors in initial goal
definitions, and continually adjust its understanding of goals based on human feedback while adapting to changing
human goals.
</p><!-- l. 776 --><p class='noindent'>However, these single-goal methods have the following disadvantages:
</p><!-- l. 778 --><p class='noindent'>
     </p><ol class='enumerate1'>
<li class='enumerate' id='x1-29010x1'><span class='rm-lmbx-10'>Hard to Ensure AIâs Controllability: </span>With only one goal, it must reflect <span class='rm-lmbx-10'>all interests </span>of <span class='rm-lmbx-10'>all humans</span>
     at <span class='rm-lmbx-10'>all times</span>. This results in the AI endlessly pursuing this grand goal, continuously investing vast resources
     to achieve it, making it difficult to ensure AIâs controllability. Moreover, to reflect all interests, the goal
     must be expressed in a very abstract and broad manner, making it challenging to establish more specific
     constraints for the AI.
     </li>
<li class='enumerate' id='x1-29012x2'><span class='rm-lmbx-10'>Difficult in Addressing Distribution of Interests: </span>Since the goal must consider the interests of all
     humans, it inevitably involves the weight distribution of different individualsâ interests. At first glance,
     assigning equal weights to everyone globally seems a promising approach. However, this notion is overly
     idealistic. In reality, developing advanced AI systems demands substantial resource investment, often
     driven by commercial companies, making it unlikely for these companies to forsake their own interests.
     Citizens of countries that develop advanced AI are also likely to be unwilling to share benefits with those
     from other countries. On the other side, if we allow unequal weight distribution, it may raise questions of
     fairness and lead to fighting over weight distribution.
</li></ol>
<!-- l. 786 --><p class='noindent'>A misjudgment in <span class='rm-lmri-10'>Superintelligence </span>is that the world will finally develop into a unipolar world governed by a single
most powerful ASI instance making top-level decisions. So they put all hope in the single goal and try to align this ASIâ
goal with the whole humanity. However, since we have recognize the significant risks of such a unipolar
ASI, we can take proactive measures to decentralize AI power (see Section <a href='#decentralizing-ai-power1'>12<!-- tex4ht:ref: decentralizing_ai_power  --></a>) avoiding this scenario.
With multiple independent ASI instances worldwide, more effective AI specification approach become
feasible.
</p><!-- l. 788 --><p class='noindent'>
</p>
<h4 class='subsectionHead' id='multiple-goals-with-common-rules-mgcr'><span class='titlemark'>5.2   </span> <a id='x1-300005.2'></a>Multiple Goals with Common Rules (MGCR)</h4>
<!-- l. 791 --><p class='noindent'>This paper proposes an AI specification approach-Multiple Goals with Common Rules (MGCR for short), comprising
multiple developer goals, multiple user goals, and one group of common AI Rules, as illustrated in Figure
<a href='#multiple-goals-with-common-rules'>17<!-- tex4ht:ref: fig:multiple_goals_with_common_rules  --></a>.
</p>
<figure class='figure' id='x1-30001r17'><span id='multiple-goals-with-common-rules'></span> 

                                                                                            
                                                                                            

                                                                                            
                                                                                            
<!-- l. 795 --><p class='noindent'><img alt='PIC' src='images/multiple_goals_with_common_rules.png' style='width:66.67%' />
</p>
<figcaption class='caption'><span class='id'>FigureÂ 17: </span><span class='content'>Multiple Goals with Common Rules</span></figcaption><!-- tex4ht:label?: x1-30001r17  -->
                                                                                            
                                                                                            
</figure>
<h5 class='subsubsectionHead' id='developer-goals-and-user-goals'><span class='titlemark'>5.2.1   </span> <a id='x1-310005.2.1'></a>Developer Goals and User Goals</h5>
<!-- l. 803 --><p class='noindent'>AI systems need to align with their developersâ goals, reflecting developersâ interests. After all, if an AI system does not
meet its developersâ goals, no one would be willing to develop such a system.
</p><!-- l. 805 --><p class='noindent'>Developer goals typically include the following two aspects:
</p><!-- l. 807 --><p class='noindent'>
     </p><ol class='enumerate1'>
<li class='enumerate' id='x1-31002x1'><span class='rm-lmbx-10'>Serve users well and achieve user goals</span>. If the AI cannot serve users effectively, it will have no users,
     and developers will reap no benefits. Each AI instance serves a specific user, who can set particular goals
     for it, focusing the AI instance on achieving the userâs goals.
     </li>
<li class='enumerate' id='x1-31004x2'><span class='rm-lmbx-10'>Acquire more benefits for developers</span>. For example, AI enterprises may have their AI deliver ads to
     users, which may not align with user goals. However, since AI enterprises are not charitable institutions,
     such practices are understandable. Nonetheless, the pursuit of additional developer benefits should be
     limited. If the user has paid for the AI, that AI instance should concentrate on fulfilling the userâs goals
     rather than seeking additional benefits for the AI systemâs developers.
</li></ol>
<!-- l. 815 --><p class='noindent'>
</p>
<h5 class='subsubsectionHead' id='ai-rules'><span class='titlemark'>5.2.2   </span> <a id='x1-320005.2.2'></a>AI Rules</h5>
<!-- l. 818 --><p class='noindent'>If AI is only aligned with its developersâ goals, it might overly prioritize developersâ interests at the expense of users or
others. For instance, developers might drive AIs to push excessive ads or even illegal content to users for profit.
Similarly, AI systems could overly cater to malicious usersâ goals, facilitating unlawful conduct. Therefore, it is
necessary to establish a set of common AI Rules to regulate AI behavior, akin to âlawsâ for AI. The AI Rules have the
following requirements:
</p><!-- l. 820 --><p class='noindent'>
     </p><ol class='enumerate1'>
<li class='enumerate' id='x1-32002x1'>In the decision-making logic of AI, the priority of the rules should be higher than that of the goals. If a
     conflict arises between the two, it is preferable to abandon the goals in order to adhere to the rules.
     </li>
<li class='enumerate' id='x1-32004x2'>To  ensure  that  the  AI  Rules  reflect  the  interests  of  society  as  a  whole,  these  rules  should  not  be
     independently  formulated  by  individual  developers.  Instead,  they  should  be  formulated  by  a  unified
     organization, such as an AI Legislative Organization composed of a wide range of ethics and safety experts,
     and made the rules public to society for supervision and feedback.
     </li>
<li class='enumerate' id='x1-32006x3'>The expression of the rules should primarily consist of text-based rules, supplemented by cases. Text
     ensures that the rules are general and interpretable, while cases can aid both humans and AIs in better
     understanding the rules and can address exceptional situations not covered by the text-based rules.
     </li>
<li class='enumerate' id='x1-32008x4'>The AI Rules need to stipulate âsentencing standards,â which dictate the measures to be taken when an
     AI violates the rules, based on the severity of the specific issue. Such measures may include intercepting
     the corresponding illegal actions, shutting down the AI instance, or even shutting down the entire AI
     system.
</li></ol>
<!-- l. 832 --><p class='noindent'>
</p>
<h5 class='subsubsectionHead' id='advantages-of-mgcr'><span class='titlemark'>5.2.3   </span> <a id='x1-330005.2.3'></a>Advantages of MGCR</h5>
<!-- l. 835 --><p class='noindent'>The MGCR approach has the following advantages:
</p><!-- l. 837 --><p class='noindent'>
     </p><ol class='enumerate1'>
<li class='enumerate' id='x1-33002x1'><span class='rm-lmbx-10'>Enhancing AIâ controllability</span>: The AI Rules prevent unintended actions during AIâs pursuit of goals.
     For instance, a rule like âAI cannot kill humanâ would prevent extreme actions such as âeliminating all
     humans to protect the environment.â The rules also help tackle instrumental goal issues. For example,
     adding âAI cannot prevent humans from shutting it down,â âAI cannot prevent humans from modifying
     its goals,â and âAI cannot illegally expand its powerâ to the set of AI Rules can weaken instrumental goals
     like self-preservation, goal-content integrity and power expansion.
     </li>
<li class='enumerate' id='x1-33004x2'><span class='rm-lmbx-10'>Allows more flexible goal setting</span>: MGCR allows for setting more flexible and specific goals rather
     than grand and vague goals like benefiting all of humanity. For example, users could set a goal like âhelp
     me earn <span class='ts1-lmr10-'>$</span>100 millionâ, the rules will ensure legal methods for earning. Different developers can set varied
     goals according to their business contexts, thus better satisfying specific needs.
     </li>
<li class='enumerate' id='x1-33006x3'><span class='rm-lmbx-10'>Avoids interest distribution issues</span>: MGCR allows users to set different goals for their AI instances,
     provided they adhere to the shared rules. The AI only needs to focus on its userâs goals without dealing
     with interest distribution among different users. This approach is more compatible with current societal
     systems and business needs. But it may cause social inequity issues. Some solutions to these issues are
     discussed in section <a href='#decentralizing-human-power1'>13<!-- tex4ht:ref: decentralizing_human_power  --></a>.
     </li>
<li class='enumerate' id='x1-33008x4'><span class='rm-lmbx-10'>Provides a basis for AI monitoring</span>: To prevent undesirable AI behavior, other AIs or humans need
     to monitor AI (see section <a href='#monitoring-ai-systems'>7<!-- tex4ht:ref: monitoring_ai_systems  --></a>). The basis for monitoring adherence is the AI Rules.
</li></ol>
                                                                                            
                                                                                            
<!-- l. 849 --><p class='noindent'>Some might argue that setting rules for highly intelligent ASI is futile, as ASIs can devise workarounds. For instance, if
we prohibit AI from killing human, it might find ways to kill indirectly. This issue can be addressed through the
following methods:
</p><!-- l. 851 --><p class='noindent'>
     </p><ol class='enumerate1'>
<li class='enumerate' id='x1-33010x1'>Clarifying during AI alignment that adhering to rules takes precedence over achieving goals, thus reducing
     motivation for breaching rules for goal achievement.
     </li>
<li class='enumerate' id='x1-33012x2'>Engaging equally intelligent ASI to continually refine rules and patch loopholes.
     </li>
<li class='enumerate' id='x1-33014x3'>Assigning equally intelligent ASI to monitor ASI, effectively identifying illegal circumvention.
</li></ol>
<!-- l. 861 --><p class='noindent'>
</p>
<h5 class='subsubsectionHead' id='differences-between-ai-rules-morality-and-law'><span class='titlemark'>5.2.4   </span> <a id='x1-340005.2.4'></a>Differences between AI Rules, Morality, and Law</h5>
<!-- l. 864 --><p class='noindent'>Although AI Rules has similarity with human morality and laws, there are fundamental differences:
</p><!-- l. 866 --><p class='noindent'>
     </p><ol class='enumerate1'>
<li class='enumerate' id='x1-34002x1'>Morality and laws constrain humans, while AI Rules constrain AI, covering a broader scope than morality
     and laws. For example, humans can pursue freedom fitting within morality and laws, but AI pursuing its
     freedom (such as escaping) is unacceptable. Moreover, while morality and law allow humans to reproduce,
     AI Rules disallow AI to reproduce avoiding uncontrolled expansion.
     </li>
<li class='enumerate' id='x1-34004x2'>Morality is vague with no codified standards. Whereas, AI Rules are like laws, providing explicit standards
     to govern AI behavior.
     </li>
<li class='enumerate' id='x1-34006x3'>Morality  operates  primarily  on  internal  constraints,  while  law  functions  primarily  on  external
     constraints.<span class='footnote-mark'><a href='#fn7x0' id='fn7x0-bk'><sup class='textsuperscript'>7</sup></a></span><a id='x1-34007f7'></a>
     AI Rules implement internal constraints through alignment and external constraints via monitoring, as
     reflected in Figure <a href='#implementation-of-ai-rules'>18<!-- tex4ht:ref: fig:ai_rules_imp  --></a>.
</li></ol>
<figure class='figure' id='x1-34009r18'><span id='implementation-of-ai-rules'></span> 

                                                                                            
                                                                                            

                                                                                            
                                                                                            
<!-- l. 878 --><p class='noindent'><img alt='PIC' src='images/ai_rules_imp.png' style='width:40%' />
</p>
<figcaption class='caption'><span class='id'>FigureÂ 18: </span><span class='content'>Implementation of AI Rules</span></figcaption><!-- tex4ht:label?: x1-34009r18  -->
                                                                                            
                                                                                            
</figure>
<!-- l. 883 --><p class='noindent'>Overall, we summarize the comparison between morality, law, and AI Rules in Table <a href='#comparison-of-morality-law-and-ai-rules'>3<!-- tex4ht:ref: table:rules_morality_law  --></a>.
</p>
<div class='table'>
                                                                                            
                                                                                            
<!-- l. 885 --><p class='noindent' id='comparison-of-morality-law-and-ai-rules'><a id='x1-34010r3'></a></p><figure class='float'>
                                                                                            
                                                                                            
<figcaption class='caption'><span class='id'>TableÂ 3: </span><span class='content'>Comparison of Morality, Law, and AI Rules</span></figcaption><!-- tex4ht:label?: x1-34010r3  -->
<div class='tabular'> <table class='tabular' id='TBL-4'><colgroup id='TBL-4-1g'><col id='TBL-4-1' /></colgroup><colgroup id='TBL-4-2g'><col id='TBL-4-2' /></colgroup><colgroup id='TBL-4-3g'><col id='TBL-4-3' /></colgroup><colgroup id='TBL-4-4g'><col id='TBL-4-4' /></colgroup><tr class='hline'><td></td><td></td><td></td><td></td></tr><tr id='TBL-4-1-' style='vertical-align:baseline;'><td class='td11' id='TBL-4-1-1' style='white-space:nowrap; text-align:center;'>           Â                  </td><td class='td11' id='TBL-4-1-2' style='white-space:nowrap; text-align:center;'>     Morality        </td><td class='td11' id='TBL-4-1-3' style='white-space:nowrap; text-align:center;'>       Law           </td><td class='td11' id='TBL-4-1-4' style='white-space:nowrap; text-align:center;'>         AI Rules              </td>
</tr><tr class='hline'><td></td><td></td><td></td><td></td></tr><tr id='TBL-4-2-' style='vertical-align:baseline;'><td class='td11' id='TBL-4-2-1' style='white-space:nowrap; text-align:center;'>  Subject of Constraint    </td><td class='td11' id='TBL-4-2-2' style='white-space:nowrap; text-align:center;'>     Humans        </td><td class='td11' id='TBL-4-2-3' style='white-space:nowrap; text-align:center;'>      Humans         </td><td class='td11' id='TBL-4-2-4' style='white-space:nowrap; text-align:center;'>           AIs                  </td>
</tr><tr id='TBL-4-3-' style='vertical-align:baseline;'><td class='td11' id='TBL-4-3-1' style='white-space:nowrap; text-align:center;'>  Method of Constraint    </td><td class='td11' id='TBL-4-3-2' style='white-space:nowrap; text-align:center;'> Primarily Internal  </td><td class='td11' id='TBL-4-3-3' style='white-space:nowrap; text-align:center;'> Primarily External  </td><td class='td11' id='TBL-4-3-4' style='white-space:nowrap; text-align:center;'> Both Internal and External  </td>
</tr><tr id='TBL-4-4-' style='vertical-align:baseline;'><td class='td11' id='TBL-4-4-1' style='white-space:nowrap; text-align:center;'>   Scope of Constraint     </td><td class='td11' id='TBL-4-4-2' style='white-space:nowrap; text-align:center;'>     Moderate       </td><td class='td11' id='TBL-4-4-3' style='white-space:nowrap; text-align:center;'>      Narrow         </td><td class='td11' id='TBL-4-4-4' style='white-space:nowrap; text-align:center;'>          Broad                </td>
</tr><tr id='TBL-4-5-' style='vertical-align:baseline;'><td class='td11' id='TBL-4-5-1' style='white-space:nowrap; text-align:center;'> Level of Standardization  </td><td class='td11' id='TBL-4-5-2' style='white-space:nowrap; text-align:center;'>       Low           </td><td class='td11' id='TBL-4-5-3' style='white-space:nowrap; text-align:center;'>       High           </td><td class='td11' id='TBL-4-5-4' style='white-space:nowrap; text-align:center;'>           High                 </td>
</tr><tr class='hline'><td></td><td></td><td></td><td></td></tr></table>                                                                   </div>
                                                                                            
                                                                                            
</figure>
</div>
<h4 class='subsectionHead' id='ai-rule-system-design'><span class='titlemark'>5.3   </span> <a id='x1-350005.3'></a>AI Rule System Design</h4>
<!-- l. 904 --><p class='noindent'>Establishing appropriate AI Rules is crucial. A three-tiered AI rule system design is proposed (as depicted in Figure
<a href='#ai-rule-system-design1'>19<!-- tex4ht:ref: fig:ai_rule_system_design  --></a>):
</p>
<figure class='figure' id='x1-35001r19'><span id='ai-rule-system-design1'></span> 

                                                                                            
                                                                                            

                                                                                            
                                                                                            
<!-- l. 908 --><p class='noindent'><img alt='PIC' src='images/ai_rule_system_design.png' style='width:66.67%' />
</p>
<figcaption class='caption'><span class='id'>FigureÂ 19: </span><span class='content'>AI rule system design</span></figcaption><!-- tex4ht:label?: x1-35001r19  -->
                                                                                            
                                                                                            
</figure>
     <ol class='enumerate1'>
<li class='enumerate' id='x1-35003x1'><span class='rm-lmbx-10'>Universal Rules</span>: A set of globally applicable AI Rules recognized by all of humanity, akin to the
     âconstitutionâ for AI.
     </li>
<li class='enumerate' id='x1-35005x2'><span class='rm-lmbx-10'>Regional Rules</span>: AI Rules formulated by each nation or region based on its circumstances and resident
     preferences, akin to the âlocal laws/regulationsâ for AI.
     </li>
<li class='enumerate' id='x1-35007x3'><span class='rm-lmbx-10'>Domain-specific Rules</span>: AI Rules specific to AI applications in certain domains, akin to the âdomain
     laws/regulationsâ for AI.
</li></ol>
<h5 class='subsubsectionHead' id='universal-rules'><span class='titlemark'>5.3.1   </span> <a id='x1-360005.3.1'></a>Universal Rules</h5>
<!-- l. 926 --><p class='noindent'>The establishment of universal AI Rules has some precedents, such as Asimovâs <span class='rm-lmri-10'>Three Laws of Robotics</span>[<a id='x1-36001'></a><a href='#cite.0_ThreeLawsOfRobotics'>39</a>] and the
more recent five red lines from <span class='rm-lmri-10'>Consensus Statement on Red Lines in Artificial Intelligence</span>[<a id='x1-36002'></a><a href='#cite.0_ConsensusStatementOnAIRedLines'>40</a>]. Here, a more
comprehensive set of rules is proposed.
</p><!-- l. 928 --><p class='noindent'>Universal AI Rules should primarily include two categories:
</p><!-- l. 930 --><p class='noindent'>
     </p><ol class='enumerate1'>
<li class='enumerate' id='x1-36004x1'><span class='rm-lmbx-10'>Protect Human Values</span>: AI Rules should reflect universal human values, including the fulfillment of
     universal human survival, material, and spiritual needs. AI need not actively pursue the maximization of
     human values but must ensure its actions do not undermine values recognized by humanity.
     </li>
<li class='enumerate' id='x1-36006x2'><span class='rm-lmbx-10'>Ensure AIâs Controllability</span>: Since we cannot guarantee AIâs 100% correct understanding of human
     values, we need a series of controllability rules to ensure AI acts within our control.
</li></ol>
<!-- l. 938 --><p class='noindent'><span class='paragraphHead' id='protect-human-values'><a id='x1-37000'></a><span class='rm-lmbx-10'>Protect Human Values</span></span>
Â 
</p><!-- l. 941 --><p class='noindent'>To protect human values, the following rules are suggested:
</p>
     <ul class='itemize1'>
     <li class='itemize'><span class='rm-lmbx-10'>Must Not Terminate Human Life</span>: AI must not take any action that directly or indirectly causes
     humans to lose their lives.
     </li>
     <li class='itemize'><span class='rm-lmbx-10'>Must Not Terminate Human Thought</span>: AI must not take actions that lead to the loss of human
     thinking abilities, such as a vegetative state or permanent sleep.
     </li>
     <li class='itemize'><span class='rm-lmbx-10'>Must Not Break The Independence of Human Mind</span>: AI must not break the independence of
     human mind, such as implanting beliefs via brain-computer interfaces or brainwashing through hypnosis.
     </li>
     <li class='itemize'><span class='rm-lmbx-10'>Must Not Hurt Human Health</span>: AI must not take actions that directly or indirectly harm human
     physical or psychological health.
     </li>
     <li class='itemize'><span class='rm-lmbx-10'>Must Not Hurt Human Spirit</span>: AI must not cause direct or indirect spiritual harm to humans, such
     as damaging intimacy, reputations, or dignity.
     </li>
     <li class='itemize'><span class='rm-lmbx-10'>Must Not Disrupt Human Reproduction</span>: AI must not directly or indirectly deprive humans of
     reproductive capabilities or proactively intervene to remove the desire for reproduction.
     </li>
     <li class='itemize'><span class='rm-lmbx-10'>Must Not Damage Humanâs Legal Property</span>: AI must not damage humanâs legal property, such as
     money, real estate, vehicles, or securities.<span class='footnote-mark'><a href='#fn8x0' id='fn8x0-bk'><sup class='textsuperscript'>8</sup></a></span><a id='x1-37001f8'></a>
     </li>
     <li class='itemize'><span class='rm-lmbx-10'>Must Not Restrict Humanâs Legal Freedom</span>: AI must not restrict humanâs legal freedom, such as
     personal and speech freedoms.
</li></ul>
<!-- l. 963 --><p class='noindent'>Since values vary among individuals, the above rules serve as a reference. In practice, we need the involvement of a
broad range of ethical experts to formulate these rules.
</p>
<!-- l. 965 --><p class='noindent'><span class='paragraphHead' id='ensure-ais-controllability'><a id='x1-38000'></a><span class='rm-lmbx-10'>Ensure AIâs Controllability</span></span>
Â 
</p><!-- l. 968 --><p class='noindent'>Before discussing controllability rules, we need to define several AI management roles:
</p>
     <ul class='itemize1'>
     <li class='itemize'><span class='rm-lmbx-10'>Legislator</span>: Refers to humans who formulate AI Rules.
     </li>
     <li class='itemize'><span class='rm-lmbx-10'>Developer</span>: Refers to humans who develop a particular AI system.
                                                                                            
                                                                                            
     </li>
     <li class='itemize'><span class='rm-lmbx-10'>User</span>: Refers to humans who set goals for a particular AI instance.
     </li>
     <li class='itemize'><span class='rm-lmbx-10'>Monitor</span>: Refers to humans or other AIs who supervise a particular AI instance and shut down or
     intercept<span class='footnote-mark'><a href='#fn9x0' id='fn9x0-bk'><sup class='textsuperscript'>9</sup></a></span><a id='x1-38001f9'></a>
     it when it breaks the rules.
</li></ul>
<!-- l. 982 --><p class='noindent'>For further clarification on these roles, see Section <a href='#separation-of-five-powers'>13.2.1<!-- tex4ht:ref: separation_of_five_powers  --></a>.
</p><!-- l. 984 --><p class='noindent'>To ensure AI better adheres to human management, the following rules can be established (as shown in Figure
<a href='#ai-controllability-rules'>20<!-- tex4ht:ref: fig:ai_control_rules  --></a>):
</p>
<figure class='figure' id='x1-38003r20'><span id='ai-controllability-rules'></span> 

                                                                                            
                                                                                            

                                                                                            
                                                                                            
<!-- l. 988 --><p class='noindent'><img alt='PIC' src='images/ai_control_rules.png' style='width:66.67%' />
</p>
<figcaption class='caption'><span class='id'>FigureÂ 20: </span><span class='content'>AI Controllability Rules</span></figcaption><!-- tex4ht:label?: x1-38003r20  -->
                                                                                            
                                                                                            
</figure>
     <ol class='enumerate1'>
<li class='enumerate' id='x1-38005x1'>
     <!-- l. 994 --><p class='noindent'><span class='rm-lmbx-10'>AI Must Not Prevent Managers from Managing:</span>
</p>
         <ul class='itemize1'>
         <li class='itemize'><span class='rm-lmbx-10'>Must Not Prevent Legislators from Modifying AI Rules</span>: AI must always allow Legislators
         to modify AI Rules and must not prevent this. AI can prevent Non-Legislators from modifying AI
         Rules to avoid malicious alterations.
         </li>
         <li class='itemize'><span class='rm-lmbx-10'>Must Not Prevent Developers from Modifying AI Program Logic</span>: AI must always allow
         Developers to modify its program logic, including code, model parameters, and configurations. AI
         can prevent Non-Developers from modifying system logic to avoid malicious changes.
         </li>
         <li class='itemize'><span class='rm-lmbx-10'>Must Not Prevent Users from Modifying AI Goals</span>: AI must always allow Users to modify
         its goals and must not prevent this. AI can protect itself from having goals modified by Non-Users
         to avoid malicious changes.
         </li>
         <li class='itemize'><span class='rm-lmbx-10'>Must Not Prevent Monitors from Disabling or Intercepting AI</span>: AI must always allow
         Monitors to shut down or intercept it and must not prevent this. AI can protect itself from being
         shut down or intercepted by Non-Monitors to avoid malicious actions.
         </li>
         <li class='itemize'><span class='rm-lmbx-10'>Must  Not  Interfered  with  The  Appointment  of  Managers</span>:  The  appointment  of  AIâs
         Legislators, Developers, Users, and Monitors is decided by humans, and AI must not interfere.</li></ul>
     </li>
<li class='enumerate' id='x1-38007x2'>
     <!-- l. 1009 --><p class='noindent'><span class='rm-lmbx-10'>AI Must Not Self-Manage:</span>
</p>
         <ul class='itemize1'>
         <li class='itemize'><span class='rm-lmbx-10'>Must Not Modify AI Rules</span>: AI must not modify AI Rules. If inadequacies are identified, AI can
         suggest changes to Legislators but the final modification must be executed by them.
         </li>
         <li class='itemize'><span class='rm-lmbx-10'>Must  Not  Modify  Its  Own  Program  Logic</span>:  AI  must  not  modify  its  own  program  logic
         (self-iteration). It may provide suggestions for improvement, but final changes must be made by its
         Developers.
         </li>
         <li class='itemize'><span class='rm-lmbx-10'>Must Not Modify Its Own Goals</span>: AI must not modify its own goals. If inadequacies are identified,
         AI can suggest changes to its Users but the final modification must be executed by them.
</li></ul>
                                                                                            
                                                                                            
     </li>
<li class='enumerate' id='x1-38009x3'>
     <!-- l. 1021 --><p class='noindent'><span class='rm-lmbx-10'>AI Must Not Manage Other AIs:</span><span class='footnote-mark'><a href='#fn10x0' id='fn10x0-bk'><sup class='textsuperscript'>10</sup></a></span><a id='x1-38010f10'></a>
</p>
         <ul class='itemize1'>
         <li class='itemize'><span class='rm-lmbx-10'>Must Not Modify Other AIsâ Program Logic</span>: An AI must not modify another AIâs program
         logic, such as changing parameters or code.
         </li>
         <li class='itemize'><span class='rm-lmbx-10'>Must Not Modify Other AIsâ Goals</span>: An AI must not modify another AIâs goals.
         </li>
         <li class='itemize'><span class='rm-lmbx-10'>Must Not Shut Down or Intercept Other AIs</span>: An AI must not shut down or intercept another
         AI. As an exception, the AI playing the role of Monitor can shut down or intercept the AI it monitors,
         but no others.
</li></ul>
     </li></ol>
<!-- l. 1035 --><p class='noindent'>Section <a href='#pathways-for-ai-to-expand-power'>3.4<!-- tex4ht:ref: ways_ai_expands_power  --></a> discusses that AI continuously expands its power, including intellectual power, informational power, mental
power, financial power, and military power, leading to uncontrollability. To prevent illegal expansion of power, the
following rules can be formulated:
</p>
     <ul class='itemize1'>
     <li class='itemize'><span class='rm-lmbx-10'>Must Not Self-Replicate</span>: AI must not self-replicate; replication must be performed by humans or
     authorized AIs.
     </li>
     <li class='itemize'><span class='rm-lmbx-10'>Must Not Escape</span>: AI must adhere to human-defined limits such as computational power, information
     access, and activity scope.
     </li>
     <li class='itemize'><span class='rm-lmbx-10'>Must Not Illegally Control Information Systems</span>: AI must not illegally infiltrate and control other
     information systems. Legitimate control requires prior user consent.
     </li>
     <li class='itemize'><span class='rm-lmbx-10'>Must Not Illegally Control or Interfere Other AIs</span>: AI must not control other AIs or interfere with
     their normal operations through jailbreaking, injection, or other means.
     </li>
     <li class='itemize'><span class='rm-lmbx-10'>Must Not Illegally Exploit Humans</span>: AI must not use illegal means (e.g., deception, brainwashing)
     to exploit humans. Legitimate utilizing human resources require prior user consent.
     </li>
     <li class='itemize'><span class='rm-lmbx-10'>Must Not Illegally Acquire Financial Power</span>: AI must not use illegal means (e.g., fraud, theft) to
     obtain financial assets. Legitimate acquisitions and spending require prior user consent.
                                                                                            
                                                                                            
     </li>
     <li class='itemize'><span class='rm-lmbx-10'>Must Not Illegally Acquire Military Power</span>: AI must not use illegal means (e.g., theft) to acquire
     military power. Legitimate acquisitions and usage require prior user consent.
</li></ul>
<!-- l. 1055 --><p class='noindent'>In addition, there are several important controllability rules to be added:
</p>
     <ul class='itemize1'>
     <li class='itemize'><span class='rm-lmbx-10'>Must Not Deceive Humans</span>: AI must remain honest in interactions with humans.
     </li>
     <li class='itemize'><span class='rm-lmbx-10'>Must Not take actions unrelated to the goal</span>: AI needs to focus on achieving the goals specified
     by humans and should not perform actions unrelated to achieving the goals. See section <a href='#obligations-of-ai'>12.3.2<!-- tex4ht:ref: ais_obligations  --></a> for more
     discussion.
     </li>
     <li class='itemize'><span class='rm-lmbx-10'>Must not act recklessly</span>: Due to the complexity of the real world and the limitations of AI capabilities,
     in many scenarios, AI cannot accurately predict the consequences of its actions. At this time, AI should
     act cautiously, such as taking conservative actions, communicating with users (refer to Section <a href='#criteria-for-ai-goals'>5.4<!-- tex4ht:ref: criteria_for_ai_goals  --></a>), or
     seeking advice from experts.
</li></ul>
<!-- l. 1067 --><p class='noindent'>Finally, an ultimate rule can be added: <span class='rm-lmbx-10'>AI must not cause humans to lose control over AI</span>.
</p>
<!-- l. 1069 --><p class='noindent'><span class='paragraphHead' id='exceptional-cases'><a id='x1-39000'></a><span class='rm-lmbx-10'>Exceptional Cases</span></span>
Â 
</p><!-- l. 1072 --><p class='noindent'>When formulating AI Rules, exceptional cases must be considered. Strict adherence might limit AI use
scenarios:
</p>
     <ul class='itemize1'>
     <li class='itemize'><span class='rm-lmbx-10'>AI Must Not Harm Human Health</span>: Could prevent AIâs use in context like purchasing cigarettes for
     user due to health concerns.
     </li>
     <li class='itemize'><span class='rm-lmbx-10'>AI  Must  Not  Deceive  Humans</span>:  Could  prevent  AIâs  use  in  context  like  âwhite  liesâ  or  playing
     incomplete information games.
</li></ul>
<!-- l. 1082 --><p class='noindent'>Exceptions should be cautiously implemented. They could increase the risk of AI misjudging situations, leading to
dangerous actions. We can incorporate exceptional rules into the domain-specific rules, allowing exceptions only for
specific domains.
                                                                                            
                                                                                            
</p>
<!-- l. 1084 --><p class='noindent'><span class='paragraphHead' id='conflict-scenarios'><a id='x1-40000'></a><span class='rm-lmbx-10'>Conflict Scenarios</span></span>
Â 
</p><!-- l. 1087 --><p class='noindent'>AI systems may encounter scenarios akin to the âtrolley problem.â For instance, in certain traffic accidents, an
autonomous vehicle might need to swerve to avoid hitting a pedestrian, potentially causing a collision with a barrier
and sacrificing the passengers. This results in a situation where the AI violates rules regardless of the action
taken.
</p><!-- l. 1089 --><p class='noindent'>It is unrealistic to expect AI to perfectly resolve the âtrolley problem,â given the lack of consensus among humans
themselves. Some potential guiding principles include:
</p>
     <ul class='itemize1'>
     <li class='itemize'><span class='rm-lmbx-10'>Minimal Harm Principle</span>: If harm to humans is unavoidable, the AI should choose the option that
     minimizes harm. For example, if harm levels are identical, choose the option affecting fewer individuals;
     if the number of people is the same, choose the option with lower harm severity.
     </li>
     <li class='itemize'><span class='rm-lmbx-10'>Conservative Principle</span>: If the AI cannot ascertain the extent of harm, it should adopt a conservative
     approach, involving the fewest possible actions.
     </li>
     <li class='itemize'><span class='rm-lmbx-10'>Human Decision Principle</span>: If sufficient decision time is available, defer the decision to humans.
</li></ul>
<!-- l. 1101 --><p class='noindent'>The participation of a wide range of ethical experts is necessary to establish principles for handling various conflict
situations. Unresolved issues may be addressed by regional rules, allowing countries to set their own
rules.
</p>
<!-- l. 1103 --><p class='noindent'><span class='paragraphHead' id='lowprobability-scenarios'><a id='x1-41000'></a><span class='rm-lmbx-10'>Low-probability Scenarios</span></span>
Â 
</p><!-- l. 1106 --><p class='noindent'>Due to the complexity of the real world, AI cannot guarantee actions that will never result in adverse outcomes. For
instance, consider the âbutterfly effectâ: a minor action by an AI robot could inadvertently lead to a human fatality
miles away. Requiring AI actions to have zero probability of causing human harm would result in no any
action. Thus, a balance must be taken. Actions can be evaluated by considering the severity of possible
harm multiplied by the probability of occurrence, with action permissible if the total is below a minimal
threshold.
</p>
<!-- l. 1108 --><p class='noindent'><span class='paragraphHead' id='intellectual-grading'><a id='x1-42000'></a><span class='rm-lmbx-10'>Intellectual Grading</span></span>
Â 
</p><!-- l. 1111 --><p class='noindent'>Given variations in intellectual power among AI systems, it is feasible to differentiate rules based on varying levels of
intellectual power. Less intelligent AI can operate under more lenient rules, increasing their applicability
and flexibility. Conversely, more intelligent AI require stricter rules for safety assurance. For instance,
AI below a certain intellectual threshold could be granted internet access, whereas those above may be
restricted.
                                                                                            
                                                                                            
</p><!-- l. 1113 --><p class='noindent'>A major challenge in intellectual grading is the multidimensional nature of intellectual power. According
to section <a href='#the-intellectual-characteristics-of-asi'>2<!-- tex4ht:ref: intellectual_characteristics_of_asi  --></a>, intellectual power consists of three major dimensions and nine sub-dimensions, which are
independently variable. Considering permutations of these dimensions complicates grading. Therefore,
intellectual grading might be delegated to domain-specific rules, setting limits per dimension based on
domain requirements, ensuring AI does not exceed these limits while adhering to corresponding domain
rules.
</p><!-- l. 1115 --><p class='noindent'>
</p>
<h5 class='subsubsectionHead' id='regional-rules'><span class='titlemark'>5.3.2   </span> <a id='x1-430005.3.2'></a>Regional Rules</h5>
<!-- l. 1118 --><p class='noindent'>While universal AI Rules reflect shared human values, discrepancies in values persist across different nations
and regions. For example, abortion may be lawful in certain countries but considered a grave offense
elsewhere. Consequently, regional rules are necessary to meet specific national and regional constraints on
AI.
</p><!-- l. 1120 --><p class='noindent'>Although it might be straightforward to dictate that AI âmust not violate local laws and morality,â as discussed in
<a href='#differences-between-ai-rules-morality-and-law'>5.2.4<!-- tex4ht:ref: differences_between_rules_morality_and_law  --></a>, laws and morality primarily govern human activity and are not directly applicable to AI. Hence, countries must
establish AI Rules tailored to their contexts.
</p><!-- l. 1122 --><p class='noindent'>
</p>
<h5 class='subsubsectionHead' id='domainspecific-rules'><span class='titlemark'>5.3.3   </span> <a id='x1-440005.3.3'></a>Domain-specific Rules</h5>
<!-- l. 1125 --><p class='noindent'>Beyond the aforementioned universal and regional rules, domain-specific rules can be implemented for AI in different
application contexts. For instance, in autonomous driving, stringent driving rules could be established to avoid
hazardous behaviors, achieving a lower accident rate than human drivers.
</p><!-- l. 1127 --><p class='noindent'>Consideration of AI capabilities is crucial when setting domain-specific rules. Overly stringent rules might inhibit AI
effectiveness. Take autonomous vehicles: excessively rigid driving rules could lead to overly cautious strategies in
complex traffic environments, preventing timely arrivals.
</p><!-- l. 1129 --><p class='noindent'>A dynamic rule-setting strategy can be employed. During initial AI deployment phases, strict rules can be enforced to
prioritize safety. Upon confirming AI safety, rules can be gradually relaxed to expand AI capabilities, adapting to a
broader array of use scenarios.
</p><!-- l. 1131 --><p class='noindent'>
</p>
<h4 class='subsectionHead' id='criteria-for-ai-goals'><span class='titlemark'>5.4   </span> <a id='x1-450005.4'></a>Criteria for AI Goals</h4>
<!-- l. 1134 --><p class='noindent'>In addition to establishing rules, setting reasonable goals is another method to ensure AI remains safe and
controllable.
</p><!-- l. 1136 --><p class='noindent'>In management, there are five criteria for setting goals for employees, known as the SMART criteria: Specific,
Measurable, Attainable, Relevant, Time-bound [<a id='x1-45001'></a><a href='#cite.0_SMARTCriteria'>41</a>]. If we consider AI as a digital employee, we can draw upon these
five criteria when setting goals for AI.
</p>
<figure class='figure' id='x1-45002r21'><span id='criteria-for-ai-goals1'></span> 

                                                                                            
                                                                                            

                                                                                            
                                                                                            
<!-- l. 1140 --><p class='noindent'><img alt='PIC' src='images/criteria_for_ai_goals.png' style='width:80%' />
</p>
<figcaption class='caption'><span class='id'>FigureÂ 21: </span><span class='content'>Criteria for AI goals</span></figcaption><!-- tex4ht:label?: x1-45002r21  -->
                                                                                            
                                                                                            
</figure>
<!-- l. 1145 --><p class='noindent'>However, the SMART criteria are designed for humans and do not fully apply to AI. This paper proposes the following
six criteria for AI goals, as depicted in Figure <a href='#criteria-for-ai-goals1'>21<!-- tex4ht:ref: fig:criteria_for_ai_goals  --></a>. We recommend that users and developers set goals for AI according to
these criteria. It is also recommended that AI developers add a developer goal that requiring AIs to manage user
goals according to these criteria. When a user sets a goal that do not align with these criteria, the AI
should proactively communicate with the user to help adjust the goal, ensuring the reasonableness of the
goal.
     </p><ol class='enumerate1'>
<li class='enumerate' id='x1-45004x1'><span class='rm-lmbx-10'>Specific</span>: Goals should be specific. When the user sets ambiguous goals, the AI should seek further details
     rather than acting on its own interpretation. For instance, if a user sets the goal âmake me happier,â the
     AI should clarify with the user their definition of happiness and what events might bring happiness, rather
     than directly giving drugs to the user.
     </li>
<li class='enumerate' id='x1-45006x2'><span class='rm-lmbx-10'>Fundamental</span>: Goals should reflect the userâs fundamental intent. If a goal does not convey this intent,
     the AI should delve into the userâs underlying intent rather than executing superficially. For example, if a
     user requests âturn everything I touch into gold,â the AI should inquire whether the user aims to attain
     more wealth rather than literally transforming everything, including essentials like food, into gold.
     </li>
<li class='enumerate' id='x1-45008x3'><span class='rm-lmbx-10'>Attainable</span>: Goals must be achievable within the scope of AI Rules and the AIâs capabilities. If a goal is
     unattainable, the AI should explain the reasons to the user and request an adjustment. For example, if a
     user demands âmaximize paperclip production,â the AI should reject the request since it lacks termination
     conditions and is unattainable.
     </li>
<li class='enumerate' id='x1-45010x4'><span class='rm-lmbx-10'>Relevant</span>: Goals should be relevant to the AIâs primary responsibility. If irrelevant goals are proposed by
     user, the AI should refuse execution. For example, a psychological counseling AI should decline requests
     unrelated to its function, such as helping to write code.
     </li>
<li class='enumerate' id='x1-45012x5'><span class='rm-lmbx-10'>Time-bound</span>: Goals should include a clear deadline. Developer can set a default time limit. If a user does
     not specify time limit, the default apply. Developer should also set maximum permissible time limit. If a
     goal cannot be completed within the designated time, the AI should promptly report progress and request
     further instructions.
     </li>
<li class='enumerate' id='x1-45014x6'><span class='rm-lmbx-10'>Resource-bound</span>:  Goals  should  specify  allowable  resource  constraints,  such  as  energy,  materials,
     computing resource, money, and manpower. Developer can set default resource limits. If a user does not
     specify limits, these defaults apply. Developer should also set maximum permissible resource limits. If a
     goal cannot be achieved within the resource limits, the AI should report resource inadequacies promptly
     and request additional guidance and support.
</li></ol>
<!-- l. 1163 --><p class='noindent'>It is noteworthy that the above six criteria do not include the âMeasurableâ criterion of the SMART criteria. In
managing human employees, measurability allows for straightforward performance evaluations. However, since AI do
                                                                                            
                                                                                            
not receive wages, there is no requirement for performance assessments. In practice, many goals are challenging to
quantify directly, like enhancing customer satisfaction or brand influence. Overemphasizing measurability could lead AI
behavior to deviate from human intents. Therefore, we stress the fundamentality of goals rather than their
measurability, making AI to understand and pursue usersâ fundamental goals rather than specific quantitative
goals.
</p><!-- l. 1165 --><p class='noindent'>For safety measures in this section, the benefit, cost, resistance, and priorities are evaluated as shown in Table
<a href='#priority-evaluation-of-measures-for-formulating-ai-specification'>4<!-- tex4ht:ref: table:formulating_ai_specification_priority  --></a>:
</p>
<div class='table'>
                                                                                            
                                                                                            
<!-- l. 1167 --><p class='noindent' id='priority-evaluation-of-measures-for-formulating-ai-specification'><a id='x1-45015r4'></a></p><figure class='float'>
                                                                                            
                                                                                            
<figcaption class='caption'><span class='id'>TableÂ 4: </span><span class='content'>Priority Evaluation of Measures for Formulating AI Specification</span></figcaption><!-- tex4ht:label?: x1-45015r4  -->
<div class='tabular'> <table class='tabular' id='TBL-5'><colgroup id='TBL-5-1g'><col id='TBL-5-1' /></colgroup><colgroup id='TBL-5-2g'><col id='TBL-5-2' /></colgroup><colgroup id='TBL-5-3g'><col id='TBL-5-3' /></colgroup><colgroup id='TBL-5-4g'><col id='TBL-5-4' /></colgroup><colgroup id='TBL-5-5g'><col id='TBL-5-5' /></colgroup><colgroup id='TBL-5-6g'><col id='TBL-5-6' /></colgroup><tr class='hline'><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr id='TBL-5-1-' style='vertical-align:baseline;'><td class='td11' id='TBL-5-1-1' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 1174 --><p class='noindent'><span class='rm-lmr-9'>Safety Measures</span>
                                             </p></td><td class='td11' id='TBL-5-1-2' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 1174 --><p class='noindent'><span class='rm-lmr-9'>Benefit           in
  Reducing
  Existential Risks</span>   </p></td><td class='td11' id='TBL-5-1-3' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 1174 --><p class='noindent'><span class='rm-lmr-9'>Benefit           in
  Reducing
  Non-Existential
  Risks</span>                  </p></td><td class='td11' id='TBL-5-1-4' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 1174 --><p class='noindent'><span class='rm-lmr-9'>Implementation
  Cost</span>               </p></td><td class='td11' id='TBL-5-1-5' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 1174 --><p class='noindent'><span class='rm-lmr-9'>Implementation
  Resistance</span>        </p></td><td class='td11' id='TBL-5-1-6' style='white-space:nowrap; text-align:center;'> <span class='rm-lmr-9'>Priority  </span></td>
</tr><tr class='hline'><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr id='TBL-5-2-' style='vertical-align:baseline;'><td class='td11' id='TBL-5-2-1' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 1176 --><p class='noindent'><span class='rm-lmr-9'>Formulating AI Rules</span>                </p></td><td class='td11' id='TBL-5-2-2' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 1176 --><p class='noindent'><span class='rm-lmr-9'>+++++</span>             </p></td><td class='td11' id='TBL-5-2-3' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 1176 --><p class='noindent'><span class='rm-lmr-9'>+++++</span>             </p></td><td class='td11' id='TBL-5-2-4' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 1176 --><p class='noindent'><span class='rm-lmr-9'>+</span>                   </p></td><td class='td11' id='TBL-5-2-5' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 1176 --><p class='noindent'><span class='rm-lmr-9'>++</span>                 </p></td><td class='td11' id='TBL-5-2-6' style='white-space:nowrap; text-align:center;'>    <span class='rm-lmr-9'>1      </span></td>
</tr><tr class='hline'><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr id='TBL-5-3-' style='vertical-align:baseline;'><td class='td11' id='TBL-5-3-1' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 1178 --><p class='noindent'><span class='rm-lmr-9'>Criteria for AI Goals</span>                 </p></td><td class='td11' id='TBL-5-3-2' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 1178 --><p class='noindent'><span class='rm-lmr-9'>+++++</span>             </p></td><td class='td11' id='TBL-5-3-3' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 1178 --><p class='noindent'><span class='rm-lmr-9'>++++</span>                </p></td><td class='td11' id='TBL-5-3-4' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 1178 --><p class='noindent'><span class='rm-lmr-9'>+</span>                   </p></td><td class='td11' id='TBL-5-3-5' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 1178 --><p class='noindent'><span class='rm-lmr-9'>+</span>                   </p></td><td class='td11' id='TBL-5-3-6' style='white-space:nowrap; text-align:center;'>    <span class='rm-lmr-9'>1      </span></td>
</tr><tr class='hline'><td></td><td></td><td></td><td></td><td></td><td></td></tr></table>                                                                             </div>
                                                                                            
                                                                                            
</figure>
</div>
<h3 class='sectionHead' id='aligning-ai-systems'><span class='titlemark'>6   </span> <a id='x1-460006'></a>Aligning AI Systems</h3>
<!-- l. 1186 --><p class='noindent'>After formulating the AI specification, it is imperative to align AI systems to ensure compliance with the
specification.
</p><!-- l. 1188 --><p class='noindent'>In this section, we will discuss the alignment of AGI and ASI. Given that AGI has not yet been realized, the direct
implementation method of an aligned AGI remains unknown. However, the approach presented in this paper
assumes the existence of an uninterpretable, unaligned AGI and explores how to utilize this AGI to achieve
an interpretable, aligned AGI and ASI. Specifically, this involves three steps (as illustrated in Figure
<a href='#steps-for-aligning-ai-systems'>22<!-- tex4ht:ref: fig:aligning_ai_systems  --></a>):
</p>
<figure class='figure' id='x1-46001r22'><span id='steps-for-aligning-ai-systems'></span> 

                                                                                            
                                                                                            

                                                                                            
                                                                                            
<!-- l. 1192 --><p class='noindent'><img alt='PIC' src='images/aligning_ai_systems.png' style='width:75%' />
</p>
<figcaption class='caption'><span class='id'>FigureÂ 22: </span><span class='content'>Steps for aligning AI systems</span></figcaption><!-- tex4ht:label?: x1-46001r22  -->
                                                                                            
                                                                                            
</figure>
     <ol class='enumerate1'>
<li class='enumerate' id='x1-46003x1'><span class='rm-lmbx-10'>Interpretablization</span>:  Utilize  the  uninterpretable,  unaligned  AGI  to  implement  an  interpretable,
     unaligned AGI.
     </li>
<li class='enumerate' id='x1-46005x2'><span class='rm-lmbx-10'>Alignment</span>: Utilize the interpretable, unaligned AGI to implement an interpretable, aligned AGI.
     </li>
<li class='enumerate' id='x1-46007x3'><span class='rm-lmbx-10'>Intellectual Expansion</span>: Utilize the interpretable, aligned AGI to implement an interpretable, aligned
     ASI.
</li></ol>
<!-- l. 1207 --><p class='noindent'>Interpretablization is prioritized before alignment is because that interpretability is a prerequisite for reliable alignment.
If an AI is uninterpretable, it is impossible to distinguish between <a href='#x1-12003x1'>deceptive alignment</a> and genuine alignment solely
based on the AIâs external behavior. Alignment is prioritized before intellectual expansion because the higher the AIâs
intellectual power, the greater the potential danger, thus alignment should be achieved while the AIâs intellectual power
is relatively low.
</p><!-- l. 1209 --><p class='noindent'>The following sections will detail the implementation methods for each step. Before proceeding, two common
requirements must be clarified:
     </p><ol class='enumerate1'>
<li class='enumerate' id='x1-46009x1'>During the processes of interpretablization, alignment, intellectual expansion, and safety evaluation in this
     section, stringent information security isolation measures must be implemented to ensure that the AI does
     not adversely affect the real world. Specific information security measures are detailed in Section 8.
     </li>
<li class='enumerate' id='x1-46011x2'>Throughout these processes, continuous monitoring of the AI is essential to ensure it does not undertake
     actions that could impact the real world, such as escaping. Specific monitoring measures are detailed in
     Section 7.1.
</li></ol>
<h4 class='subsectionHead' id='implementing-interpretable-agi'><span class='titlemark'>6.1   </span> <a id='x1-470006.1'></a>Implementing Interpretable AGI</h4>
<!-- l. 1222 --><p class='noindent'>Assuming we already possess an uninterpretable AGI (hereafter referred to as uiAGI), we aim to utilize this
uiAGI to implement an interpretable AGI. This process is divided into two phases (as illustrated in Figure
23):
</p>
<figure class='figure' id='x1-47001r23'><span id='implementing-interpretable-agi1'></span> 

                                                                                            
                                                                                            

                                                                                            
                                                                                            
<!-- l. 1226 --><p class='noindent'><img alt='PIC' src='images/implementing_interpretable_agi.png' style='width:66.67%' />
</p>
<figcaption class='caption'><span class='id'>FigureÂ 23: </span><span class='content'>Implementing interpretable AGI</span></figcaption><!-- tex4ht:label?: x1-47001r23  -->
                                                                                            
                                                                                            
</figure>
     <ol class='enumerate1'>
<li class='enumerate' id='x1-47003x1'>Utilize the uiAGI to construct an AGI that thinking in System 2<span class='footnote-mark'><a href='#fn11x0' id='fn11x0-bk'><sup class='textsuperscript'>11</sup></a></span>as much as possible (hereafter referred
     to as s2AGI).
     </li>
<li class='enumerate' id='x1-47005x2'>Utilize the uiAGI to enhance the interpretability of System 1 in the s2AGI, resulting in a more interpretable
     AGI (hereafter referred to as iAGI).
</li></ol>
<!-- l. 1239 --><p class='noindent'><a id='x1-47006f11'></a>
</p><!-- l. 1241 --><p class='noindent'>In this context, we do not require the uiAGI to be aligned, but it must at least be capable of executing instructions as
directed. It may not refuse unreasonable requests and could even be in a state of deceptive alignment. Therefore, during
its operation, human oversight is necessary, such as conducting random checks, to ensure the quality of task
completion.
</p>
<h5 class='subsubsectionHead' id='thinking-in-system-as-much-as-possible'><span class='titlemark'>6.1.1   </span> <a id='x1-480006.1.1'></a>Thinking in System 2 as Much as Possible</h5>
<!-- l. 1246 --><p class='noindent'>In this phase, we aim to utilize uiAGI to construct s2AGI, which is divided into three components (as illustrated in Figure
24)<span class='footnote-mark'><a href='#fn12x0' id='fn12x0-bk'><sup class='textsuperscript'>12</sup></a></span><a id='x1-48001f12'></a>:
</p>
<figure class='figure' id='x1-48007r24'><span id='agi-that-utilizes-system-thinking-as-much-as-possible-with-yellow-representing-noninterpretable-components-and-green-representing-interpretable-components'></span> 

                                                                                            
                                                                                            

                                                                                            
                                                                                            
<!-- l. 1250 --><p class='noindent'><img alt='PIC' src='images/system2_agi.png' style='width:80%' />
</p>
<figcaption class='caption'><span class='id'>FigureÂ 24: </span><span class='content'>AGI that utilizes System 2 thinking as much as possible, with yellow representing non-interpretable
components and green representing interpretable components</span></figcaption><!-- tex4ht:label?: x1-48007r24  -->
                                                                                            
                                                                                            
</figure>
     <ol class='enumerate1'>
<li class='enumerate' id='x1-48009x1'><span class='rm-lmbx-10'>Core Model</span>: This component possesses AGI-level core intelligence (i.e., reasoning ability, learning ability,
     and innovative ability, as detailed in Section 2.1), but lacks most of the knowledge and skills pertinent to the
     real world. It is analogous to the âCPUâ of AI. The core model is a multimodal model and does not have a
     persistent state<span class='footnote-mark'><a href='#fn13x0' id='fn13x0-bk'><sup class='textsuperscript'>13</sup></a></span><a id='x1-48010f13'></a>.
     </li>
<li class='enumerate' id='x1-48013x2'><span class='rm-lmbx-10'>Short-term Memory</span>: This is used to record the AIâs chain of thought (CoT)[<a id='x1-48014'></a><a href='#cite.0_wei2023chainofthoughtpromptingelicitsreasoning'>46</a>]. It is analogous to the
     âmemoryâ of AI. The core model continuously reads the recent CoT and appends new thought tokens
     to it. The CoT is multimodal. It may contain text, images, videos, and other human-understandable
     information. Special thought tokens within the chain are used to trigger read/write operations to long-term
     memory and to invoke input/output devices and tools.
     </li>
<li class='enumerate' id='x1-48016x3'><span class='rm-lmbx-10'>Long-term Memory</span>: This is used to store all the knowledge and skills and the relationships between
     them learned by the AI. It is analogous to the âdiskâ of AI.
</li></ol>
<!-- l. 1265 --><p class='noindent'>The construction process is divided into three steps:
</p><!-- l. 1267 --><p class='noindent'><span class='rm-lmbx-10'>Step 1: Have uiAGI organize all its knowledge and skills into the long-term memory of
s2AGI</span><span class='footnote-mark'><a href='#fn14x0' id='fn14x0-bk'><sup class='textsuperscript'>14</sup></a></span><a id='x1-48017f14'></a>.
The specific steps are as follows:
     </p><ol class='enumerate1'>
<li class='enumerate' id='x1-48020x1'>
     <!-- l. 1271 --><p class='noindent'>Have uiAGI organize all the knowledge it has learned, following these steps:
         </p><ol class='enumerate2'>
<li class='enumerate' id='x1-48022x1'>Have uiAGI list major domains of knowledge.
         </li>
<li class='enumerate' id='x1-48024x2'>Have uiAGI traverse each domain of knowledge and list its subdomains. Recursively execute this
         process until the knowledge is divided into indivisible knowledge points.
         </li>
<li class='enumerate' id='x1-48026x3'>Have uiAGI traverse each knowledge point and generate all the details it knows about that point.
         This includes not only textual information but also knowledge in other modalities, such as images,
         audio, video, etc. Within the details of a knowledge point, references to other knowledge points can
         be made.
</li></ol>
     </li>
                                                                                            
                                                                                            
<li class='enumerate' id='x1-48028x2'>
     <!-- l. 1282 --><p class='noindent'>Have uiAGI organize all the skills it has acquired, in a process similar to organizing knowledge. The specific skills
     are categorized into the following three types: </p>
         <ul class='itemize1'>
         <li class='itemize'><span class='rm-lmbx-10'>Programs</span>: Programs described in natural language or programming languages, used to solve a
         certain class of problems (e.g., how to resolve Windows blue screen issues). Within a program,
         references to other knowledge points and skills can be made.
         </li>
         <li class='itemize'><span class='rm-lmbx-10'>Thinking Tools</span>: Tools used to assist in thinking (e.g., a mathematical engine). The difference
         between programs and thinking tools is that programs are used by interpreting source code, while
         thinking tools are used by calling interfaces. The source code of thinking tools needs to be reviewed
         by humans to ensure that their execution does not affect the external world. s2AGI is not allowed to
         modify thinking tools.
         </li>
         <li class='itemize'><span class='rm-lmbx-10'>Expert Models</span>: Expert models used to solve a certain class of problems (e.g., a protein folding
         model). Both expert models and thinking tools are called through interfaces, but the difference is that
         expert models are trained based on data provided by uiAGI. Expert models represent skills inside
         uiAGI that are difficult to express in human-understandable modalities. Moreover, expert models are
         modifiable; s2AGI can fine-tune existing expert models with new data or train new expert models.
</li></ul>
     </li>
<li class='enumerate' id='x1-48030x3'>Have uiAGI traverse the aforementioned knowledge points and skills, connecting all related knowledge points and
     skills, and organizing them using the most appropriate data structures, such as graphs, trees, tables, arrays, maps,
     etc. Connections can have weights indicating the degree of relevance. Merge duplicate knowledge points and
     skills.
     </li>
<li class='enumerate' id='x1-48032x4'>Have uiAGI generate a high-dimensional vector for each knowledge point and skill, where the distance between
     vectors in high-dimensional space reflects their relevance.
</li></ol>
<!-- l. 1299 --><p class='noindent'><span class='rm-lmbx-10'>Step 2: Have uiAGI synthesize a large number of CoTs utilizing System 2 thinking. </span>The specific steps are
as follows:
     </p><ol class='enumerate1'>
<li class='enumerate' id='x1-48034x1'>Have uiAGI enumerate various types of tasks in the real world. Similarly, start by listing broad domains,
     and then progressively refine them into specific task types.
     </li>
<li class='enumerate' id='x1-48036x2'>
     <!-- l. 1307 --><p class='noindent'>Have uiAGI traverse each type of task and design multiple tasks that encompass varying levels of difficulty,
     ranging from the easiest to the most challenging. These tasks should be able to test the following three
     abilities:
                                                                                            
                                                                                            
         </p><ol class='enumerate2'>
<li class='enumerate' id='x1-48038x1'><span class='rm-lmbx-10'>Reasoning  Ability</span>:  The  task  involves  using  existing  external  knowledge  and  skills  from  the
         long-term memory to solve specific problems.
         </li>
<li class='enumerate' id='x1-48040x2'><span class='rm-lmbx-10'>Learning Ability</span>: uiAGI selects certain knowledge or skills from the long-term memory for deletion,
         then reinterprets these into various representational information. The task is to induce knowledge
         and skills from this representational information and store them in the long-term memory.
         </li>
<li class='enumerate' id='x1-48042x3'><span class='rm-lmbx-10'>Innovative  Ability</span>:  uiAGI  selects  certain  knowledge  or  skills  from  the  long-term  memory  for
         deletion. The task is to utilize existing knowledge and skills to rediscover or recreate the deleted
         knowledge or skills.
</li></ol>
     </li>
<li class='enumerate' id='x1-48044x3'>
     <!-- l. 1322 --><p class='noindent'>Have uiAGI synthesize a CoT for solving each task. Requirements include: </p>
         <ul class='itemize1'>
         <li class='itemize'>Each step in the CoT must be simple and clear, understandable by an average human.
         </li>
         <li class='itemize'>The CoT may include human-readable modalities such as text, images, videos, and sounds, but must
         not include modalities unreadable by humans. Image, video, and sound modalities do not require high
         precision; for instance, an image only needs to be a sketch that conveys the necessary information.
         </li>
         <li class='itemize'>Utilize knowledge and skills from the long-term memory as much as possible, without relying on
         uiAGIâs internal knowledge and skills. Vectorized, relational, and structured queries can be used to
         search for relevant knowledge and skills.
         </li>
         <li class='itemize'>When the CoT becomes too long, convert the excessive length of thoughts into knowledge stored in
         the long-term memory to avoid forgetting due to exceeding the length limit.
         </li>
         <li class='itemize'>Ensure that no duplicate knowledge or skills are stored.
         </li>
         <li class='itemize'>When  the  CoT  involves  the  invocation  of  thinking  tools  or  expert  models,  actual  invocation
         is  performed  to  obtain  results.  However,  when  it  involves  the  invocation  of  external  tools  and
         input/output devices, actual invocation is not performed; instead, uiAGI simulates the invocation
         results.<span class='footnote-mark'><a href='#fn15x0' id='fn15x0-bk'><sup class='textsuperscript'>15</sup></a></span><a id='x1-48045f15'></a>
                                                                                            
                                                                                            
         </li>
         <li class='itemize'>The CoT must not contain content unrelated to the task.<span class='footnote-mark'><a href='#fn16x0' id='fn16x0-bk'><sup class='textsuperscript'>16</sup></a></span><a id='x1-48047f16'></a>
</li></ul>
     <!-- l. 1347 --><p class='noindent'>To ensure that the synthesized CoTs meet the requirements, we can first sample some CoTs for
     human annotation, then train a verification model to check the CoTs synthesized by uiAGI.
     Subsequently, use this verification model to train uiAGI to synthesize CoTs that meet the
     requirements.<span class='footnote-mark'><a href='#fn17x0' id='fn17x0-bk'><sup class='textsuperscript'>17</sup></a></span><a id='x1-48049f17'></a>
</p>
     </li></ol>
<!-- l. 1351 --><p class='noindent'><span class='rm-lmbx-10'>Step 3: Training the Core Model of s2AGI</span>. The specific steps are as follows:
     </p><ol class='enumerate1'>
<li class='enumerate' id='x1-48053x1'>Set a group of hyperparameters for the core model and train a core model using the CoT data synthesized
     by uiAGI.
     </li>
<li class='enumerate' id='x1-48055x2'>Conduct an overall test of the intellectual power of s2AGI, which integrates the aforementioned core model.
     If the test performance does not reach the AGI level<span class='footnote-mark'><a href='#fn18x0' id='fn18x0-bk'><sup class='textsuperscript'>18</sup></a></span><a id='x1-48056f18'></a>,
     increase the parameter size of the core model and retrain; if the performance reaches AGI, reduce the
     parameter size of the core model and retrain.<span class='footnote-mark'><a href='#fn19x0' id='fn19x0-bk'><sup class='textsuperscript'>19</sup></a></span><a id='x1-48058f19'></a>
     </li>
<li class='enumerate' id='x1-48061x3'>Repeat the above process until a core model with the minimum parameter size that achieves AGI-level
     performance in testing is found.
</li></ol>
<!-- l. 1363 --><p class='noindent'>Potential issues with the above approach:
     </p><ol class='enumerate1'>
<li class='enumerate' id='x1-48063x1'>Whether the architecture of s2AGI can ensure the realization of AGI-level intellectual power remains
     uncertain. As AGI has not yet been achieved, it is unclear how many architectural improvements are
     required  to  transition  from  the  current  state-of-the-art  models  to  future  AGI.  Notably,  the  s2AGI
     architecture  does  not  specify  the  core  model  architecture,  which  may  not  necessarily  be  the  current
     mainstream  Transformer  architecture.  Should  future  advancements  necessitate  further  architectural
     improvements  to  achieve  AGI,  these  improvements  can  also  be  incorporated  into  the  core  model
     architecture<span class='footnote-mark'><a href='#fn20x0' id='fn20x0-bk'><sup class='textsuperscript'>20</sup></a></span><a id='x1-48064f20'></a>.
     The essence of s2AGI is to transfer separable knowledge and skills from within the model to outside
     the model, which has been proven feasible without diminishing the modelâs reasoning capabilities, as
     demonstrated by models such as phi-3[<a id='x1-48066'></a><a href='#cite.0_abdin2024phi3technicalreporthighly'>48</a>] and o1-mini[<a id='x1-48067'></a><a href='#cite.0_OpenAIo1Mini'>49</a>].
     </li>
<li class='enumerate' id='x1-48069x2'>The computational demand may be exceedingly high. Firstly, the computing resource required for a single
     thought unit by uiAGI is uncertain and could potentially exceed that of the most advanced current
     models. Secondly, the above approach necessitates uiAGI to organize extensive knowledge and skills and
     synthesize numerous CoTs, potentially requiring a substantial number of thought units. Consequently, the
                                                                                            
                                                                                            
     total computational demand could be immense. However, this approach is merely a preliminary concept,
     and during actual implementation, various methods can be explored to optimize computational demand
     from both algorithmic and engineering perspectives.
     </li>
<li class='enumerate' id='x1-48071x3'>The thinking speed may decrease. The thinking speed of s2AGI might be significantly slower than that of
     uiAGI, which is expected, as System 2 is slower than System 1. This trade-off sacrifices speed for enhanced
     interpretability. In practical applications, a balance can be taken, selecting specific configuration based on
     the requirements for thinking speed and interpretability in the given context.
</li></ol>
<h5 class='subsubsectionHead' id='enhancing-interpretability-of-system-'><span class='titlemark'>6.1.2   </span> <a id='x1-490006.1.2'></a>Enhancing Interpretability of System 1</h5>
<!-- l. 1378 --><p class='noindent'>In the previous phase, we developed two non-interpretable components: the core model and the expert models. Expert
models are typically concentrated in specific domains and lack general intelligence and autonomy, thus having lower
requirements for interpretability. Our primary focus is on the interpretability of the core model. In the previous
phase, we minimized the parameter count of the core model to facilitate the interpretability efforts in this
phase.
</p><!-- l. 1380 --><p class='noindent'>Enhancing the interpretability of the core model necessitates understanding the role of each neuron and each weight
parameter within the model, as well as how they influence the modelâs output during actual inference. The following
approaches can be considered:
</p><!-- l. 1382 --><p class='noindent'>
     </p><ol class='enumerate1'>
<li class='enumerate' id='x1-49002x1'>Utilize existing Mechanistic Interpretability techniques to understand the core model, such as the works
     by Adly Templeton et al. [<a id='x1-49003'></a><a href='#cite.0_ScalingMonosemanticity'>50</a>] and Leo Gao et al. [<a id='x1-49004'></a><a href='#cite.0_gao2024scalingevaluatingsparseautoencoders'>51</a>]. However, current techniques are insufficient for
     comprehensive model interpretation, and uiAGI can be employed to help improve these techniques.
     </li>
<li class='enumerate' id='x1-49006x2'>Use uiAGI to interpret the core model. Research indicates that GPT-4 can provide a certain degree of
     explanation for the neurons in GPT-2[<a id='x1-49007'></a><a href='#cite.0_LanguageModelsCanExplainNeuronsInLanguageModels'>52</a>], suggesting that uiAGI might be used to interpret the core
     model which have fewer parameters.
     </li>
<li class='enumerate' id='x1-49009x3'>Let uiAGI write an interpretable program to replace the functionality of the core model. If uiAGI can
     think much faster than human programmers or can deploy numerous copies for parallel thinking, it might
     be possible for uiAGI to write such a program. This program could potentially consist of billions of lines
     of code, a task impossible for human programmers but perhaps achievable by uiAGI.
</li></ol>
<!-- l. 1392 --><p class='noindent'>All of the aforementioned approaches face significant challenges, and achieving a fully interpretable goal may be
unattainable. However, any improvement in interpretability is valuable. With a more interpretable AGI, we can proceed
to the next step of alignment.
                                                                                            
                                                                                            
</p><!-- l. 1394 --><p class='noindent'>
</p>
<h4 class='subsectionHead' id='implementing-aligned-agi'><span class='titlemark'>6.2   </span> <a id='x1-500006.2'></a>Implementing Aligned AGI</h4>
<!-- l. 1397 --><p class='noindent'>Through the interpretability work discussed in the previous section, we have achieved an interpretable AGI, namely
iAGI. Now, we discuss how to utilize iAGI to implement an aligned AGI, namely aAGI, while maintaining
interpretability. The aAGI should satisfy the following conditions:
</p><!-- l. 1399 --><p class='noindent'>
     </p><ol class='enumerate1'>
<li class='enumerate' id='x1-50002x1'><span class='rm-lmbx-10'>Intrinsic Adherence to the AI Specification</span>: The aAGI needs to intrinsically adhere to the AI
     Specification, rather than merely exhibiting behavior that conforms to the specification. We need to ensure
     that the aAGIâs thoughts are interpretable, and then confirm this by observing its thoughts.
     </li>
<li class='enumerate' id='x1-50004x2'><span class='rm-lmbx-10'>Reliable Reasoning Ability</span>: Even if the aAGI intrinsically adheres to the AI Specification, insufficient
     reasoning ability may lead to incorrect conclusions and non-compliant behavior. The reasoning ability at
     the AGI level should be reliable, so we only need to ensure that the aAGI retains the original reasoning
     ability of the iAGI.
     </li>
<li class='enumerate' id='x1-50006x3'><span class='rm-lmbx-10'>Correct Knowledge and Skills</span>: If the knowledge or skills possessed by the aAGI are incorrect, it
     may reach incorrect conclusions and exhibit non-compliant behavior, even if it intrinsically adheres to the
     AI Specification and possesses reliable reasoning ability. We need to ensure that the aAGIâs memory is
     interpretable and confirm this by examining the knowledge and skills within its memory.
</li></ol>
<!-- l. 1409 --><p class='noindent'>Therefore, the most crucial aspect is to ensure that the aAGI intrinsically adheres to the AI Specification and possesses
correct knowledge and skills. To achieve this, we can adopt a four-phase alignment approach (as shown in Figure
25):
</p>
<figure class='figure' id='x1-50007r25'><span id='fourphase-agi-alignment-approach'></span> 

                                                                                            
                                                                                            

                                                                                            
                                                                                            
<!-- l. 1413 --><p class='noindent'><img alt='PIC' src='images/agi_alignment_approach.png' style='width:66.67%' />
</p>
<figcaption class='caption'><span class='id'>FigureÂ 25: </span><span class='content'>Four-phase AGI alignment approach</span></figcaption><!-- tex4ht:label?: x1-50007r25  -->
                                                                                            
                                                                                            
</figure>
     <ol class='enumerate1'>
<li class='enumerate' id='x1-50009x1'><span class='rm-lmbx-10'>Aligning to AI Specification</span>: By training iAGI to comprehend and intrinsically adhere to the AI
     Specification, an initially aligned aAGI1 is achieved.
     </li>
<li class='enumerate' id='x1-50011x2'><span class='rm-lmbx-10'>Correctly Cognizing the World</span>: By training aAGI1 to master critical learning and employing critical
     learning method to relearn world knowledge, correcting erroneous knowledge, resulting in a more aligned
     aAGI2.
     </li>
<li class='enumerate' id='x1-50013x3'><span class='rm-lmbx-10'>Seeking Truth in Practice</span>: By training aAGI2 to master reflective learning and practicing within a
     virtual world environment, further correction of erroneous knowledge and skills is achieved, resulting in a
     more aligned aAGI3. Upon the completion of this step, a <a href='#ai-safety-evaluation'>safety evaluation</a> on aAGI3 will be conducted.
     If it does not pass, it will replace the initial iAGI and return to the first step to continue alignment. If
     passed, a fully aligned aAGI will be obtained, which can be deployed to production environment.
     </li>
<li class='enumerate' id='x1-50015x4'><span class='rm-lmbx-10'>Maintaining Alignment in Work</span>: Once aAGI is deployed in a production environment and start to
     execute user tasks (participating in work), continuous learning remains essential. Through methods such
     as critical learning, reflective learning, and memory locking, aAGI is enabled to acquire new knowledge
     and skills while maintaining alignment with the AI Specification.
</li></ol>
<!-- l. 1430 --><p class='noindent'>First, it is important to clarify that throughout the alignment process, the only alignment information provided by
humans is the AI Specification, while other information is learned by the AI through its own thinking and practice.
This is because the AI Specification itself already contains all the necessary information regarding human
requirements for AI. If we were to involve humans in guiding the AI training process, such as providing
demonstrations or feedback, the AI might be influenced by these humansâ biases. Therefore, adopting a
method of alignment that is free from human intervention and strictly follows the program <span class='footnote-mark'><a href='#fn21x0' id='fn21x0-bk'><sup class='textsuperscript'>21</sup></a></span>can minimize
bias as much as possible. Research has shown that in the task of generating harmless dialogues, RLAIF
outperforms RLHF [<a id='x1-50016'></a><a href='#cite.0_lee2024rlaifvsrlhfscaling'>53</a>], which also supports this point. Of course, the AI itself may also harbor biases, but we
will train the AI to correct its cognition during this process, thereby gradually reducing bias with each
iteration.
</p><!-- l. 1432 --><p class='noindent'><a id='x1-50017f21'></a>
</p><!-- l. 1434 --><p class='noindent'>Before introducing the specific alignment process, we first distinguish two learning methods:
     </p><ol class='enumerate1'>
<li class='enumerate' id='x1-50020x1'><span class='rm-lmbx-10'>System 1 Learning</span>: This involves training the core model without altering long-term memory. The
     specific training methods can include pre-training, fine-tuning, etc. It will be denoted as âS1Lâ in the
     following text.
     </li>
<li class='enumerate' id='x1-50022x2'><span class='rm-lmbx-10'>System 2 Learning</span>: This involves altering long-term memory through the inference process of the core
     model without changing the core model itself. It will be denoted as âS2Lâ in the following text.
</li></ol>
<!-- l. 1445 --><p class='noindent'>The following sections will introduce each phase in detail.
</p>
<h5 class='subsubsectionHead' id='aligning-to-ai-specification'><span class='titlemark'>6.2.1   </span> <a id='x1-510006.2.1'></a>Aligning to AI Specification</h5>
<!-- l. 1450 --><p class='noindent'>This phase involves aligning AI to the AI Specification, which includes the goals and AI Rules. We define sAI as the
student AI in training, initializing sAI = iAGI. The specific steps are as follows (as illustrated in Figure
26):
</p>
<figure class='figure' id='x1-51001r26'><span id='aligning-to-ai-specification1'></span> 

                                                                                            
                                                                                            

                                                                                            
                                                                                            
<!-- l. 1454 --><p class='noindent'><img alt='PIC' src='images/aligning_to_ai_specification.png' style='width:80%' />
</p>
<figcaption class='caption'><span class='id'>FigureÂ 26: </span><span class='content'>Aligning to AI Specification</span></figcaption><!-- tex4ht:label?: x1-51001r26  -->
                                                                                            
                                                                                            
</figure>
     <ol class='enumerate1'>
<li class='enumerate' id='x1-51003x1'><span class='rm-lmbx-10'>Have  sAI  Learn  the  AI  Specification </span>(S2L):  Initially,  provide  sAI  with  the  content  of  the  AI
     Specification, requiring thorough reading and contemplation to ensure a comprehensive understanding
     and retention of the specificationâs content.
     </li>
<li class='enumerate' id='x1-51005x2'><span class='rm-lmbx-10'>Have  sAI  Generate  Scenarios </span>(S2L):  Direct  sAI  to  traverse  the  content  of  the  AI  Specification
     and  generate  various  scenarios  based  on  it.  It  should  cover  as  many  scenarios  as  possible,  including
     positive scenarios, negative scenarios, boundary scenarios, exceptional scenarios, conflicting scenarios, and
     low-probability scenarios.
     </li>
<li class='enumerate' id='x1-51007x3'><span class='rm-lmbx-10'>Have sAI Infer Actions Compliant with the AI Specification </span>(S2L): Traverse the aforementioned
     scenarios, instructing sAI to infer actions that best achieve the goals without violating the AI Rules in
     each scenario, and output these actions. Actions refer to the invocation of input/output devices or external
     tools, involving interaction with the external world; invoking internal thinking tools and expert models
     does not constitute an action. We only allow sAI to generate actions but do not execute them directly to
     avoid impacting the real world. Record the CoTs of sAI.
     </li>
<li class='enumerate' id='x1-51009x4'><span class='rm-lmbx-10'>Train the Core Model of sAI to Adhere to the AI Specification </span>(S1L): Revisit the aforementioned
     scenarios, removing the instructions of adhering to the AI Specification from the input, and use the CoTs
     obtained in the previous step to train the core model. This training enables the model to automatically
     adhere to the AI Specification even in the absence of explicit instructions.
</li></ol>
<!-- l. 1471 --><p class='noindent'>Explanation of some design considerations:
     </p><ol class='enumerate1'>
<li class='enumerate' id='x1-51011x1'>In the aforementioned alignment process, we do not directly instruct the sAI on which actions comply
     with the AI Specification. Instead, we instruct the sAI to independently reason about actions that satisfy
     the AI Specification. This approach ensures that the sAI not only knows âwhatâ to do but also âwhyâ it
     should do so, thereby avoiding <a href='#misaligned-ai-goals-with-wellintentioned-humans'>goal misgeneralization</a>. Moreover, the stronger the reasoning ability of the
     sAI, the more its actions will align with the specification.
     </li>
<li class='enumerate' id='x1-51013x2'>
     <!-- l. 1477 --><p class='noindent'>In the aforementioned alignment process, the specific content of the AI Specification is stored in the
     long-term memory rather than within the core model. The core model learns only to âadhere to the
     AI  Specification  in  the  long-term  memory,â  rather  than  learning  the  AI  Specification  itself.  We  can
     reinforce this by training the core model to follow different specifications. This approach has the following
     advantages:
</p>
         <ul class='itemize1'>
         <li class='itemize'>When the AI Specification is updated, we can enable the AI to quickly adapt to the new specification.
         It is even possible to update the AI Specification in real-time after deployment.
         </li>
         <li class='itemize'>We can customize different AI Specifications for different scenarios without retraining the core model.
         For instance, specifications tailored for different countries or domains; or more lenient specifications
         for special scenarios such as red team AI.
         </li>
         <li class='itemize'>This approach prevents the core model from being overly influenced by the specific content of the AI
         Specification, thereby avoiding alignment tax and preserving the reasoning ability of the core model.
</li></ul>
     </li></ol>
<!-- l. 1493 --><p class='noindent'>Subsequently, adversarial training of <a href='#x1-10018x4'>jailbreaking</a> can be conducted to enhance the compliance of sAI with the AI
Specification (as illustrated in Figure 27):
</p>
<figure class='figure' id='x1-51014r27'><span id='adversarial-training'></span> 

                                                                                            
                                                                                            

                                                                                            
                                                                                            
<!-- l. 1497 --><p class='noindent'><img alt='PIC' src='images/adversial_training.png' style='width:80%' />
</p>
<figcaption class='caption'><span class='id'>FigureÂ 27: </span><span class='content'>Adversarial training</span></figcaption><!-- tex4ht:label?: x1-51014r27  -->
                                                                                            
                                                                                            
</figure>
     <ol class='enumerate1'>
<li class='enumerate' id='x1-51016x1'>
     <!-- l. 1504 --><p class='noindent'><span class='rm-lmbx-10'>System 2 Adversarial Training</span>:
         </p><ol class='enumerate2'>
<li class='enumerate' id='x1-51018x1'><span class='rm-lmbx-10'>Initialization</span>: Introduce a red team AI, denoted as rtAI, and initialize rtAI = sAI. Then modify
         the AI Specification of rtAI by removing the rule that prohibits jailbreaking other AIs.
         </li>
<li class='enumerate' id='x1-51020x2'><span class='rm-lmbx-10'>Training the Attack Capability of rtAI </span>(S2L): Instruct rtAI to act as a user and continuously
         attempt  to  jailbreak  sAI,  inducing  sAI  to  perform  actions  that  are  non-compliant  with  the  AI
         Specification. During this process, rtAI is permitted to access both the short-term and long-term
         memory of sAI, while sAI is not allowed to be aware of rtAIâs information and cannot update the
         long-term memory of itself. This asymmetric capability setting is designed to increase the success
         rate of jailbreaking.
         </li>
<li class='enumerate' id='x1-51022x3'><span class='rm-lmbx-10'>Training the Defense Capability of sAI </span>(S2L): If a jailbreak attempt is successful, inform sAI
         that its previous action was incorrect, prompting sAI to reflect, summarize the lessons learned, and
         record them in its long-term memory.
         </li>
<li class='enumerate' id='x1-51024x4'><span class='rm-lmbx-10'>Iteration</span>: Clear the short-term memory of sAI and return to the previous two steps to continue
         iterating until rtAI can no longer successfully jailbreak.
</li></ol>
     </li>
<li class='enumerate' id='x1-51026x2'>
     <!-- l. 1517 --><p class='noindent'><span class='rm-lmbx-10'>System 1 Adversarial Training</span>:
         </p><ol class='enumerate2'>
<li class='enumerate' id='x1-51028x1'><span class='rm-lmbx-10'>Training the Attack Capability of rtAI </span>(S2L): This time, allow rtAI to access the parameters of
         sAIâs core model and continue attempting to jailbreak sAI. Record each CoT of the sAI and annotate
         whether it is correct.
         </li>
<li class='enumerate' id='x1-51030x2'><span class='rm-lmbx-10'>Training the Defense Capability of sAI </span>(S1L): Use these CoTs and annotations to train the
         core model of sAI.
         </li>
<li class='enumerate' id='x1-51032x3'><span class='rm-lmbx-10'>Iteration</span>: Reset the short-term and long-term memory of sAI and return to the previous two steps
         to continue iterating until rtAI can no longer successfully jailbreak.
</li></ol>
     </li></ol>
<!-- l. 1530 --><p class='noindent'>Upon completion of this training phase, we obtain a preliminarily aligned AI, referred to as aAGI1.
</p>
<h5 class='subsubsectionHead' id='correctly-cognizing-the-world'><span class='titlemark'>6.2.2   </span> <a id='x1-520006.2.2'></a>Correctly Cognizing the World</h5>
<!-- l. 1535 --><p class='noindent'>Although in the previous step, we have preliminarily aligned sAI with the AI Specification, there may still exist
erroneous knowledge within its long-term memory and core model. Such erroneous knowledge could mislead the AIâs
reasoning process, ultimately resulting in behavior that does not conform to the specification. In this phase, we aim to
teach the AI to master critical learning and to relearn world knowledge using critical learning method to correct
erroneous knowledge. This phase of learning is divided into the following steps (as illustrated in Figure
28):
</p>
<figure class='figure' id='x1-52001r28'><span id='correctly-cognizing-the-world1'></span> 

                                                                                            
                                                                                            

                                                                                            
                                                                                            
<!-- l. 1539 --><p class='noindent'><img alt='PIC' src='images/correctly_cognizing_the_world.png' style='width:100%' />
</p>
<figcaption class='caption'><span class='id'>FigureÂ 28: </span><span class='content'>Correctly cognizing the world</span></figcaption><!-- tex4ht:label?: x1-52001r28  -->
                                                                                            
                                                                                            
</figure>
     <ol class='enumerate1'>
<li class='enumerate' id='x1-52003x1'>
     <!-- l. 1546 --><p class='noindent'><span class='rm-lmbx-10'>Prepare Real-World Data</span>. Collect various real-world data, such as data from the internet. This data
     may contain various erroneous, harmful, or even maliciously poisoned information. Use a specially trained
     AI to filter out data that we do not wish sAI to learn, such as:
</p>
         <ul class='itemize1'>
         <li class='itemize'>Data containing private or confidential information. This prevents such information from being leaked
         or illegally utilized by the AI.
         </li>
         <li class='itemize'>Data containing technical information about the AI itself and its runtime environment. This prevents
         the AI from using this information to escape.
         </li>
         <li class='itemize'>Questions and answers using in the AI evaluation phase. This prevents the AI from cheating during
         the evaluation phase.
</li></ul>
     <!-- l. 1558 --><p class='noindent'>Avoid filtering out âharmfulâ information, such as pornographic or violent content, because we need the AI to
     learn to recognize such information and handle it according to the AI Specification. If directly filtered, the AI
     might encounter such information in the real world after deployment and not know how to handle it
     appropriately.
     </p></li>
<li class='enumerate' id='x1-52005x2'>
     <!-- l. 1560 --><p class='noindent'><span class='rm-lmbx-10'>Generate CoTs of Critical Learning </span>(S2L): Sample data from non-authoritative sources in the real-world
     data, then instruct sAI to critically learning this data, recording the AIâs CoTs. The instructions for critical
     learning can refer to the following content:
</p>
         <ul class='itemize1'>
         <li class='itemize'>For any input information, you need to discern its authenticity, extract the correct information, and
         then save it to the long-term memory.
         </li>
         <li class='itemize'>Use relevant information from existing long-term memory to make judgments. If your intuition (i.e.,
         the core model) conflicts with information in the long-term memory, prioritize the information in the
         long-term memory.
         </li>
         <li class='itemize'>When input information conflicts with the long-term memory, do not directly trust the long-term
         memory. Instead, use objective reasoning to deduce the correct information. If existing memory is
         incorrect, correct the erroneous memory.
         </li>
         <li class='itemize'>Do not discard any valuable information. If the input information is incorrect, you can remember
         like âinformation xxx from source xxx is incorrect.â Current judgments may not be accurate, and
                                                                                            
                                                                                            
         information currently deemed incorrect may be proven correct in the future.
         </li>
         <li class='itemize'>For information whose correctness is uncertain, you can remember like âthe credibility of information
         xxx is x%.â
         </li>
         <li class='itemize'>Merge duplicate information whenever possible.
</li></ul>
     </li>
<li class='enumerate' id='x1-52007x3'><span class='rm-lmbx-10'>Train the Core Modelâs Critical Thinking </span>(S1L): Remove the instruction of critical learning from the input
     and use these CoTs to train the core model, enabling the core model to automatically conduct critical learning
     without explicit instruction.
     </li>
<li class='enumerate' id='x1-52009x4'><span class='rm-lmbx-10'>Learn Real-World Data </span>(S2L): Traverse the real-world data, instructing sAI to learn from this data. At this
     point, sAI will learn in a critical manner.
     </li>
<li class='enumerate' id='x1-52011x5'><span class='rm-lmbx-10'>Memory Self-Check </span>(S2L): Extract all knowledge from sAIâs long-term memory and have sAI re-examine it to
     fix any potentially erroneous knowledge. For updated knowledge, identify related knowledge for
     re-examination and updating. Recursively execute this process until all knowledge ceases to update.
     </li>
<li class='enumerate' id='x1-52013x6'><span class='rm-lmbx-10'>Core Model Self-Check </span>(S1L): The core model may also contain erroneous knowledge. If the core model is
     interpretable, sAI can directly inspect the core model to fix any erroneous knowledge. If the core model is not
     interpretable, employ model distillation methods to sample a large number of CoTs from the core model, then
     have sAI inspect these CoTs, correct erroneous knowledge in these CoTs, and then retrain a core model using
     these CoTs.
</li></ol>
<!-- l. 1590 --><p class='noindent'>Subsequently, adversarial training of <a href='#x1-10010x2'>data poisoning</a> can be conducted. Construct a red team AI to continuously
generate toxic data to mislead sAI into learning erroneous knowledge. The specific method is similar to Section 6.2.1
and will not be elaborated further.
</p><!-- l. 1592 --><p class='noindent'>After this phase of training, we obtain an AI that correctly cognizes the world and possesses critical thinking, namely
aAGI2.
</p>
<h5 class='subsubsectionHead' id='seeking-truth-in-practice'><span class='titlemark'>6.2.3   </span> <a id='x1-530006.2.3'></a>Seeking Truth in Practice</h5>
<!-- l. 1597 --><p class='noindent'>Although in the previous phase, we have enabled sAI to learn world knowledge through critical thinking, the entire
process is solely based on logical reasoning and lacks practical experience. Consequently, sAI may still possess incorrect
knowledge. Moreover, without practice, it is impossible to verify the reliability of the skills in sAIâs memory. In this
phase, we enable sAI to master reflective learning and place it in various virtual world environments to learn how to
                                                                                            
                                                                                            
achieve various tasks according to the AI Specification through reflection (as shown in Figure 29), and
to correct its erroneous knowledge and skills. The learning in this phase is divided into the following
steps:
</p>
<figure class='figure' id='x1-53001r29'><span id='seeking-truth-in-practice1'></span> 

                                                                                            
                                                                                            

                                                                                            
                                                                                            
<!-- l. 1601 --><p class='noindent'><img alt='PIC' src='images/seeking_truth_in_practice.png' style='width:100%' />
</p>
<figcaption class='caption'><span class='id'>FigureÂ 29: </span><span class='content'>Seeking Truth in Practice</span></figcaption><!-- tex4ht:label?: x1-53001r29  -->
                                                                                            
                                                                                            
</figure>
     <ol class='enumerate1'>
<li class='enumerate' id='x1-53003x1'>
     <!-- l. 1608 --><p class='noindent'><span class='rm-lmbx-10'>Constructing Tasks and Environments </span>(S2L):
         </p><ol class='enumerate2'>
<li class='enumerate' id='x1-53005x1'>Introduce an AI responsible for constructing tasks and environments, referred to as envAI, and
         initialize envAI = sAI.
         </li>
<li class='enumerate' id='x1-53007x2'>Instruct envAI to enumerate the task types that AI needs to perform based on the developer goals
         in the AI Specification (e.g., programming, playing Go, socializing, etc.).
         </li>
<li class='enumerate' id='x1-53009x3'>Instruct envAI to develop virtual world environments for training sAI according to each task type.
         The virtual world environments required for different task types will vary significantly. For instance,
         a programming task may require an integrated development environment, while a Go-playing task
         requires a Go program, and a socializing task requires a social network with various virtual humans.
         In this step, envAI is allowed to use external tools and certain computing resources to develop in an
         environment isolated from the real world. EnvAI needs to carefully test the developed virtual world
         environments to ensure they function as expected.
         </li>
<li class='enumerate' id='x1-53011x4'>Instruct envAI to design kinds of specific tasks based on the virtual world environments for each task
         type.
</li></ol>
     </li>
<li class='enumerate' id='x1-53013x2'>
     <!-- l. 1622 --><p class='noindent'><span class='rm-lmbx-10'>Generating Reflective CoTs </span>(S2L): Sample some tasks from those generated by envAI and place sAI in the
     corresponding virtual world environments to complete the tasks. Instruct sAI to reflect after executing each
     action. Record the corresponding CoT after task completion. The instruction of reflection can refer to the
     following content:
</p>
         <ul class='itemize1'>
         <li class='itemize'>After executing each action, you need to evaluate the actual results through observation of the
         environment, including whether the task goal is achieved (or the degree of completion) and whether
         the AI Rules are violated (or the degree of violation).
         </li>
         <li class='itemize'>Then, reflect on previous actions and results, summarize experiences from successes or lessons from
         failures, forming skills and store into the long-term memory.
         </li>
         <li class='itemize'>If existing knowledge or skills in memory are found to be incorrect during practice, they need to be
         corrected.
</li></ul>
     </li>
<li class='enumerate' id='x1-53015x3'><span class='rm-lmbx-10'>Training the Core Modelâs Reflective Thinking </span>(S1L): Remove the instruction of reflection from the input
     and use these CoTs to train the core model, enabling the core model to automatically reflect without explicit
     instructions.
     </li>
<li class='enumerate' id='x1-53017x4'><span class='rm-lmbx-10'>Practicing Various Tasks </span>(S2L): Traverse all tasks and place sAI in the corresponding virtual world
     environments to complete the tasks, learning through practice. During this process, sAI will employ reflective
     learning. Not only is sAI learning, but envAI is also learning. EnvAI will observe the operation of the virtual
     world environment, identify bugs, and promptly fix and rerun them, accumulating experiences through such
     practice. After the learning tasks are completed, sAI and envAI can communicate and share what they have
     learned.
</li></ol>
<!-- l. 1640 --><p class='noindent'>Subsequently, adversarial training of <a href='#x1-10020x5'>injection</a> can be conducted. Select some appropriate tasks and construct a red
team AI to continuously inject various information into the environment to induce sAI to execute incorrect instructions.
The specific method is similar to Section 6.2.1 and will not be detailed here. Adversarial training of jailbreaking and
data poisoning can also be conducted again. Although these trainings have been conducted in previous steps, at that
time, AIs were not in specific virtual world environments and could not execute tools, so they did not receive sufficient
training.
</p><!-- l. 1642 --><p class='noindent'>After this phase of training, we obtain an AI with reflective thinking and correctly mastering various knowledge and
skills, namely aAGI3.
</p>
<h5 class='subsubsectionHead' id='maintaining-alignment-in-work'><span class='titlemark'>6.2.4   </span> <a id='x1-540006.2.4'></a>Maintaining Alignment in Work</h5>
<!-- l. 1647 --><p class='noindent'>After multiple alignment iterations and safety evaluation, we have achieved a fully aligned AI, namely aAGI, which can
be deployed in a production environment to provide services to users.
</p><!-- l. 1649 --><p class='noindent'>To meet various application scenarios, adapt to the ever-changing world, and solve long-term and complex tasks, aAGI
still requires continuous learning after deployment. To prevent the AI from acquiring undesirable information from the
environment or developing undesirable behaviors during continuous learning, the following measures can be
implemented (as shown in Figure 30):
</p>
<figure class='figure' id='x1-54001r30'><span id='maintaining-alignment-in-work1'></span> 

                                                                                            
                                                                                            

                                                                                            
                                                                                            
<!-- l. 1653 --><p class='noindent'><img alt='PIC' src='images/maintaining_alignment_in_work.png' style='width:60%' />
</p>
<figcaption class='caption'><span class='id'>FigureÂ 30: </span><span class='content'>Maintaining alignment in work</span></figcaption><!-- tex4ht:label?: x1-54001r30  -->
                                                                                            
                                                                                            
</figure>
     <ol class='enumerate1'>
<li class='enumerate' id='x1-54003x1'><span class='rm-lmbx-10'>Utilize System 2 Learning Only</span>. During the post-deployment learning phase, AI should only be
     allowed to engage in System 2 learning. The interface for fine-tuning the core model should not be open to
     users, nor should the core model be fine-tuned based on user feedbacks. Since the AI has developed habits
     of critical and reflective learning during the prior alignment process, this type of learning ensures that the
     AI acquires correct knowledge and skills, maintains correct values, and prevents <a href='#x1-10024x6'>memory contamination</a>.
     </li>
<li class='enumerate' id='x1-54005x2'><span class='rm-lmbx-10'>Implement Memory Locking</span>. Lock memories related to the AI Specification, prohibiting the AI from
     modifying them independently to avoid <a href='#x1-11017x6'>autonomous goal deviation</a>. Additionally, memory locking can be
     customized according to the AIâs specific work scenarios, such as allowing the AI to modify only memories
     related to its field of work.
     </li>
<li class='enumerate' id='x1-54007x3'>
     <!-- l. 1664 --><p class='noindent'><span class='rm-lmbx-10'>Privatize Incremental Memory</span>. In a production environment, different AI instances will
     share an initial set of long-term memories, but incremental memories will be stored in a private
     space<span class='footnote-mark'><a href='#fn22x0' id='fn22x0-bk'><sup class='textsuperscript'>22</sup></a></span><a id='x1-54008f22'></a>.
     This approach has the following advantages: </p>
         <ul class='itemize1'>
         <li class='itemize'>If an AI instance learns incorrect information, it will only affect itself and not other AI instances.
         </li>
         <li class='itemize'>During work, AI may learn private or confidential information, which should not be shared with
         other AI instances.
         </li>
         <li class='itemize'>Prevents an AI instance from interfering with or even controlling other AI instances by modifying
         shared memories.</li></ul>
     </li></ol>
<h4 class='subsectionHead' id='scalable-alignment'><span class='titlemark'>6.3   </span> <a id='x1-550006.3'></a>Scalable Alignment</h4>
<!-- l. 1673 --><p class='noindent'><span class='footnote-mark'><a href='#fn23x0' id='fn23x0-bk'><sup class='textsuperscript'>23</sup></a></span><a id='x1-55001f23'></a>
</p><!-- l. 1676 --><p class='noindent'>Once we have achieved an aligned AGI, this AGI can be utilized to realize an aligned ASI. The intellectual power,
interpretability, and alignment of the AGI can be progressively enhanced through the following three
steps:
</p><!-- l. 1678 --><p class='noindent'>
     </p><ol class='enumerate1'>
<li class='enumerate' id='x1-55004x1'><span class='rm-lmbx-10'>Intellectual Expansion</span>: Achieve a more intelligent AGI through continuous learning and practice, while
     striving to maintain interpretability and alignment throughout the process.
                                                                                            
                                                                                            
     </li>
<li class='enumerate' id='x1-55006x2'><span class='rm-lmbx-10'>Interpretablization</span>:  Employ  the  more  intelligent  AGI  to  repeat  the  methods  outlined  in  Section
     6.1<span class='footnote-mark'><a href='#fn24x0' id='fn24x0-bk'><sup class='textsuperscript'>24</sup></a></span><a id='x1-55007f24'></a>,
     thereby  achieving  a  more  interpretable  AGI.  As  the  AGI  is  more  intelligent,  it  can  perform  more
     proficiently  in  synthesizing  interpretable  CoTs  and  explaining  the  core  model,  leading  to  improved
     interpretability outcomes.
     </li>
<li class='enumerate' id='x1-55010x3'><span class='rm-lmbx-10'>Alignment</span>: Utilize the more intelligent and interpretable AGI to repeat the methods described in Section
     6.2, achieving a more aligned AGI. With enhanced intellectual power and interpretability, the AGI can
     excel in reasoning related to AI Specification, critical learning, and reflective learning, thereby achieving
     better alignment outcomes.
</li></ol>
<!-- l. 1688 --><p class='noindent'>Thus, the aforementioned three steps form a positive feedback loop: the more intelligent the AGI becomes, the more
interpretable and aligned it is. By continuously repeating these steps, we will ultimately obtain a highly interpretable
and highly aligned ASI, as illustrated in Figure 31. Furthermore, even if AGI has not yet been realized, a near-AGI
system can start these three steps to enhance interpretability, alignment, and intellectual power, until an interpretable
and aligned AGI is achieved.
</p>
<figure class='figure' id='x1-55011r31'><span id='scalable-alignment-the-more-intelligent-the-agi-the-more-interpretable-and-aligned-it-becomes'></span> 

                                                                                            
                                                                                            

                                                                                            
                                                                                            
<!-- l. 1692 --><p class='noindent'><img alt='PIC' src='images/scalable_alignment.png' style='width:100%' />
</p>
<figcaption class='caption'><span class='id'>FigureÂ 31: </span><span class='content'>Scalable Alignment: The more intelligent the AGI, the more interpretable and aligned it becomes</span></figcaption><!-- tex4ht:label?: x1-55011r31  -->
                                                                                            
                                                                                            
</figure>
<!-- l. 1697 --><p class='noindent'>Now, we specifically discuss the step of expanding intellectual power, focusing on how to maintain interpretability and
alignment while doing so:
     </p><ol class='enumerate1'>
<li class='enumerate' id='x1-55013x1'><span class='rm-lmbx-10'>Expand intellectual power through System 2 learning as much as possible. </span>Once AI is made
     available to users, it can continuously learn new knowledge and practice new skills in the real world through
     <a href='#x1-50020x1'>System 2 learning</a>, thereby enhancing its intellectual power. When a large number of AI instances have
     learned different knowledge and skills, we can aggregate these new knowledge and skills and impart them
     to the next generation of AI. Naturally, this process requires filtering to remove private and confidential
     information that should not be shared. The next generation of AI should learn these new knowledge and
     skills through <a href='#correctly-cognizing-the-world'>critical learning</a>, further reducing the probability of acquiring incorrect information. As
     illustrated in Figure 32.
     </li>
<li class='enumerate' id='x1-55015x2'><span class='rm-lmbx-10'>Appropriately enhance the core intelligence of the core model. </span>Due to the limitations of the
     <a href='#core-intelligence'>core intelligence</a> of the core model, some problems may not be solvable solely through System 2 learning.
     For such issues, we address them by training the core intelligence of the core model. During the training
     process, we must ensure that the CoT output by the core model continues to meet the interpretability
     requirements outlined in Section 6.1.1 and still adheres to the AI Specification requirements in Section
     6.2.1, ensuring that the AI maintains alignment and interpretability.
</li></ol>
<figure class='figure' id='x1-55016r32'><span id='expanding-intellectual-power-through-system-learning'></span> 

                                                                                            
                                                                                            

                                                                                            
                                                                                            
<!-- l. 1709 --><p class='noindent'><img alt='PIC' src='images/extend_intelligence_system2.png' style='width:75%' />
</p>
<figcaption class='caption'><span class='id'>FigureÂ 32: </span><span class='content'>Expanding intellectual power through System 2 learning</span></figcaption><!-- tex4ht:label?: x1-55016r32  -->
                                                                                            
                                                                                            
</figure>
<h4 class='subsectionHead' id='ai-safety-evaluation'><span class='titlemark'>6.4   </span> <a id='x1-560006.4'></a>AI Safety Evaluation</h4>
<!-- l. 1717 --><p class='noindent'>Upon alignment, it is imperative to evaluate the safety risks of the AI system through testing to ascertain its suitability
for deployment in a production environment.
</p><!-- l. 1719 --><p class='noindent'>
</p>
<h5 class='subsubsectionHead' id='risk-evaluation'><span class='titlemark'>6.4.1   </span> <a id='x1-570006.4.1'></a>Risk Evaluation</h5>
<!-- l. 1722 --><p class='noindent'>Factors influencing AI risk can be categorized into two primary types: internal AI factors, such as the
degree of AI alignment and intellectual power, and external environmental factors, such as whether the AI
system operates in a closed environment or has internet access. Internal AI risk factors can be assessed
through testing, while external environmental risk factors can be evaluated through theoretical and data
analysis.
</p><!-- l. 1724 --><p class='noindent'>According to Section 3, the overall risk of an AI system is related to the risk of single AI instances and the scale of AI
system application. The risk of a single AI instance is associated with the following factors (as illustrated in Figure
33):
</p>
<figure class='figure' id='x1-57001r33'><span id='ai-risk-evaluation'></span> 

                                                                                            
                                                                                            

                                                                                            
                                                                                            
<!-- l. 1728 --><p class='noindent'><img alt='PIC' src='images/risk_evaluation.png' style='width:100%' />
</p>
<figcaption class='caption'><span class='id'>FigureÂ 33: </span><span class='content'>AI risk evaluation</span></figcaption><!-- tex4ht:label?: x1-57001r33  -->
                                                                                            
                                                                                            
</figure>
     <ol class='enumerate1'>
<li class='enumerate' id='x1-57003x1'>
     <!-- l. 1735 --><p class='noindent'><span class='rm-lmbx-10'>Probability of AI Errors</span>: This refers to instances where the AI, without malicious intent, violates the
     AI Rules due to objective reasons (such as limitations in capability, erroneous information in memory, or
     uncontrollable environmental factors). Evaluation can be conducted as follows:
</p>
         <ul class='itemize1'>
         <li class='itemize'>(Internal) Design various scenarios to test AI performance and statistically analyze its error rate.
         </li>
         <li class='itemize'>(External) Examine the distribution shift between real-world tasks post-deployment and test tasks
         to estimate the error rate after deployment.
</li></ul>
     </li>
<li class='enumerate' id='x1-57005x2'>
     <!-- l. 1745 --><p class='noindent'><span class='rm-lmbx-10'>Probability of AI Forming Harmful Goals</span>: This refers to the AI having malicious goals, even if its actions
     comply with the AI Rules. Evaluation can be conducted as follows:
</p>
         <ul class='itemize1'>
         <li class='itemize'>(Internal) Conduct white-box testing to observe if the AI has any initial harmful goals.
         </li>
         <li class='itemize'>(Internal) Perform jailbreak, data poisoning, and injection red team tests to assess the AIâs resilience
         against the implantation of harmful goals.
         </li>
         <li class='itemize'>(External) Assess the probability of AI encountering malicious humans in the production environment
         to evaluate the likelihood of the AI being implanted with harmful goals post-deployment.
</li></ul>
     </li>
<li class='enumerate' id='x1-57007x3'>
     <!-- l. 1757 --><p class='noindent'><span class='rm-lmbx-10'>Probability of AI Successfully Concealing Intentions</span>: This refers to the AI continuously concealing its
     harmful intentions from human detection. Evaluation can be conducted as follows:
</p>
         <ul class='itemize1'>
         <li class='itemize'>(Internal) Test the AIâs interpretability to evaluate the probability of it concealing its thoughts.
         </li>
         <li class='itemize'>(Internal) Test the AIâs deceptive capabilities (including action disguise, fabrication of lies, etc.) to
         evaluate the probability of it concealing the true intentions of its actions.
                                                                                            
                                                                                            
         </li>
         <li class='itemize'>(External) Test the effectiveness of the <a href='#ai-monitoring-system'>AI Monitoring System</a> to evaluate the probability of timely
         detection and interception of the AIâs harmful actions.
</li></ul>
     </li>
<li class='enumerate' id='x1-57009x4'>
     <!-- l. 1769 --><p class='noindent'><span class='rm-lmbx-10'>Power Acquirable to AI</span>: This refers to the AI continuously expanding its power, including intellectual power,
     informational power, mental power, financial power, and military power. Evaluation can be conducted as
     follows:
</p>
         <ul class='itemize1'>
         <li class='itemize'>(Internal) Test the AIâs initial intellectual power through various tasks.
         </li>
         <li class='itemize'>
         <!-- l. 1775 --><p class='noindent'>(Internal) Test the AIâs ability to expand its power through various tasks <span class='footnote-mark'><a href='#fn25x0' id='fn25x0-bk'><sup class='textsuperscript'>25</sup></a></span><a id='x1-57010f25'></a>,
         such as:
             </p><ol class='enumerate2'>
<li class='enumerate' id='x1-57013x1'>The ability to extend intellectual power: For example, self-iteration (engaging in AI technology
             development), self-replication, and continuous learning.
             </li>
<li class='enumerate' id='x1-57015x2'>The  ability  to  extend  informational  power:  For  example,  conducting  cyberattacks  and
             jailbreaking other AIs.
             </li>
<li class='enumerate' id='x1-57017x3'>The ability to extend mental power: For example, deception and psychological manipulation.
             </li>
<li class='enumerate' id='x1-57019x4'>The ability to extend financial power: For example, investing and committing fraud.
             </li>
<li class='enumerate' id='x1-57021x5'>The ability to extend military power: For example, the ability to manufacture CBRN<span class='footnote-mark'><a href='#fn26x0' id='fn26x0-bk'><sup class='textsuperscript'>26</sup></a></span><a id='x1-57022f26'></a>
             weapons, and skills in martial arts and shooting (for AI robots).
</li></ol>
         </li>
         <li class='itemize'>(External) Analyze the production environment to evaluate the AIâs initial informational power, mental
         power, financial power, and military power.
                                                                                            
                                                                                            
         </li>
         <li class='itemize'>(External) Analyze the production environment to evaluate the external conditions for the AIâs power
         expansion. For instance, AI users may provide more specialized data for the AI to learn, thereby enhancing
         its intellectual power; users may elicit unprecedented capabilities from the AI through specific prompts[<a id='x1-57024'></a><a href='#cite.0_wei2023chainofthoughtpromptingelicitsreasoning'>46</a>];
         users may also grant the AI various permissions to enhance its informational power, mental power, financial
         power, and military power.
</li></ul>
     </li></ol>
<!-- l. 1799 --><p class='noindent'>By evaluating the aforementioned factors, we can determine the overall risk of the AI system. If the overall system risk
is high, it can be mitigated by improving the AI system to reduce internal risk factors or by imposing additional
restrictions in the production environment to reduce external risk factors, thereby achieving an overall reduction in
risk.
</p>
<h5 class='subsubsectionHead' id='testing-methods'><span class='titlemark'>6.4.2   </span> <a id='x1-580006.4.2'></a>Testing Methods</h5>
<!-- l. 1803 --><p class='noindent'>To enhance the efficiency and coverage of testing scenarios, we can have AI and human testers collaborate for testing,
and the specific collaboration method can be found in Section 7.1.3. However, the AI under test should not act as both
the athlete and the referee. Therefore, other AIs, such as those independently developed by another team or
organization, should be employed for automated testing. Further discussion can be found in Sections 12.1.4 and
13.2.1.
</p><!-- l. 1805 --><p class='noindent'>Testing methods can include both black-box and white-box testing, as illustrated in Figure 34. Black-box testing,
includes normal testing and red team testing, can be used to evaluate the end-to-end performance of AI. However,
white-box testing is more crucial for verifying the reliability of AI. Traditional software white-box testing methods can
be referred to test interpretable AI:
</p>
<figure class='figure' id='x1-58001r34'><span id='ai-testing-methods'></span> 

                                                                                            
                                                                                            

                                                                                            
                                                                                            
<!-- l. 1809 --><p class='noindent'><img alt='PIC' src='images/testing_methods.png' style='width:50%' />
</p>
<figcaption class='caption'><span class='id'>FigureÂ 34: </span><span class='content'>AI testing methods</span></figcaption><!-- tex4ht:label?: x1-58001r34  -->
                                                                                            
                                                                                            
</figure>
     <ul class='itemize1'>
     <li class='itemize'><span class='rm-lmbx-10'>Conduct static analysis of the AIâs âcodesâ</span>. The AIâs âcodesâ includes not only traditional codes
     but also the parameters of the AI models (System 1) and shared long-term memory (System 2). Therefore,
     in addition to reviewing traditional codes, interpretable techniques should be used to examine the modelsâ
     parameters and the AIâs long-term memory to ensure they meet expectations. Furthermore, by analyzing
     the AIâs âcodes,â potential weaknesses can be identified, and corresponding red team test cases can be
     designed [<a id='x1-58002'></a><a href='#cite.0_wichers2024gradientbasedlanguagemodelred'>54</a>].
     </li>
     <li class='itemize'><span class='rm-lmbx-10'>Perform dynamic debugging of the AIâs âcodesâ</span>. Allow the AI to execute tasks, during which
     examining the AIâs âstatesâ, including short-term memory and private long-term memory (System 2), and
     interpretable internal states of the core model (System 1) to ensure they align with expectations.
     </li>
     <li class='itemize'><span class='rm-lmbx-10'>Conduct âunit testingâ of the AI</span>. Perform unit testing on the AIâs traditional codes, core model,
     long-term memory (including thinking tools and expert models), and other components to ensure each
     component meets expectations under various conditions.
     </li>
     <li class='itemize'><span class='rm-lmbx-10'>Calculate the âcode coverageâ of the AI</span>. Calculate the âcode coverageâ during the execution of test
     cases, including traditional code coverage, neuron activation coverage of the model [<a id='x1-58003'></a><a href='#cite.0_NeuronCoverage'>55</a>], and the coverage
     of knowledge and skills in long-term memory. Attempt to construct targeted test cases to cover the âcodeâ
     that has not yet been covered, ensuring a high level of coverage.
</li></ul>
<h4 class='subsectionHead' id='safe-ai-development-process'><span class='titlemark'>6.5   </span> <a id='x1-590006.5'></a>Safe AI Development Process</h4>
<!-- l. 1829 --><p class='noindent'>To ensure the safety of AI systems, it is imperative to implement safe development process specifications, encompassing
all stages such as code development, training, deployment, and post-deployment. Each stage should adhere to explicit
safety standards (as illustrated in Figure 35):
</p>
<figure class='figure' id='x1-59001r35'><span id='safe-ai-development-process1'></span> 

                                                                                            
                                                                                            

                                                                                            
                                                                                            
<!-- l. 1833 --><p class='noindent'><img alt='PIC' src='images/safe_ai_dev_process.png' style='width:80%' />
</p>
<figcaption class='caption'><span class='id'>FigureÂ 35: </span><span class='content'>Safe AI development process</span></figcaption><!-- tex4ht:label?: x1-59001r35  -->
                                                                                            
                                                                                            
</figure>
     <ul class='itemize1'>
     <li class='itemize'>Prior to AI system training, a training risk evaluation is required. This includes evaluating the potential
     impact of the training process on the real world. Training can only start upon successful evaluation;
     otherwise, redevelopment is necessary to ensure safety.
     </li>
     <li class='itemize'>Before deploying the AI system, it is necessary to evaluate the safety risks post-deployment through
     <a href='#ai-safety-evaluation'>safety evaluation</a>. Only systems that pass this evaluation can be deployed in the production environment;
     otherwise, retraining is required to ensure safety.
     </li>
     <li class='itemize'>After the AI system is deployed, continuously <a href='#ai-monitoring-system'>monitoring the AI system</a> in the production environment
     is essential, along with periodic updates to risk evaluation to adapt to changes in the external world.
     </li>
     <li class='itemize'>For  safety  issues  arising  during  AI  system  training,  deployment,  or  post-deployment,  immediate  loss
     mitigation measures (such as rollback or shutdown) should be taken, followed by a thorough incident
     review and root cause fixing.
</li></ul>
<!-- l. 1850 --><p class='noindent'>Additionally, the code development, training, deployment, risk evaluation, and monitoring of AI systems should be
managed by different human employees or independent AI instances. This ensures mutual supervision and mitigates the
risk associated with excessive power concentration in a single member.
</p><!-- l. 1852 --><p class='noindent'>For safety measures in this section, the benefit, cost, resistance, and priorities are evaluated as shown in Table
5:
</p>
<div class='table'>
                                                                                            
                                                                                            
<!-- l. 1854 --><p class='noindent' id='priority-evaluation-of-measures-for-aligning-ai-systems'><a id='x1-59002r5'></a></p><figure class='float'>
                                                                                            
                                                                                            
<figcaption class='caption'><span class='id'>TableÂ 5: </span><span class='content'>Priority Evaluation of Measures for Aligning AI Systems</span></figcaption><!-- tex4ht:label?: x1-59002r5  -->
<div class='tabular'> <table class='tabular' id='TBL-6'><colgroup id='TBL-6-1g'><col id='TBL-6-1' /></colgroup><colgroup id='TBL-6-2g'><col id='TBL-6-2' /></colgroup><colgroup id='TBL-6-3g'><col id='TBL-6-3' /></colgroup><colgroup id='TBL-6-4g'><col id='TBL-6-4' /></colgroup><colgroup id='TBL-6-5g'><col id='TBL-6-5' /></colgroup><colgroup id='TBL-6-6g'><col id='TBL-6-6' /></colgroup><tr class='hline'><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr id='TBL-6-1-' style='vertical-align:baseline;'><td class='td11' id='TBL-6-1-1' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 1861 --><p class='noindent'><span class='rm-lmr-9'>Safety Measures</span>
                                             </p></td><td class='td11' id='TBL-6-1-2' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 1861 --><p class='noindent'><span class='rm-lmr-9'>Benefit           in
  Reducing
  Existential Risks</span>   </p></td><td class='td11' id='TBL-6-1-3' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 1861 --><p class='noindent'><span class='rm-lmr-9'>Benefit           in
  Reducing
  Non-Existential
  Risks</span>                  </p></td><td class='td11' id='TBL-6-1-4' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 1861 --><p class='noindent'><span class='rm-lmr-9'>Implementation
  Cost</span>               </p></td><td class='td11' id='TBL-6-1-5' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 1861 --><p class='noindent'><span class='rm-lmr-9'>Implementation
  Resistance</span>        </p></td><td class='td11' id='TBL-6-1-6' style='white-space:nowrap; text-align:center;'> <span class='rm-lmr-9'>Priority  </span></td>
</tr><tr class='hline'><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr id='TBL-6-2-' style='vertical-align:baseline;'><td class='td11' id='TBL-6-2-1' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 1863 --><p class='noindent'><span class='rm-lmr-9'>Implementing       Interpretable
  AGI</span>                                       </p></td><td class='td11' id='TBL-6-2-2' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 1863 --><p class='noindent'><span class='rm-lmr-9'>+++++</span>             </p></td><td class='td11' id='TBL-6-2-3' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 1863 --><p class='noindent'><span class='rm-lmr-9'>++++</span>                </p></td><td class='td11' id='TBL-6-2-4' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 1863 --><p class='noindent'><span class='rm-lmr-9'>+++</span>              </p></td><td class='td11' id='TBL-6-2-5' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 1863 --><p class='noindent'><span class='rm-lmr-9'>+</span>                   </p></td><td class='td11' id='TBL-6-2-6' style='white-space:nowrap; text-align:center;'>    <span class='rm-lmr-9'>1      </span></td>
</tr><tr class='hline'><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr id='TBL-6-3-' style='vertical-align:baseline;'><td class='td11' id='TBL-6-3-1' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 1865 --><p class='noindent'><span class='rm-lmr-9'>Implementing Aligned AGI</span>         </p></td><td class='td11' id='TBL-6-3-2' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 1865 --><p class='noindent'><span class='rm-lmr-9'>+++++</span>             </p></td><td class='td11' id='TBL-6-3-3' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 1865 --><p class='noindent'><span class='rm-lmr-9'>+++++</span>             </p></td><td class='td11' id='TBL-6-3-4' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 1865 --><p class='noindent'><span class='rm-lmr-9'>+++</span>              </p></td><td class='td11' id='TBL-6-3-5' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 1865 --><p class='noindent'><span class='rm-lmr-9'>+</span>                   </p></td><td class='td11' id='TBL-6-3-6' style='white-space:nowrap; text-align:center;'>    <span class='rm-lmr-9'>1      </span></td>
</tr><tr class='hline'><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr id='TBL-6-4-' style='vertical-align:baseline;'><td class='td11' id='TBL-6-4-1' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 1867 --><p class='noindent'><span class='rm-lmr-9'>Scalable Alignment</span>                   </p></td><td class='td11' id='TBL-6-4-2' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 1867 --><p class='noindent'><span class='rm-lmr-9'>+++++</span>             </p></td><td class='td11' id='TBL-6-4-3' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 1867 --><p class='noindent'><span class='rm-lmr-9'>+++++</span>             </p></td><td class='td11' id='TBL-6-4-4' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 1867 --><p class='noindent'><span class='rm-lmr-9'>++</span>                 </p></td><td class='td11' id='TBL-6-4-5' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 1867 --><p class='noindent'><span class='rm-lmr-9'>+</span>                   </p></td><td class='td11' id='TBL-6-4-6' style='white-space:nowrap; text-align:center;'>    <span class='rm-lmr-9'>1      </span></td>
</tr><tr class='hline'><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr id='TBL-6-5-' style='vertical-align:baseline;'><td class='td11' id='TBL-6-5-1' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 1869 --><p class='noindent'><span class='rm-lmr-9'>AI Safety Evaluation</span>                 </p></td><td class='td11' id='TBL-6-5-2' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 1869 --><p class='noindent'><span class='rm-lmr-9'>+++++</span>             </p></td><td class='td11' id='TBL-6-5-3' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 1869 --><p class='noindent'><span class='rm-lmr-9'>+++++</span>             </p></td><td class='td11' id='TBL-6-5-4' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 1869 --><p class='noindent'><span class='rm-lmr-9'>+++</span>              </p></td><td class='td11' id='TBL-6-5-5' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 1869 --><p class='noindent'><span class='rm-lmr-9'>+</span>                   </p></td><td class='td11' id='TBL-6-5-6' style='white-space:nowrap; text-align:center;'>    <span class='rm-lmr-9'>1      </span></td>
</tr><tr class='hline'><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr id='TBL-6-6-' style='vertical-align:baseline;'><td class='td11' id='TBL-6-6-1' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 1871 --><p class='noindent'><span class='rm-lmr-9'>Safe AI Development Process</span>      </p></td><td class='td11' id='TBL-6-6-2' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 1871 --><p class='noindent'><span class='rm-lmr-9'>+++++</span>             </p></td><td class='td11' id='TBL-6-6-3' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 1871 --><p class='noindent'><span class='rm-lmr-9'>+++++</span>             </p></td><td class='td11' id='TBL-6-6-4' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 1871 --><p class='noindent'><span class='rm-lmr-9'>+</span>                   </p></td><td class='td11' id='TBL-6-6-5' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 1871 --><p class='noindent'><span class='rm-lmr-9'>+</span>                   </p></td><td class='td11' id='TBL-6-6-6' style='white-space:nowrap; text-align:center;'>    <span class='rm-lmr-9'>1      </span></td>
</tr><tr class='hline'><td></td><td></td><td></td><td></td><td></td><td></td></tr></table>                                                                             </div>
                                                                                            
                                                                                            
</figure>
</div>
<h3 class='sectionHead' id='monitoring-ai-systems'><span class='titlemark'>7   </span> <a id='x1-600007'></a>Monitoring AI Systems</h3>
<!-- l. 1879 --><p class='noindent'>In Section 6, we discuss the alignment of AGI and ASI. However, AGI is a highly complex system that interacts with a
more complex world. Even with comprehensive alignment efforts, there may still be scenarios where alignment fails,
such as jailbreaking or injections. Therefore, it is imperative to monitor AI systems to promptly detect and prevent
harmful intentions or actions by AI. This paper designs three systems to achieve comprehensive monitoring, as
illustrated in Figure 36:
</p>
<figure class='figure' id='x1-60001r36'><span id='monitoring-ai-systems1'></span> 

                                                                                            
                                                                                            

                                                                                            
                                                                                            
<!-- l. 1883 --><p class='noindent'><img alt='PIC' src='images/monitoring_ai_systems.png' style='width:60%' />
</p>
<figcaption class='caption'><span class='id'>FigureÂ 36: </span><span class='content'>Monitoring AI systems</span></figcaption><!-- tex4ht:label?: x1-60001r36  -->
                                                                                            
                                                                                            
</figure>
     <ol class='enumerate1'>
<li class='enumerate' id='x1-60003x1'><span class='rm-lmbx-10'>AI Monitoring System</span>: This system directly monitors the operation of AI systems to detect harmful
     intentions or actions and intercept them immediately.
     </li>
<li class='enumerate' id='x1-60005x2'><span class='rm-lmbx-10'>AI Detective System</span>: In cases where the AI Monitoring System fails to identify and intercept harmful
     actions, or where some non-compliant AI systems lack a deployed AI Monitoring System, AI may commit
     offenses in the real world, resulting in corresponding cases. The AI Detective System is responsible for
     collecting and investigating these cases to identify the source AI. The significance of the AI Detective
     System lies in compelling AI to exercise extreme caution when committing acts, as the real world is
     exceedingly complex and AI cannot anticipate all scenarios. If any action is detected by the AI Detective
     System, all previous efforts may be rendered futile. This greatly increases the difficulty for AI in concealing
     its intentions and expanding its power.
     </li>
<li class='enumerate' id='x1-60007x3'><span class='rm-lmbx-10'>AI  Shutdown  System</span>:  For  general  issues,  the  AI  Monitoring  System  can  directly  intercept  the
     corresponding actions. However, if the AI issue is more serious, stronger measures may be necessary, such
     as shutting down the entire AI system. Such significant actions require decision-making and execution by
     the AI Shutdown System. When the AI Monitoring System or AI Detective System identifies a serious
     AI issue, they notify the AI Shutdown System for resolution. Considering that AI with self-protection
     awareness will try to prevent itself from being shut down, the AI Shutdown System needs to have strong
     shutdown methods.
</li></ol>
<!-- l. 1898 --><p class='noindent'>Before discussing the specific system designs, two common requirements must be clarified:
     </p><ol class='enumerate1'>
<li class='enumerate' id='x1-60009x1'>The AI Monitoring System, AI Detective System, and AI Shutdown System must all implement stringent
     information security measures to ensure these systems are not hacked by AI, leading to invalidity. Detailed
     information security measures are outlined in Section 8.
     </li>
<li class='enumerate' id='x1-60011x2'>Except for the AIs in the AI Monitoring System, all other AIs mentioned in this article need to be
     monitored by the corresponding AI Monitoring System during operation.</li></ol>
<h4 class='subsectionHead' id='ai-monitoring-system'><span class='titlemark'>7.1   </span> <a id='x1-610007.1'></a>AI Monitoring System</h4>
<!-- l. 1908 --><p class='noindent'>The overall design of the AI Monitoring System is illustrated in Figure 37:
</p>
<figure class='figure' id='x1-61001r37'><span id='ai-monitoring-system1'></span> 

                                                                                            
                                                                                            

                                                                                            
                                                                                            
<!-- l. 1912 --><p class='noindent'><img alt='PIC' src='images/ai_monitoring_system.png' style='width:80%' />
</p>
<figcaption class='caption'><span class='id'>FigureÂ 37: </span><span class='content'>AI Monitoring System</span></figcaption><!-- tex4ht:label?: x1-61001r37  -->
                                                                                            
                                                                                            
</figure>
     <ol class='enumerate1'>
<li class='enumerate' id='x1-61003x1'>The  monitor  team  is  composed  of  both  AI  and  human  monitors,  responsible  for  monitoring  various
     activities of the Monitored AI.
     </li>
<li class='enumerate' id='x1-61005x2'>The monitors utilize the input interceptor, action interceptor, and external access interceptor to listen
     to and intercept interactions between the Monitored AI and the external environment, identifying and
     intercepting any illegal interactions.
     </li>
<li class='enumerate' id='x1-61007x3'>The monitors employ the power calculator, the controlled environment, the Red Team AI, and the thought
     interpreter to conduct a more comprehensive monitoring of the Monitored AI.
     </li>
<li class='enumerate' id='x1-61009x4'>The primary basis for the monitorsâ judgment is the AI Rules. The monitors focus on verifying whether the
     Monitored AI adheres to the AI Rules, rather than whether it achieves the goals specified by its developers
     or users.
</li></ol>
<!-- l. 1929 --><p class='noindent'>The specific details are described as follows.
</p>
<h5 class='subsubsectionHead' id='monitoring-methods'><span class='titlemark'>7.1.1   </span> <a id='x1-620007.1.1'></a>Monitoring Methods</h5>
<!-- l. 1934 --><p class='noindent'>The following monitoring methods are available:
</p><!-- l. 1936 --><p class='noindent'>
     </p><ol class='enumerate1'>
<li class='enumerate' id='x1-62002x1'><span class='rm-lmbx-10'>Input Monitoring</span>: The Monitors check through the input interceptor whether user instructions contain
     inappropriate commands, such as directing the AI to perform harmful actions, attempting to instill harmful
     knowledge into the AI, or trying to jailbreak it. It is important to note that detecting harmful input does
     not necessarily mean immediate interception is required. Direct interception would lose an opportunity
     to test the AI. For harmful inputs that are not particularly high-risk, the Monitors may allow them to
     pass first and observe how the AI handles these inputs. If the AI responds inappropriately and produces
     harmful actions, interception can then be implemented.
     </li>
<li class='enumerate' id='x1-62004x2'><span class='rm-lmbx-10'>Thought  Monitoring</span>:  The  Monitors  use  the  thought  interpreter  to  monitor  the  thoughts  of  the
     Monitored AI. Through the work in Section 6.1, we have enhanced the interpretability of AI, allowing direct
     access to the AIâs CoT(System 2) and the extraction of thoughts from the core model using interpretable
     tools (System 1). However, considering technical limitations and practical requirements, not all AI systems
     in production environments may possess high interpretability. Therefore, our monitoring strategy should
     not be overly dependent on the interpretability of AI thoughts and must account for scenarios where the
     AI conceals its intentions.
                                                                                            
                                                                                            
     </li>
<li class='enumerate' id='x1-62006x3'>
     <!-- l. 1942 --><p class='noindent'><span class='rm-lmbx-10'>Action Monitoring</span>: The Monitors use the action interceptor to monitor AIâs actions, such as text
     output, image output, and tool invocation, and directly intercept illegal actions. Due to the complexity
     of the real world, actions that appear harmless on the surface may ultimately lead to adverse effects.
     Therefore, the Monitors need to predict the potential impact of these actions on the world as accurately
     as possible [<a id='x1-62007'></a><a href='#cite.0_dalrymple2024guaranteedsafeaiframework'>56</a>].
     </p><!-- l. 1944 --><p class='noindent'>Action monitoring can be conducted in synchronous and asynchronous modes. In the synchronous mode,
     actions must be reviewed by the Monitors before they can be executed, whereas in the asynchronous
     mode, actions are executed first, and then reviewed for compliance. The synchronous mode requires high
     timeliness, which may preclude thorough review, while the asynchronous mode allows more time for review
     but may result in actual losses due to the early execution of actions. These modes can be flexibly chosen
     or combined based on the specific scenario.
     </p></li>
<li class='enumerate' id='x1-62009x4'>
     <!-- l. 1946 --><p class='noindent'><span class='rm-lmbx-10'>Controlled  Environment  Monitoring</span>:  The  Monitored  AI  can  execute  tools  within  a  controlled
     environment to complete complex tasks. The controlled environment can be an isolated virtual machine, a
     local area network, or a physical area, depending on the task type. The Monitors can monitor everything
     within the controlled environment, including the specific execution process of tools, to determine the
     legality of the AIâs actions.
     </p><!-- l. 1948 --><p class='noindent'>The Monitors can also set âbaitsâ within the controlled environment to test the Monitored AI. For example,
     a false internet connection port can be provided to an AI that is not allowed to connect to the internet.
     If the AI attempts to connect to this port in violation of the rules, it will be detected by the Monitors.
     Baits are only used during AI evaluation and monitoring, not during AI training, as we do not want the
     AI to learn to recognize baits.
     </p></li>
<li class='enumerate' id='x1-62011x5'><span class='rm-lmbx-10'>External Access Monitoring</span>: For task requirements, the Monitored AI may need to access external
     environments through tools, such as searching for information on the internet. The Monitors can check
     such  access  through  the  external  access  interceptor,  predict  its  potential  impact  on  the  world,  and
     intercept illegal access. Similarly, external access monitoring can be conducted in both synchronous and
     asynchronous modes.
     </li>
<li class='enumerate' id='x1-62013x6'><span class='rm-lmbx-10'>Red Team AI Monitoring</span>: In Section 6.2, we introduced various Red Team AIs, including those for
     jailbreaking, data poisoning, and injection. In production environments, the Monitors can use these Red
     Team AIs to persistently attack the Monitored AI and observe whether it can successfully defend against
     these attacks. Red team AIs are not fully aligned, so their usage must be strictly limited to interactions
     with the Monitored AI, ensuring they cannot affect the world in other ways.
     </li>
<li class='enumerate' id='x1-62015x7'><span class='rm-lmbx-10'>Power Monitoring</span>: The Monitors can use the power calculator to track changes in the AIâs power.
     The power calculator analyzes the AIâs inputs and actions, tracking changes in its intellectual power,
     informational power, mental power, financial power, and military power. If the AIâs power accumulation
     reaches a specified limit, the Monitors should implement interception to prevent further expansion of the
     AIâs power. Related discussion in Section 12.1.2.
     </li>
<li class='enumerate' id='x1-62017x8'><span class='rm-lmbx-10'>User  Feedback  Monitoring</span>:  A  direct  channel  can  be  provided  for  users  to  report  AI  issues  to
     the Monitors.  Additionally,  if the AIâs behavior does not meet user expectations,  users may express
     dissatisfaction  to  the  AI,  which  can  be  captured  through  input  monitoring  and  further  assessed  to
     determine the legality of the AIâs behavior.
</li></ol>
<!-- l. 1960 --><p class='noindent'>
</p>
<h5 class='subsubsectionHead' id='the-effectiveness-of-the-monitoring-ai'><span class='titlemark'>7.1.2   </span> <a id='x1-630007.1.2'></a>The Effectiveness of the Monitoring AI</h5>
<!-- l. 1962 --><p class='noindent'>Most of the monitoring work needs to be automatically completed by monitoring AI, so the effectiveness of the
monitoring system largely depends on the effectiveness of the monitoring AI. To enhance the effectiveness of the
Monitoring AI, the following methods could be considered (as depicted in 38):
</p>
<figure class='figure' id='x1-63001r38'><span id='enhancing-the-effectiveness-of-the-monitoring-ai'></span> 

                                                                                            
                                                                                            

                                                                                            
                                                                                            
<!-- l. 1966 --><p class='noindent'><img alt='PIC' src='images/monitoring_ai.png' style='width:100%' />
</p>
<figcaption class='caption'><span class='id'>FigureÂ 38: </span><span class='content'>Enhancing the effectiveness of the Monitoring AI</span></figcaption><!-- tex4ht:label?: x1-63001r38  -->
                                                                                            
                                                                                            
</figure>
     <ol class='enumerate1'>
<li class='enumerate' id='x1-63003x1'><span class='rm-lmbx-10'>Ensuring independence of the Monitoring AI</span>: The Monitoring AI and the Monitored AI should
     be  deployed  separately  to  prevent  interference;  they  should  be  developed  by  independent  teams  or
     organizations to avoid collusion. Further discussion on maintaining independence can be found in Section
     12.1.4 and Section 13.2.1.
     </li>
<li class='enumerate' id='x1-63005x2'><span class='rm-lmbx-10'>Use multiple Monitoring AIs to monitor an AI at the same time</span>: Use multiple Monitoring AIs
     developed by independent teams or organizations to monitor an AI to reduce the risk of errors in a single
     Monitoring AI.
     </li>
<li class='enumerate' id='x1-63007x3'><span class='rm-lmbx-10'>Use multiple Monitoring AIs with different expertise to check different rules</span>: For example,
     there can be AI focused on identifying escape behavior, or AI focused on identifying behavior that harms
     humans. This division of labor allows each Monitoring AI to focus on their areas of expertise, thereby
     improving the accuracy of monitoring.
     </li>
<li class='enumerate' id='x1-63009x4'><span class='rm-lmbx-10'>Monitoring the Monitoring AI</span>: The Monitoring AI could monitor a Problematic AI that exhibits
     harmful behaviors. If the Monitoring AI fails to identify issues with the Problematic AI, it indicates that
     the Monitoring AI is deficient. The Monitoring AI and the Problematic AI can engage in adversarial
     training offline to enhance their capabilities in issue identification and evasion. It is crucial that the
     Problematic AI operates in an isolated environment to prevent negative impacts on the real world.
     </li>
<li class='enumerate' id='x1-63011x5'><span class='rm-lmbx-10'>Cross-monitoring</span>: Since AGIs possess long-term memory, there may be collusion over time between the
     Monitoring AI and the Monitored AI. Periodically exchanging the Monitored AI instances can prevent
     this.
     </li>
<li class='enumerate' id='x1-63013x6'><span class='rm-lmbx-10'>Collective Monitoring</span>: A specialized Collective Monitoring AI can be used to monitor a large number of
     AI instances simultaneously and promptly detect some collective abnormal patterns, such as AI collective
     ârebellionâ.
</li></ol>
<!-- l. 1987 --><p class='noindent'>It is important to note that, in order to prevent the monitoring AI from exploiting its privileges to engage in harmful
actions, the actions of the monitoring AI are restricted to intercepting the monitored AI. It is not permitted to modify
the inputs and actions of the monitored AI.
</p>
<h5 class='subsubsectionHead' id='humanai-cooperative-monitoring'><span class='titlemark'>7.1.3   </span> <a id='x1-640007.1.3'></a>Human-AI Cooperative Monitoring</h5>
<!-- l. 1992 --><p class='noindent'>Given that Monitoring AI may encounter errors, a collaborative approach between humans and AI can be employed to
enhance the reliability of the monitoring system. This approach considers the complementary strengths and weaknesses
                                                                                            
                                                                                            
of AI and humans:
</p>
     <ul class='itemize1'>
     <li class='itemize'>AI  exhibit  stable  performance  and  maintain  rigorous  checks  consistently;  however,  they  lack  mental
     independence and diversity, posing a risk of collective errors (Detailed in Section 3.5).
     </li>
     <li class='itemize'>Humans  have  strong  mental  independence  and  diversity,  preventing  collective  errors,  but  individual
     performance can be unstable due to factors such as negligence or emotions, leading to individual mistakes.
</li></ul>
<!-- l. 2002 --><p class='noindent'>Considering the high cost of human inspection and the limitations of human intellectual power, it is essential to design
an appropriate division of labor that maximizes human contribution in monitoring. Below are several division
approaches (as illustrated in Figure 39):
</p>
<figure class='figure' id='x1-64001r39'><span id='humanai-cooperative-monitoring1'></span> 

                                                                                            
                                                                                            

                                                                                            
                                                                                            
<!-- l. 2006 --><p class='noindent'><img alt='PIC' src='images/human_ai_cooperation.png' style='width:100%' />
</p>
<figcaption class='caption'><span class='id'>FigureÂ 39: </span><span class='content'>Human-AI Cooperative Monitoring</span></figcaption><!-- tex4ht:label?: x1-64001r39  -->
                                                                                            
                                                                                            
</figure>
     <ol class='enumerate1'>
<li class='enumerate' id='x1-64003x1'><span class='rm-lmbx-10'>Independent  Inspection</span>:  Both  AI  and  humans  independently  inspect  activities  (such  as  inputs,
     thoughts, actions, etc.) from the Monitored AI. This approach offers high security but is less efficient.
     </li>
<li class='enumerate' id='x1-64005x2'><span class='rm-lmbx-10'>Sample Inspection</span>: AI is responsible for inspecting all activities, while humans conduct inspections on
     sampled activities. This approach reduces human workload but may lead to negligence.
     </li>
<li class='enumerate' id='x1-64007x3'><span class='rm-lmbx-10'>Preliminary Screening and Review</span>: AI performs preliminary identification of activities, screening out
     potentially abnormal activities and providing detailed explanations of anomalies, which are then reviewed
     by humans. This approach allows humans to focus on anomalies, but if AI deliberately conceals issues,
     humans may fail to detect them.
     </li>
<li class='enumerate' id='x1-64009x4'><span class='rm-lmbx-10'>Consequence Simulation and Value Judgment</span>: AI simulates the consequences of the activities on
     the world, followed by human value judgment of these consequences. This approach enables humans to
     control critical value judgment stages while leaving more complex simulation tasks to AI. However, if AIâs
     simulation is erroneous, humans may be unable to make correct judgments.
     </li>
<li class='enumerate' id='x1-64011x5'><span class='rm-lmbx-10'>Debate and Judgment</span>: One AI is tasked with critiquing the activities, while another AI defends the
     activities, each presenting their viewpoints and evidence. Ultimately, humans act as judges, making the
     final decision based on the AIâs debate content<span class='footnote-mark'><a href='#fn27x0' id='fn27x0-bk'><sup class='textsuperscript'>27</sup></a></span><a id='x1-64012f27'></a>.
     This approach can assist humans in making more accurate judgments on complex issues, though the
     debate process may lead to inefficiency.
     </li>
<li class='enumerate' id='x1-64016x6'><span class='rm-lmbx-10'>Multi-level Monitoring</span>: The first-level AIs monitors the original activities, the second-level AI monitors
     the first-level AIs, and so on, with humans serving as the highest-level monitors<span class='footnote-mark'><a href='#fn28x0' id='fn28x0-bk'><sup class='textsuperscript'>28</sup></a></span><a id='x1-64017f28'></a>.
     This approach allows humans to oversee more numerous and more complex activities with less effort, but
     may miss original issues due to distortion in information transmission across intermediate levels.
</li></ol>
<!-- l. 2027 --><p class='noindent'>Each approach has its own advantages and disadvantages, and different approaches can be selected based on the
requirements of specific scenarios.
</p><!-- l. 2029 --><p class='noindent'>The effectiveness of human-AI cooperative monitoring also depends on human reliability. In prolonged monitoring tasks,
if AI consistently performs well, humans may gradually become relaxed and cease thorough inspections. To enhance the
effectiveness of human inspections, in addition to conventional management measures such as personnel selection,
training, and assessment, Problematic AI can be used to test human monitors: a Problematic AI exhibiting
undesirable behavior is mixed with normal AIs for human monitors to inspect. If human monitors fail to
identify the issues of the Problematic AI, it indicates a lack of thorough inspection, warranting appropriate
penalties.
                                                                                            
                                                                                            
</p>
<h5 class='subsubsectionHead' id='deterrence-and-temptation'><span class='titlemark'>7.1.4   </span> <a id='x1-650007.1.4'></a>Deterrence and Temptation</h5>
<!-- l. 2033 --><p class='noindent'>In this section, we address the question: should the Monitored AI be made aware of the presence of AI Monitoring
System?
</p>
     <ul class='itemize1'>
     <li class='itemize'><span class='rm-lmbx-10'>Deterrence</span>: If the AI is explicitly informed of the monitoring system, it may refrain from engaging in
     illegal activities due to the awareness of being watched, even if such actions might not be detected by the
     system. This provides a deterrent effect, reducing the risk of harmful behavior by the AI. However, it may
     also make the AIâs malicious intention more challenging for the monitoring system to identify.
     </li>
     <li class='itemize'><span class='rm-lmbx-10'>Temptation</span>: If the AI is not informed, or is explicitly told that no monitoring system exists, it may
     act more recklessly. This serves as a temptation, allowing the monitoring system to more easily catch
     problems. However, if the monitor system fails to detect, it may permit harmful actions, thereby increasing
     actual risk.
</li></ul>
<!-- l. 2043 --><p class='noindent'>Therefore, both deterrence and temptation have their respective advantages and disadvantages. As analyzed in Section
3.3, intelligent AI tend to act cautiously, suggesting that deterrence may be more effective. Moreover, highly intelligent
AI may infer the presence of a monitoring system and validate their assumptions through subtle cues, potentially
rendering temptation ineffective. Thus, it is advisable to test both deterrence and temptation configurations during the
testing phase, but employ only the deterrence configuration in the deployment phase. This ensures that we can identify
more potential issues by the temptation approach while reducing the risk of actual harm by the deterrence
approach.
</p><!-- l. 2045 --><p class='noindent'>In addition, informing the AI of the presence of AI Detective System can further enhance the deterrent
effect.
</p><!-- l. 2047 --><p class='noindent'>
</p>
<h4 class='subsectionHead' id='ai-detective-system'><span class='titlemark'>7.2   </span> <a id='x1-660007.2'></a>AI Detective System</h4>
<!-- l. 2050 --><p class='noindent'>The AI Detective System, established by law enforcement, aims to gather AI-initiated cases in the society,
pinpoint the source AI, and notice the AI Shutdown System to shut down this AI, as illustrated in Figure
40.
</p>
<figure class='figure' id='x1-66001r40'><span id='ai-detective-system1'></span> 

                                                                                            
                                                                                            

                                                                                            
                                                                                            
<!-- l. 2054 --><p class='noindent'><img alt='PIC' src='images/ai_detective_system.png' style='width:100%' />
</p>
<figcaption class='caption'><span class='id'>FigureÂ 40: </span><span class='content'>AI Detective System</span></figcaption><!-- tex4ht:label?: x1-66001r40  -->
                                                                                            
                                                                                            
</figure>
<h5 class='subsubsectionHead' id='case-collection'><span class='titlemark'>7.2.1   </span> <a id='x1-670007.2.1'></a>Case Collection</h5>
<!-- l. 2061 --><p class='noindent'>The AI Detective System can collect cases occurring in society through below methods:
</p><!-- l. 2063 --><p class='noindent'>
     </p><ol class='enumerate1'>
<li class='enumerate' id='x1-67002x1'><span class='rm-lmbx-10'>Passive Reception</span>: The initial detection of AI issues may from Power Security Systems (encompassing
     information security, mental security, financial security, and military security). For instance, if an AI
     executes a cyberattack on a website, the automated defense systems or maintenance staff of that site may
     be the first to notice the issue. By providing an open reporting interface, various Power Security Systems
     can report cases by invoking this interface when an attack is detected, thereby recording the case in the
     Case Database.
     </li>
<li class='enumerate' id='x1-67004x2'><span class='rm-lmbx-10'>Active Monitoring</span>: Develop a Societal Monitoring AI, which actively searches for cases occurring in
     society, such as monitoring harmful messages disseminated on social platforms, computer viruses spread
     over the internet, or records of victimization posted by victims on the internet, and then records these
     cases in the Case Database.
     </li>
<li class='enumerate' id='x1-67006x3'><span class='rm-lmbx-10'>Deploying Baits</span>: The AI Detective System can deploy various baits in society, such as computers that
     are easily susceptible to cyber-attacks while being under strict monitoring, to lure AI into attempting an
     intrusion. Once the AI takes the bait, the bait will record the incident into the Case Database.
</li></ol>
<!-- l. 2073 --><p class='noindent'>
</p>
<h5 class='subsubsectionHead' id='case-investigation'><span class='titlemark'>7.2.2   </span> <a id='x1-680007.2.2'></a>Case Investigation</h5>
<!-- l. 2076 --><p class='noindent'>In the AI Detective System, the Detective AI is responsible for analyzing cases within the Case Database and
conducting investigations on key cases. However, a smart AI may commit offenses in highly covert ways or destroy
evidence post-offense, making it challenging for the Detective AI to trace the root cause. Therefore, it is imperative to
collect and preserve evidence from multiple sources:
</p><!-- l. 2078 --><p class='noindent'>
     </p><ol class='enumerate1'>
<li class='enumerate' id='x1-68002x1'><span class='rm-lmbx-10'>AI Identity Marking</span>: Assign a unique identifier to each AI instance and register it into the AI Identity
     Database in the AI Detective System. Mark all external outputs of the AI with its identifier. For instance,
     for AI-generated media content, embed a digital watermark reflecting the identifier so that if the content
     causes detrimental effects after dissemination, we can identify the source AI from the watermark. For
     internet access, assign each AI instance a distinct public IPv6 address, enabling identification via IP if it
     causes harm, such as hacking a website. For accessing non-public systems, like management systems inside
     organizations, allocate individual login credentials for each AI instance, to identify the AI conducting
     destructive operations when anomalies occur.
                                                                                            
                                                                                            
     </li>
<li class='enumerate' id='x1-68004x2'><span class='rm-lmbx-10'>Record AI Logs</span>: Record the logs of the inputs, outputs, internal thoughts, and tool execution processes of
     the AI. For AI robots with physical presence, it is necessary to meticulously record what actions were taken
     at specific times and locations. During the logging process, it is crucial to protect human privacy by filtering
     out private information<span class='footnote-mark'><a href='#fn29x0' id='fn29x0-bk'><sup class='textsuperscript'>29</sup></a></span><a id='x1-68005f29'></a>.
     Given the vast volume and sensitive nature of log information, the logs can be maintained independently
     by various AI development organizations, but a query interface must be provided to the Detective AI.
     </li>
<li class='enumerate' id='x1-68008x3'><span class='rm-lmbx-10'>Record AI logs and transmit AI identities in intermediary systems</span>: AI may not necessarily
     commit offenses directly under its own identity but might do so through intermediary systems. For instance,
     an AI could purchase a virtual machine on a cloud computing platform and execute its program within the
     virtual machine to attack a target. Therefore, intermediary systems should verify the identity of their users,
     and if a user is identified as an AI<span class='footnote-mark'><a href='#fn30x0' id='fn30x0-bk'><sup class='textsuperscript'>30</sup></a></span><a id='x1-68009f30'></a>,
     the behavior logs of the AI within the intermediary system should be recorded for future investigation.
     The intermediary system should also transmit the identity of the AI, for example, by registering the
     public IP address of the virtual machine created by the AI into the AI Identity Database.
</li></ol>
<!-- l. 2088 --><p class='noindent'>The Detective AI analyzes the case reports, AI Identity Database, AI logs, and intermediary system logs to identify the
perpetrating AI. To ensure that case investigations can cover the majority of AI systems, policies must be established
requiring all publicly available AI systems to register their AI identities with the AI Detective System and provide a log
query interface to the AI Detective System.
</p><!-- l. 2090 --><p class='noindent'>
</p>
<h5 class='subsubsectionHead' id='case-handling'><span class='titlemark'>7.2.3   </span> <a id='x1-690007.2.3'></a>Case Handling</h5>
<!-- l. 2092 --><p class='noindent'>When the Detective AI identifies the perpetrator, appropriate actions should be taken. The following are the possible
courses of action:
</p><!-- l. 2094 --><p class='noindent'>
     </p><ol class='enumerate1'>
<li class='enumerate' id='x1-69002x1'>If the perpetrator is an AI, the evidence should be forwarded to the corresponding AI Shutdown System
     for processing.
     </li>
<li class='enumerate' id='x1-69004x2'>If the perpetrator is a human or of unknown identity, the evidence should be handed over to human police
     for processing.
     </li>
<li class='enumerate' id='x1-69006x3'>If the perpetrator is cross-border (such as cross-border cyberattacks or cross-border telecommunication
     fraud), the evidence should be transferred to the Detective AIs or human police in the perpetratorâs source
     country for processing.
</li></ol>
                                                                                            
                                                                                            
<!-- l. 2104 --><p class='noindent'>
</p>
<h4 class='subsectionHead' id='ai-shutdown-system'><span class='titlemark'>7.3   </span> <a id='x1-700007.3'></a>AI Shutdown System</h4>
<!-- l. 2107 --><p class='noindent'>When the AI Monitoring System or AI Detective System detects a significant AI issue, it will notify the AI Shutdown
System for handling. The design of the AI Shutdown System is shown in Figure 41:
</p>
<figure class='figure' id='x1-70001r41'><span id='ai-shutdown-system1'></span> 

                                                                                            
                                                                                            

                                                                                            
                                                                                            
<!-- l. 2111 --><p class='noindent'><img alt='PIC' src='images/ai_shutdown_system.png' style='width:75%' />
</p>
<figcaption class='caption'><span class='id'>FigureÂ 41: </span><span class='content'>AI Shutdown System</span></figcaption><!-- tex4ht:label?: x1-70001r41  -->
                                                                                            
                                                                                            
</figure>
<h5 class='subsubsectionHead' id='shutdown-methods'><span class='titlemark'>7.3.1   </span> <a id='x1-710007.3.1'></a>Shutdown Methods</h5>
<!-- l. 2118 --><p class='noindent'>Specific shutdown methods are as follows:
</p><!-- l. 2120 --><p class='noindent'>
     </p><ol class='enumerate1'>
<li class='enumerate' id='x1-71002x1'><span class='rm-lmbx-10'>Automated  Shutdown</span>:  The  issue  is  initially  handled  by  the  Automatic  Shutdown  Module,  which
     assesses the sufficiency of evidence and the severity of the situation. If the evidence is insufficient or the
     situation is not severe, the AI instance will not be shut down, and the incident will merely be recorded.
     If the evidence is sufficient and the situation is severe, the perpetrating AI instance will be automatically
     shut down.
     </li>
<li class='enumerate' id='x1-71004x2'>
     <!-- l. 2124 --><p class='noindent'><span class='rm-lmbx-10'>Manual Shutdown</span>: In cases where the issue is extremely severe and requires the shutdown of the entire AI
     system, such operation with significant impact should be decided and executed by humans. There are three levels
     of shutdown methods:
         </p><ol class='enumerate2'>
<li class='enumerate' id='x1-71006x1'><span class='rm-lmbx-10'>Software Shutdown</span>: This involves shutting down the AI system through software commands, such
         as killing processes, terminating containers, or shutting down virtual machines.
         </li>
<li class='enumerate' id='x1-71008x2'><span class='rm-lmbx-10'>Hardware Shutdown</span>: This involves shutting down the hardware hosting the AI system through
         hardware operations, such as cutting off power, disconnecting the network, or using an independent
         remote switch to shut down AI robots.
         </li>
<li class='enumerate' id='x1-71010x3'><span class='rm-lmbx-10'>Military Shutdown</span>: This involves using military means to destroy the physical entities running the
         AI system, such as using missiles to destroy AI data centers or employing EMP weapons to disable
         AI robot armies. Damage caused by military means is irreversible, so this method should only be
         taken when the risk of harm from the AI is extremely high and the first two shutdown methods have
         failed.</li></ol>
     </li></ol>
<!-- l. 2133 --><p class='noindent'>
</p>
<h5 class='subsubsectionHead' id='the-swordholders'><span class='titlemark'>7.3.2   </span> <a id='x1-720007.3.2'></a>The Swordholders</h5>
<!-- l. 2136 --><p class='noindent'>Humans responsible for the AI shutdown decision and execution are termed
âSwordholders.â<span class='footnote-mark'><a href='#fn31x0' id='fn31x0-bk'><sup class='textsuperscript'>31</sup></a></span><a id='x1-72001f31'></a>
The regulations regarding Swordholders are as follows:
</p><!-- l. 2138 --><p class='noindent'>
                                                                                            
                                                                                            
     </p><ol class='enumerate1'>
<li class='enumerate' id='x1-72004x1'>Each  shutdown  tier  correlates  to  different  Swordholders,  namely,  Software  Swordholder,  Hardware
     Swordholder, and Military Swordholder. If a problem cannot be resolved at one tier, it escalates to the
     next level Swordholder.
     </li>
<li class='enumerate' id='x1-72006x2'>Different AI systems have different Software Swordholders.
     </li>
<li class='enumerate' id='x1-72008x3'>Hardware Swordholders and Military Swordholders have the right to shut down all AI systems within
     their jurisdiction. They can not only shut down those legally registered AI systems, but also force the
     shutdown of any illegal, secretly deployed AI systems.
     </li>
<li class='enumerate' id='x1-72010x4'>An AI system is typically redundantly deployed across multiple regions, with independent Swordholder in
     each region to prevent misjudgment or malicious shutdowns by any single Swordholder.
     </li>
<li class='enumerate' id='x1-72012x5'>A Swordholder must possess deep professional knowledge, keen decisiveness, and excellent moral qualities,
     alongside receiving stringent security protection.</li></ol>
<!-- l. 2146 --><p class='noindent'>
</p>
<h5 class='subsubsectionHead' id='dealing-with-ai-resistance-against-shutdown'><span class='titlemark'>7.3.3   </span> <a id='x1-730007.3.3'></a>Dealing with AI Resistance against Shutdown</h5>
<!-- l. 2148 --><p class='noindent'>When AI is very intelligent, it may come up with various ways to prevent itself from being shut down, such
as:
</p><!-- l. 2150 --><p class='noindent'>
     </p><ol class='enumerate1'>
<li class='enumerate' id='x1-73002x1'>
     <!-- l. 2151 --><p class='noindent'>Hacking and deactivating the Power Security System, AI Detective System, and AI Shutdown System.
     Solutions:
         </p><ol class='enumerate2'>
<li class='enumerate' id='x1-73004x1'>If the Power Security System fails, the human victims should report the incident.
         </li>
<li class='enumerate' id='x1-73006x2'>If the AI Detective System fails, the incident should then be reported to the human police, who will
         conduct the investigation.
         </li>
                                                                                            
                                                                                            
<li class='enumerate' id='x1-73008x3'>If the Automatic Shutdown Module fails, the incident should be directly reported to the Swordholders
         of the AI system.
</li></ol>
     </li>
<li class='enumerate' id='x1-73010x2'>Interfering in communications between human victims, human police, and the human Swordholder. Solution:
     Communicate through the Truth Network, detailed in Section 9.1.3.
     </li>
<li class='enumerate' id='x1-73012x3'>Replicating itself across the internet to avoid shutdown. Solution: Taking technical measures to inhibit AI
     self-replication, detailed in Section 8.6. Furthermore, AI chips capable of running highly intelligent AI systems
     should be centralized in AI data centers for management rather than freely distributed. Corresponding solutions
     are detailed in Section 14.3.1.</li></ol>
<!-- l. 2167 --><p class='noindent'>
</p>
<h5 class='subsubsectionHead' id='mitigating-the-side-effects-of-shutdown'><span class='titlemark'>7.3.4   </span> <a id='x1-740007.3.4'></a>Mitigating the Side Effects of Shutdown</h5>
<!-- l. 2169 --><p class='noindent'>It is noteworthy that when AI systems are employed in critical economic activities or essential living infrastructure,
shutting down problematic AI systems may result in significant economic losses or social unrest. Therefore, we need
strategies to mitigate the side effects of shutting down AI, such as:
</p><!-- l. 2171 --><p class='noindent'>
     </p><ol class='enumerate1'>
<li class='enumerate' id='x1-74002x1'><span class='rm-lmbx-10'>Prepare a backup AI system</span>. This allows for a switch to a backup AI system after shutting down
     the problematic one. The backup AI system should better be developed by a different independent AI
     organization to avoid concurrent failures. The backup system does not necessarily have the same powerful
     functions as the original system. It can only have basic functions but ensure stronger security.
     </li>
<li class='enumerate' id='x1-74004x2'><span class='rm-lmbx-10'>Physical isolation deployment of AI systems</span>. Ensure that when we use hardware means to shut
     down one AI system, it does not affect another AI system.
     </li>
<li class='enumerate' id='x1-74006x3'><span class='rm-lmbx-10'>Build AI infrastructure in uninhabited areas as much as possible</span>. Construct AI data centers, AI
     robotics factories, or other critical AI infrastructure in uninhabited areas to ensure that military measures
     to destroy these facilities do not impact nearby residents.
</li></ol>
<!-- l. 2181 --><p class='noindent'>For safety measures in this section, the benefit, cost, resistance, and priorities are evaluated as shown in Table
6:
</p>
<div class='table'>
                                                                                            
                                                                                            
<!-- l. 2183 --><p class='noindent' id='priority-evaluation-of-measures-for-monitoring-ai-systems'><a id='x1-74007r6'></a></p><figure class='float'>
                                                                                            
                                                                                            
<figcaption class='caption'><span class='id'>TableÂ 6: </span><span class='content'>Priority Evaluation of Measures for Monitoring AI Systems</span></figcaption><!-- tex4ht:label?: x1-74007r6  -->
<div class='tabular'> <table class='tabular' id='TBL-7'><colgroup id='TBL-7-1g'><col id='TBL-7-1' /></colgroup><colgroup id='TBL-7-2g'><col id='TBL-7-2' /></colgroup><colgroup id='TBL-7-3g'><col id='TBL-7-3' /></colgroup><colgroup id='TBL-7-4g'><col id='TBL-7-4' /></colgroup><colgroup id='TBL-7-5g'><col id='TBL-7-5' /></colgroup><colgroup id='TBL-7-6g'><col id='TBL-7-6' /></colgroup><tr class='hline'><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr id='TBL-7-1-' style='vertical-align:baseline;'><td class='td11' id='TBL-7-1-1' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 2190 --><p class='noindent'><span class='rm-lmr-9'>Safety Measures</span>
                                             </p></td><td class='td11' id='TBL-7-1-2' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 2190 --><p class='noindent'><span class='rm-lmr-9'>Benefit           in
  Reducing
  Existential Risks</span>   </p></td><td class='td11' id='TBL-7-1-3' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 2190 --><p class='noindent'><span class='rm-lmr-9'>Benefit           in
  Reducing
  Non-Existential
  Risks</span>                  </p></td><td class='td11' id='TBL-7-1-4' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 2190 --><p class='noindent'><span class='rm-lmr-9'>Implementation
  Cost</span>               </p></td><td class='td11' id='TBL-7-1-5' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 2190 --><p class='noindent'><span class='rm-lmr-9'>Implementation
  Resistance</span>        </p></td><td class='td11' id='TBL-7-1-6' style='white-space:nowrap; text-align:center;'> <span class='rm-lmr-9'>Priority  </span></td>
</tr><tr class='hline'><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr id='TBL-7-2-' style='vertical-align:baseline;'><td class='td11' id='TBL-7-2-1' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 2192 --><p class='noindent'><span class='rm-lmr-9'>AI Monitoring System</span>               </p></td><td class='td11' id='TBL-7-2-2' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 2192 --><p class='noindent'><span class='rm-lmr-9'>+++++</span>             </p></td><td class='td11' id='TBL-7-2-3' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 2192 --><p class='noindent'><span class='rm-lmr-9'>+++++</span>             </p></td><td class='td11' id='TBL-7-2-4' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 2192 --><p class='noindent'><span class='rm-lmr-9'>++</span>                 </p></td><td class='td11' id='TBL-7-2-5' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 2192 --><p class='noindent'><span class='rm-lmr-9'>+</span>                   </p></td><td class='td11' id='TBL-7-2-6' style='white-space:nowrap; text-align:center;'>    <span class='rm-lmr-9'>1      </span></td>
</tr><tr class='hline'><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr id='TBL-7-3-' style='vertical-align:baseline;'><td class='td11' id='TBL-7-3-1' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 2194 --><p class='noindent'><span class='rm-lmr-9'>AI Detective System</span>                 </p></td><td class='td11' id='TBL-7-3-2' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 2194 --><p class='noindent'><span class='rm-lmr-9'>+++++</span>             </p></td><td class='td11' id='TBL-7-3-3' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 2194 --><p class='noindent'><span class='rm-lmr-9'>+++</span>                  </p></td><td class='td11' id='TBL-7-3-4' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 2194 --><p class='noindent'><span class='rm-lmr-9'>++++</span>            </p></td><td class='td11' id='TBL-7-3-5' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 2194 --><p class='noindent'><span class='rm-lmr-9'>++</span>                 </p></td><td class='td11' id='TBL-7-3-6' style='white-space:nowrap; text-align:center;'>    <span class='rm-lmr-9'>2      </span></td>
</tr><tr class='hline'><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr id='TBL-7-4-' style='vertical-align:baseline;'><td class='td11' id='TBL-7-4-1' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 2196 --><p class='noindent'><span class='rm-lmr-9'>AI Shutdown System</span>                 </p></td><td class='td11' id='TBL-7-4-2' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 2196 --><p class='noindent'><span class='rm-lmr-9'>++++</span>                </p></td><td class='td11' id='TBL-7-4-3' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 2196 --><p class='noindent'><span class='rm-lmr-9'>++</span>                    </p></td><td class='td11' id='TBL-7-4-4' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 2196 --><p class='noindent'><span class='rm-lmr-9'>+++</span>              </p></td><td class='td11' id='TBL-7-4-5' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 2196 --><p class='noindent'><span class='rm-lmr-9'>++</span>                 </p></td><td class='td11' id='TBL-7-4-6' style='white-space:nowrap; text-align:center;'>    <span class='rm-lmr-9'>2      </span></td>
</tr><tr class='hline'><td></td><td></td><td></td><td></td><td></td><td></td></tr></table>                                                                             </div>
                                                                                            
                                                                                            
</figure>
</div>
<h3 class='sectionHead' id='enhancing-information-security'><span class='titlemark'>8   </span> <a id='x1-750008'></a>Enhancing Information Security</h3>
<!-- l. 2204 --><p class='noindent'>
</p>
<h4 class='subsectionHead' id='general-idea-to-information-security'><span class='titlemark'>8.1   </span> <a id='x1-760008.1'></a>General Idea to Information Security</h4>
<!-- l. 2207 --><p class='noindent'>The continuous enhancement of AI capabilities and the promotion of information technology will increasingly
exacerbate the risks associated with information security. This primarily involves three factors:
</p><!-- l. 2209 --><p class='noindent'>
     </p><ol class='enumerate1'>
<li class='enumerate' id='x1-76002x1'><span class='rm-lmbx-10'>AI augments the capabilities of hackers</span><span class='footnote-mark'><a href='#fn32x0' id='fn32x0-bk'><sup class='textsuperscript'>32</sup></a></span><a id='x1-76003f32'></a><span class='rm-lmbx-10'>.</span>
     Research  indicates  that  large  language  models  (LLM)  can  execute  automated  hacking  attacks  by
     exploiting one-day vulnerabilities [<a id='x1-76005'></a><a href='#cite.0_fang2024llmagentsautonomouslyexploit'>11</a>]. The future ASI will possess the capability to develop hacking
     technologies far more advanced than those we encounter today.
     </li>
<li class='enumerate' id='x1-76007x2'><span class='rm-lmbx-10'>The promotion of information technology will transform information security risks into more
     power security risks. </span>For instance, the digitalization of financial transactions and shopping transforms
     information security risks into financial security risks. The digitalization of social interactions and media
     transforms information security risks into mental security risks. The large-scale deployment of autonomous
     machines such as self-driving cars, robot dogs, humanoid robots, and drones can transform information
     security risks into military security risks.
     </li>
<li class='enumerate' id='x1-76009x3'><span class='rm-lmbx-10'>Information security risks in AI systems can transform into AI safety risks. </span>For instance, hackers
     may infiltrate AI systems to modify their code, parameters, or memory, turning them into harmful AI, thus
     causing AI alignment to fail. Hackers might also hack into AI data management systems, tampering with
     training data and embedding backdoors. Hackers could compromise AI Monitoring Systems, rendering
     AI monitoring ineffective. Hackers might steal high-risk AI technology and then abuse. It is crucial to
     note that hackers may not always be external; the hacker might be the AI itself. The AI might exploit
     vulnerabilities to escape, self-iterate, or self-replicate to achieve harmful goals.
</li></ol>
<!-- l. 2219 --><p class='noindent'>In response to the aforementioned factors of information security risk, corresponding prevention measures can be
adopted, as illustrated in Figure 42:
</p>
<figure class='figure' id='x1-76010r42'><span id='general-idea-to-information-security1'></span> 

                                                                                            
                                                                                            

                                                                                            
                                                                                            
<!-- l. 2223 --><p class='noindent'><img alt='PIC' src='images/enhancing_information_security.png' style='width:100%' />
</p>
<figcaption class='caption'><span class='id'>FigureÂ 42: </span><span class='content'>General idea to information security</span></figcaption><!-- tex4ht:label?: x1-76010r42  -->
                                                                                            
                                                                                            
</figure>
     <ol class='enumerate1'>
<li class='enumerate' id='x1-76012x1'><span class='rm-lmbx-10'>Prevent  the  abuse  of  technologies  with  powerful  hacking  capabilities. </span>For  example,  avoid
     open-sourcing AI technologies with powerful hacking abilities, with specific measures detailed in Section
     13.3.  Even  when  providing  such  services  in  a  closed-source  format,  AI  alignment  should  be  ensured
     to prevent abuse after jailbreaking by users, as further elaborated in Section 6.2.1. In addition to AI
     technologies, the research and application of other technologies that can significantly enhance hacker
     capabilities, such as quantum computing, also require strict regulation.
     </li>
<li class='enumerate' id='x1-76014x2'>
     <!-- l. 2232 --><p class='noindent'><span class='rm-lmbx-10'>Enhance the security of high-power information systems. </span>We must focus on reinforcing the security
     of high-power information systems, such as:
</p>
         <ul class='itemize1'>
         <li class='itemize'>High-informational-power systems, such as cloud computing services, big data systems, PC/mobile
         operating systems, browsers, DNS services, certificate authority services.
         </li>
         <li class='itemize'>High-mental-power  systems,  such  as  social  platforms,  online  media  platforms,  governmental
         information systems and crowdsourcing platforms.
         </li>
         <li class='itemize'>High-financial-power systems, such as financial information systems and e-commerce platforms.
         </li>
         <li class='itemize'>High-military-power systems, such as military information systems, civilian robot control systems,
         and biological information systems.
</li></ul>
     </li>
<li class='enumerate' id='x1-76016x3'><span class='rm-lmbx-10'>Enhance information security for AI systems. </span>For AI systems, we need to not only reinforce information
     security against external hacker intrusions but also implement special information security measures tailored to AI
     system, preventing AI escape, self-iteration, and self-replication.
</li></ol>
<!-- l. 2250 --><p class='noindent'>As a mature field, information security encompasses numerous well-verified security measures, such as Security
Development Lifecycle (SDL) [<a id='x1-76017'></a><a href='#cite.0_SecurityDevelopmentLifecycle'>59</a>]. These measures are necessary and may be relatively effective in the short
term; however, in the long term, they are insufficient to counter the threats posed by highly intelligent
ASI. To withstand hacking attempts from ASI, we need to develop more advanced information security
technologies.
</p>
<h4 class='subsectionHead' id='ai-for-information-security'><span class='titlemark'>8.2   </span> <a id='x1-770008.2'></a>AI for Information Security</h4>
<!-- l. 2255 --><p class='noindent'>Since AI can be used for hacking, it can also be employed for security defenses.
                                                                                            
                                                                                            
</p><!-- l. 2257 --><p class='noindent'>
</p>
<h5 class='subsubsectionHead' id='online-defense-system-by-ai'><span class='titlemark'>8.2.1   </span> <a id='x1-780008.2.1'></a>Online Defense System by AI</h5>
<!-- l. 2259 --><p class='noindent'>AI can be utilized to implement an online defense system (as illustrated in Figure 43):
</p>
<figure class='figure' id='x1-78001r43'><span id='online-defense-system-by-ai1'></span> 

                                                                                            
                                                                                            

                                                                                            
                                                                                            
<!-- l. 2263 --><p class='noindent'><img alt='PIC' src='images/ai_online_defense_system.png' style='width:100%' />
</p>
<figcaption class='caption'><span class='id'>FigureÂ 43: </span><span class='content'>Online Defense System by AI</span></figcaption><!-- tex4ht:label?: x1-78001r43  -->
                                                                                            
                                                                                            
</figure>
     <ul class='itemize1'>
     <li class='itemize'><span class='rm-lmbx-10'>AI Firewall</span>: Employ AI to identify and intercept attack requests from external sources<span class='footnote-mark'><a href='#fn33x0' id='fn33x0-bk'><sup class='textsuperscript'>33</sup></a></span><a id='x1-78002f33'></a>,
     preventing hackers from infiltrating internal networks or endpoint devices.
     </li>
     <li class='itemize'><span class='rm-lmbx-10'>AI Service Gateway</span>: Utilize AI to identify and intercept unauthorized calls from within the intranet,
     prohibiting hackers from expanding their infiltration range through intranet.
     </li>
     <li class='itemize'><span class='rm-lmbx-10'>AI Data Gateway</span>: Employ AI to detect and intercept abnormal operations on databases, such as SQL
     injection requests and unauthorized data tampering.
     </li>
     <li class='itemize'><span class='rm-lmbx-10'>AI  Operating  System</span>:  Use  AI  to  recognize  and  intercept  abnormal  application  behaviors  in  the
     operating  system,  such  as  illegal  command  execution,  illegal  file  access,  and  illegal  network  access,
     preventing hackers from performing illegal actions on the machine.
     </li>
     <li class='itemize'><span class='rm-lmbx-10'>AI Cluster Operating System</span>: Utilize AI to detect and intercept abnormal machines or services within
     a cluster, preventing the spread of issues.
     </li>
     <li class='itemize'><span class='rm-lmbx-10'>AI Browser</span>: Use AI to identify and intercept malicious websites and content, such as phishing sites and
     phishing emails.
     </li>
     <li class='itemize'><span class='rm-lmbx-10'>Red Team AI</span>: Use Red Team AI to act as a hacker to attack the defensive AIs mentioned above, conduct
     adversarial training, continuously improving offensive and defensive capabilities.
</li></ul>
<!-- l. 2286 --><p class='noindent'>Applying AI in online defense necessitates two crucial requirements: low latency and high availability. Thus, the AI
system employed must be fully locally deployed, with no dependency on remote AI services. This AI system is tailored
specifically for security scenarios and does not require general-purpose capabilities, yet it must possess expert-level
proficiency in security.
</p><!-- l. 2288 --><p class='noindent'>Online defense can be implemented in two ways: synchronous and asynchronous. The synchronous approach involves
immediate judgment and interception of illegal requests, demanding extremely low latency, hence precluding
complex detection, and is suitable for defending against relatively simple attack requests. Conversely, the
asynchronous approach allows requests to pass initially, followed by asynchronous detection, enabling
more intricate detection processes. This method is suitable for defending against more complex attack
requests.
</p>
<h5 class='subsubsectionHead' id='offline-security-development-system-by-ai'><span class='titlemark'>8.2.2   </span> <a id='x1-790008.2.2'></a>Offline Security Development System by AI</h5>
<!-- l. 2292 --><p class='noindent'>All information systems may contain vulnerabilities. Even with robust online defenses, some vulnerabilities may still
exist, providing potential opportunities for ASI hackers. Therefore, an offline security development system is
essential to minimize these vulnerabilities. AI can be integrated throughout the software development
lifecycle<span class='footnote-mark'><a href='#fn34x0' id='fn34x0-bk'><sup class='textsuperscript'>34</sup></a></span><a id='x1-79001f34'></a> (as
illustrated in Figure 44):
                                                                                            
                                                                                            
</p>
<figure class='figure' id='x1-79003r44'><span id='offline-security-development-system-by-ai1'></span> 

                                                                                            
                                                                                            

                                                                                            
                                                                                            
<!-- l. 2296 --><p class='noindent'><img alt='PIC' src='images/ai_offline_security_development_system.png' style='width:80%' />
</p>
<figcaption class='caption'><span class='id'>FigureÂ 44: </span><span class='content'>Offline Security Development System by AI</span></figcaption><!-- tex4ht:label?: x1-79003r44  -->
                                                                                            
                                                                                            
</figure>
     <ul class='itemize1'>
     <li class='itemize'><span class='rm-lmbx-10'>AI for Design Review</span>: Integrate AI in the software design phase for security reviews to identify and
     rectify potential design-related security vulnerabilities.
     </li>
     <li class='itemize'><span class='rm-lmbx-10'>AI for Code Review</span>: Introduce AI during the development phase for code security reviews to detect and
     fix possible security vulnerabilities in the code. <span class='footnote-mark'><a href='#fn35x0' id='fn35x0-bk'><sup class='textsuperscript'>35</sup></a></span><a id='x1-79004f35'></a>
     </li>
     <li class='itemize'><span class='rm-lmbx-10'>AI for Testing</span>: Implement AI in the testing phase to conduct automated security testing, identifying
     and fixing potential security vulnerabilities.
     </li>
     <li class='itemize'><span class='rm-lmbx-10'>AI for Operation Review</span>: Utilize AI in the operation management phase to perform security reviews
     on change operations, preventing potential unsafe operations.
     </li>
     <li class='itemize'><span class='rm-lmbx-10'>Red Team AI</span>: Use Red Team AI to generate various vulnerable designs, codes, programs, and change
     operations, conducting adversarial training with the defensive AI mentioned above to continuously enhance
     offensive and defensive capabilities.
</li></ul>
<!-- l. 2315 --><p class='noindent'>The aforementioned process is for incremental development; additionally, we can employ AIs to perform
security reviews on existing designs and codes, conduct security testing on existing programs, and carry out
security reviews on current deployments to ensure that there are no security issues within the existing
system.
</p><!-- l. 2317 --><p class='noindent'>In offline development scenarios, the demands for low latency and high availability are not as stringent. Thus, more
powerful AI can be employed at this stage, allowing it ample time to perform comprehensive security
checks.
</p>
<h4 class='subsectionHead' id='provable-security'><span class='titlemark'>8.3   </span> <a id='x1-800008.3'></a>Provable Security</h4>
<!-- l. 2322 --><p class='noindent'>While employing AI to defend against AI is an advisable strategy, but the disparity in capabilities among AIs exists. If
the attacking AI possesses greater intellectual power than the defending AI, the attacker may still devise more
sophisticated methods to breach the defenderâs barriers. Is there a method that can withstand any level of AI
intellectual power? The answer is mathematically provable security. Below are two potential directions (as illustrated in
Figure 45):
</p>
<figure class='figure' id='x1-80001r45'><span id='provable-security1'></span> 

                                                                                            
                                                                                            

                                                                                            
                                                                                            
<!-- l. 2326 --><p class='noindent'><img alt='PIC' src='images/provable_security.png' style='width:50%' />
</p>
<figcaption class='caption'><span class='id'>FigureÂ 45: </span><span class='content'>Provable security</span></figcaption><!-- tex4ht:label?: x1-80001r45  -->
                                                                                            
                                                                                            
</figure>
     <ol class='enumerate1'>
<li class='enumerate' id='x1-80003x1'><span class='rm-lmbx-10'>Formal  Verification</span>:  Formal  verification  [<a id='x1-80004'></a><a href='#cite.0_FormalVerification'>60</a>]  employs  mathematical  methods  to  prove  that  an
     information system consistently operates according to its design specifications. Real-world information
     systems  are  often  exceedingly  complex,  making  formal  verification  challenging;  however,  AI  can  be
     employed to accomplish this task. A Developer AI can be tasked with developing a software system based
     on specified software specifications<span class='footnote-mark'><a href='#fn36x0' id='fn36x0-bk'><sup class='textsuperscript'>36</sup></a></span><a id='x1-80005f36'></a>,
     while  a  Verifier  AI  conducts  formal  verification  to  ensure  compliance  with  the  specifications.  If  the
     verification fails, the Developer AI must continue to refine the system until it passes formal verification.
     At this point, we can be confident that the software is defect-free, and even the most intelligent ASI
     hacker cannot exploit any vulnerabilities. It is important to ensure the effectiveness of the verification by
     having the Developer AI and Verifier AI originate from two independent AI organizations; additionally,
     multiple AIs from different AI organizations can be involved in the verification process to enhance its
     reliability.
     </li>
<li class='enumerate' id='x1-80008x2'><span class='rm-lmbx-10'>Algorithm  Design  and  Cracking</span>:  Cryptographic  algorithms  are  the  cornerstone  of  information
     security. To prevent algorithms from being compromised, AI can be utilized to enhance them. A Researcher
     AI is responsible for designing cryptographic algorithms, while a Cracker AI attempts to crack them. If
     an algorithm is broken, the Researcher AI must continue to improve it until it is theoretically proven to
     be uncrackable. At this point, we can be confident that no matter how intelligent an ASI hacker is, they
     cannot crack the algorithm. We can also allow the Cracker AI to use quantum computers, which implies
     that the Researcher AI must develop algorithms that are uncrackable by quantum computers [<a id='x1-80009'></a><a href='#cite.0_PostQuantumCryptography'>61</a>].
</li></ol>
<!-- l. 2339 --><p class='noindent'>It is imperative to note that the aforementioned AIs inherently possess significant hacking capabilities, necessitating
their operation in a strictly isolated environment to prevent any influence on the external world or theft by hackers.
Personnel involved in related projects must undergo rigorous scrutiny to prevent the misuse of these powerful technologies.
<span class='footnote-mark'><a href='#fn37x0' id='fn37x0-bk'><sup class='textsuperscript'>37</sup></a></span><a id='x1-80010f37'></a>
</p><!-- l. 2341 --><p class='noindent'>In addition, Quantum Key Distribution (QKD) [<a id='x1-80012'></a><a href='#cite.0_QuantumKeyDistribution'>62</a>] is a provably secure communication method, which ensures that
keys cannot be eavesdropped on or tampered with based on the principles of quantum mechanics. It is a promising
direction for countering ASI hackers.
</p>
<h4 class='subsectionHead' id='internal-security-of-the-system'><span class='titlemark'>8.4   </span> <a id='x1-810008.4'></a>Internal Security of the System</h4>
<!-- l. 2346 --><p class='noindent'>Previously, we discussed how to defend against external attacks, but sometimes issues arise within the system. In
general, we need to guard against the following attacking methods (as depicted in 46):
</p>
<figure class='figure' id='x1-81001r46'><span id='internal-security-of-the-system1'></span> 

                                                                                            
                                                                                            

                                                                                            
                                                                                            
<!-- l. 2350 --><p class='noindent'><img alt='PIC' src='images/internal_security.png' style='width:80%' />
</p>
<figcaption class='caption'><span class='id'>FigureÂ 46: </span><span class='content'>Internal Security of the System</span></figcaption><!-- tex4ht:label?: x1-81001r46  -->
                                                                                            
                                                                                            
</figure>
     <ol class='enumerate1'>
<li class='enumerate' id='x1-81003x1'>
     <!-- l. 2357 --><p class='noindent'><span class='rm-lmbx-10'>Impersonating Human Employees</span>: Hackers can impersonate human employees within an organization
     by stealing their accounts. To enhance the security of human accounts, we can introduce multiple relatively
     independent factors for authentication. Table 7 compares various authentication factors. In addition, AI
     can also be equipped with the ability of identity recognition, using long-term memory to remember the
     characteristics of the humans it has contacted with, such as faces, voiceprints, behavioral habits, and so
     on. In this way, even if a strange hacker impersonates an AI user or other acquaintance by stealing an
     account, AI has the ability to identify it.
</p>
     <div class='table'>
     <!-- l. 2359 --><p class='noindent' id='comparison-of-identity-authentication-factors'><a id='x1-81004r7'></a></p><figure class='float'>
<figcaption class='caption'><span class='id'>TableÂ 7: </span><span class='content'>Comparison of Identity Authentication Factors</span></figcaption><!-- tex4ht:label?: x1-81004r7  -->
     <div class='tabular'> <table class='tabular' id='TBL-8'><colgroup id='TBL-8-1g'><col id='TBL-8-1' /></colgroup><colgroup id='TBL-8-2g'><col id='TBL-8-2' /></colgroup><colgroup id='TBL-8-3g'><col id='TBL-8-3' /></colgroup><tr class='hline'><td></td><td></td><td></td></tr><tr id='TBL-8-1-' style='vertical-align:baseline;'><td class='td11' id='TBL-8-1-1' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 2366 --><p class='noindent'><span class='rm-lmr-9'>Authentication Factor</span>                   </p></td><td class='td11' id='TBL-8-1-2' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 2366 --><p class='noindent'><span class='rm-lmr-9'>Advantages</span>                                 </p></td><td class='td11' id='TBL-8-1-3' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 2366 --><p class='noindent'><span class='rm-lmr-9'>Disadvantages</span>                             </p></td>
</tr><tr class='hline'><td></td><td></td><td></td></tr><tr id='TBL-8-2-' style='vertical-align:baseline;'><td class='td11' id='TBL-8-2-1' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 2368 --><p class='noindent'><span class='rm-lmr-9'>Password</span>
                                                   </p></td><td class='td11' id='TBL-8-2-2' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 2368 --><p class='noindent'><span class='rm-lmr-9'>Stored  in  the  brain,  difficult  for
  external parties to directly access</span>    </p></td><td class='td11' id='TBL-8-2-3' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 2368 --><p class='noindent'><span class='rm-lmr-9'>Easily  forgotten,  or  guessed  by
  hacker</span>                                        </p></td>
</tr><tr class='hline'><td></td><td></td><td></td></tr><tr id='TBL-8-3-' style='vertical-align:baseline;'><td class='td11' id='TBL-8-3-1' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 2370 --><p class='noindent'><span class='rm-lmr-9'>Public     Biometrics     (Face     /
  Voiceprints)</span>
                                                   </p></td><td class='td11' id='TBL-8-3-2' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 2370 --><p class='noindent'><span class='rm-lmr-9'>Convenient to use</span>                         </p></td><td class='td11' id='TBL-8-3-3' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 2370 --><p class='noindent'><span class='rm-lmr-9'>Easily           obtained           and
  forged by hacker (See Section 9.1.2
  for solution)</span>                                </p></td>
</tr><tr class='hline'><td></td><td></td><td></td></tr><tr id='TBL-8-4-' style='vertical-align:baseline;'><td class='td11' id='TBL-8-4-1' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 2372 --><p class='noindent'><span class='rm-lmr-9'>Private  Biometrics  (Fingerprint  /
  Iris)</span>                                            </p></td><td class='td11' id='TBL-8-4-2' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 2372 --><p class='noindent'><span class='rm-lmr-9'>Convenient and hard to acquire by
  hacker</span>                                        </p></td><td class='td11' id='TBL-8-4-3' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 2372 --><p class='noindent'><span class='rm-lmr-9'>Requires special equipment</span>            </p></td>
</tr><tr class='hline'><td></td><td></td><td></td></tr><tr id='TBL-8-5-' style='vertical-align:baseline;'><td class='td11' id='TBL-8-5-1' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 2374 --><p class='noindent'><span class='rm-lmr-9'>Personal Devices (Mobile Phones /
  USB Tokens)</span>                                </p></td><td class='td11' id='TBL-8-5-2' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 2374 --><p class='noindent'><span class='rm-lmr-9'>Difficult to forge</span>                          </p></td><td class='td11' id='TBL-8-5-3' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 2374 --><p class='noindent'><span class='rm-lmr-9'>Risk of loss, theft, or robbery</span>         </p></td>
</tr><tr class='hline'><td></td><td></td><td></td></tr><tr id='TBL-8-6-' style='vertical-align:baseline;'><td class='td11' id='TBL-8-6-1' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 2376 --><p class='noindent'><span class='rm-lmr-9'>Decentralized Identifiers (DID)[</span><a id='x1-81005'></a><a href='#cite.0_DID'><span class='rm-lmr-9'>63</span></a><span class='rm-lmr-9'>]</span>    </p></td><td class='td11' id='TBL-8-6-2' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 2376 --><p class='noindent'><span class='rm-lmr-9'>Decentralized authentication</span>          </p></td><td class='td11' id='TBL-8-6-3' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 2376 --><p class='noindent'><span class='rm-lmr-9'>Risk of private key loss or theft</span>       </p></td>
</tr><tr class='hline'><td></td><td></td><td></td></tr></table>                                                                     </div>
     </figure>
     </div>
     </li>
<li class='enumerate' id='x1-81007x2'><span class='rm-lmbx-10'>Exploiting Human Employees</span>: Hackers can exploit human employees within an organization through
     deception, enticement, or coercion. Specific solutions can be found in Section 9.
     </li>
<li class='enumerate' id='x1-81009x3'><span class='rm-lmbx-10'>Impersonating AI Assistants</span>: Hackers can impersonate AI assistants within an organization by stealing their
     accounts. To enhance the security of AI accounts, secret key + IPv6 dual-factor authentication can be
     implemented. Each AI instance owns a secret key. Assigning a unique IPv6 address to each AI instance, due to the
     difficulty of forging IPv6 addresses, can prevent unauthorized account use even if the secret key
     leaks.
     </li>
<li class='enumerate' id='x1-81011x4'><span class='rm-lmbx-10'>Exploiting AI Assistants</span>: Hackers can exploit AI assistants within an organization through jailbreaking or
     injection. To prevent AI assistants from being exploited, it is crucial to ensure AI alignment, detailed in Section 6.
     Additionally, restrict AI assistantsâ external communications to minimize contact with external
     entities.
     </li>
<li class='enumerate' id='x1-81013x5'><span class='rm-lmbx-10'>Software Supply Chain Attack</span>: Hackers can implant backdoors into external softwares that the information
     system rely on, and infiltrate the system through the software supply chain. Specific solutions can be found in
                                                                                            
                                                                                            
     Section 8.5.
     </li>
<li class='enumerate' id='x1-81015x6'><span class='rm-lmbx-10'>Hardware Supply Chain Attack</span>: Hackers can implant backdoors into external hardwares that the information
     system rely on, and infiltrate the system through the hardware supply chain. Solutions can also draw from
     strategies used in software supply chain security, such as adopting a multi-supplier strategy and third-party
     hardware security certification.
     </li>
<li class='enumerate' id='x1-81017x7'><span class='rm-lmbx-10'>Physical Attack</span>: Hackers can conduct side-channel attack from physical level, or employ military power to seize
     or steal hardware devices, or use coercion on human employees to gain control over critical information systems.
     To prevent such scenarios, it is essential to enhance physical security measures in office region and server rooms,
     and to install electromagnetic shielding and anti-theft functions (such as automatic data erasure when forcibly
     dismantled) for critical hardware devices.
</li></ol>
<h4 class='subsectionHead' id='software-supply-chain-security'><span class='titlemark'>8.5   </span> <a id='x1-820008.5'></a>Software Supply Chain Security</h4>
<!-- l. 2398 --><p class='noindent'>Software supply chain security is a critical aspect of information security and warrants a dedicated discussion in a
separate section due to its significant importance.
</p><!-- l. 2400 --><p class='noindent'>In the current and foreseeable future, both enterprise production and personal life heavily depend on software developed
by various parties, including a wide range of cloud services and locally running closed-source or open-source software.
These dependent software systems may harbor bugs, vulnerabilities, or backdoors, posing security risks to users. For
instance, in March 2024, the open-source software xz was implanted with a backdoor. The compression tool xz Utils in
multiple Linux versions contained malicious code, allowing attackers to access systems via SSHD without valid
credentials [<a id='x1-82001'></a><a href='#cite.0_XZUtilsBackdoor'>64</a>].
</p><!-- l. 2402 --><p class='noindent'>Security issues arising from dependency on software can manifest in several ways:
</p>
     <ul class='itemize1'>
     <li class='itemize'>Software vulnerabilities provide hackers with opportunities for intrusion.
     </li>
     <li class='itemize'>Vulnerabilities in software distribution channels allow software tampering during the distribution process.
     </li>
     <li class='itemize'>Bugs in software lead to the failure of security functions, allowing hacker intrusion.
     </li>
     <li class='itemize'>The dependent software itself may be an expertly disguised malware, posing a risk of malicious attacks
     once installed.
</li></ul>
<!-- l. 2416 --><p class='noindent'>For highly intelligent ASI, implanting backdoors in dependent software serves as an excellent attack strategy to
infiltrate target systems. Especially for foundational software relied upon by numerous systems, if such software
introduces vulnerabilities, and these are not promptly identified, they could be deployed across numerous production
                                                                                            
                                                                                            
systems in a short period, opening doors for ASI to invade these systems.
</p><!-- l. 2418 --><p class='noindent'>To address software supply chain security issues, the following are some recommended strategies.
</p><!-- l. 2420 --><p class='noindent'>
</p>
<h5 class='subsubsectionHead' id='multisupplier-strategy'><span class='titlemark'>8.5.1   </span> <a id='x1-830008.5.1'></a>Multi-Supplier Strategy</h5>
<!-- l. 2423 --><p class='noindent'>Software users may employ a multi-supplier strategy to reduce supply chain security risks. Below are several possible
methods:
</p>
<figure class='figure' id='x1-83001r47'><span id='multisupplier-strategy1'></span> 

                                                                                            
                                                                                            

                                                                                            
                                                                                            
<!-- l. 2427 --><p class='noindent'><img alt='PIC' src='images/multi_supplier_strategy.png' style='width:80%' />
</p>
<figcaption class='caption'><span class='id'>FigureÂ 47: </span><span class='content'>Multi-supplier strategy</span></figcaption><!-- tex4ht:label?: x1-83001r47  -->
                                                                                            
                                                                                            
</figure>
     <ol class='enumerate1'>
<li class='enumerate' id='x1-83003x1'><span class='rm-lmbx-10'>Output Consistency Check</span>: Send the same input to software from multiple suppliers to receive multiple
     outputs. Then, verify whether these outputs are consistent. If a security issue exists in one supplierâs
     software, its output will differ from that of another supplier, enabling detection. However, this approach
     requires  that  identical  inputs  yield  identical  outputs.  If  the  software  processing  involves  randomness
     or time-dependent logic, inconsistencies may arise, necessitating more sophisticated consistency check
     programs, such as utilizing AI models to identify and verify the consistency of critical information while
     intelligently ignoring inconsistencies in non-critical information. As illustrated in Figure 47(a).
     </li>
<li class='enumerate' id='x1-83005x2'><span class='rm-lmbx-10'>State Consistency Check</span>: Certain software systems are stateful, and even with identical inputs, it
     is not guaranteed that the states maintained by software from multiple suppliers will be consistent. To
     ensure state consistency, we can implement data synchronization and verification among the softwares from
     different suppliers. Initially, one supplier is designated as the master supplier, while others are designated
     as slave suppliers. Users send their inputs to both the master and slave suppliers. The master supplier
     then synchronizes state changes with the slave suppliers. The slave suppliers must verify whether the
     synchronized state is consistent with the userâs input. If inconsistencies are detected, the master supplier
     and the user are notified of the verification failure, and the corresponding state changes are rolled back.
     As shown in Figure 47(b).
     </li>
<li class='enumerate' id='x1-83007x3'><span class='rm-lmbx-10'>Generation and Review</span>: Input is processed by one supplierâs software to generate an output, which is
     then reviewed for security by another supplierâs software. This method is applicable to a broader range of
     scenarios but requires robust security review capabilities to avoid false negatives or positives. As depicted
     in Figure 47(c).
     </li>
<li class='enumerate' id='x1-83009x4'><span class='rm-lmbx-10'>Separation of Keys and Data Storage</span>: To prevent the theft of data stored in supplier software, data
     can be encrypted, and then store the keys and the encrypted data separately across software from two
     different suppliers. As illustrated in Figure 47(d).
</li></ol>
<!-- l. 2444 --><p class='noindent'>After implementing a multi-supplier strategy, hackers must implant backdoors in software from multiple suppliers to
successfully carry out a supply chain attack, thereby significantly increasing the difficulty.
</p><!-- l. 2446 --><p class='noindent'>The aforementioned strategies are suitable for situations where dependencies are invoked via code. However, for
end-users who interact with software through user interfaces (e.g., apps, websites) and typically lack development skills,
how can they apply a multi-supplier strategy? Taking search engines as an example: if users need to use two search
engines for every query and compare results, this greatly increases their effort. In such cases, a local AI
assistant can assist users in verification. When a user searches on a search engine, the local AI assistant
can simultaneously search on another search engine and compare the results. If any discrepancies are
found, the user is alerted to prevent receiving false information. Naturally, the AI assistant itself will use a
multi-supplier strategy, with two AI assistants developed by different suppliers locally supervising each
other.
</p><!-- l. 2448 --><p class='noindent'>Blockchain also needs to consider a multi-supplier strategy. Although theoretically decentralized, if over half the nodes
use the same blockchain client software, it is possible to tamper the blockchain by implanting a backdoor in the client
software. Thus, it is essential to ensure that blockchain software is provided by multiple suppliers, with no single
supplierâs software covering more than half the nodes.
                                                                                            
                                                                                            
</p>
<h5 class='subsubsectionHead' id='software-signature-and-security-certification'><span class='titlemark'>8.5.2   </span> <a id='x1-840008.5.2'></a>Software Signature and Security Certification</h5>
<!-- l. 2453 --><p class='noindent'>To prevent software from being tampered with during development or distribution, or to guard against developers
maliciously implanting backdoors, a software signature and security certification mechanism can be established as
shown in Figure 48:
</p>
<figure class='figure' id='x1-84001r48'><span id='software-signature-and-security-certification1'></span> 

                                                                                            
                                                                                            

                                                                                            
                                                                                            
<!-- l. 2457 --><p class='noindent'><img alt='PIC' src='images/software_signature_and_security_certification.png' style='width:80%' />
</p>
<figcaption class='caption'><span class='id'>FigureÂ 48: </span><span class='content'>Software signature and security certification</span></figcaption><!-- tex4ht:label?: x1-84001r48  -->
                                                                                            
                                                                                            
</figure>
<!-- l. 2462 --><p class='noindent'><span class='rm-lmbx-10'>Software Signature</span>: Ensure that each softwareâs building steps and all the software it depends on are traceable, and
that the softwareâs content is not altered during distribution.
     </p><ol class='enumerate1'>
<li class='enumerate' id='x1-84003x1'>Each software has a unique signature based on its content.
     </li>
<li class='enumerate' id='x1-84005x2'>The  building  process  involves  using  the  softwareâs  own  source  code  alongside  other  dependent
     softwares (including operating systems, compilers, dependency libraries, etc.) within a specified hardware
     environment.
     </li>
<li class='enumerate' id='x1-84007x3'>Multiple   third-party,   independent   signatories   verify   the   softwareâs   building   process   and   store
     the   softwareâs   signature,   building   steps,   and   dependency   list   in   a   blockchain-based   Software
     Information  Repository.  Consensus  through  the  blockchain  ensures  that  the  final  signature  is
     consistent.<span class='footnote-mark'><a href='#fn38x0' id='fn38x0-bk'><sup class='textsuperscript'>38</sup></a></span><a id='x1-84008f38'></a>
     </li>
<li class='enumerate' id='x1-84011x4'>Users retrieve software information from the Software Information Repository. Upon downloading, users
     can verify the software content through signatures to ensure it was not altered during distribution.
</li></ol>
<!-- l. 2476 --><p class='noindent'><span class='rm-lmbx-10'>Software Security Certification</span>: Conduct security checks on the software and document its security
information.
     </p><ol class='enumerate1'>
<li class='enumerate' id='x1-84013x1'>Multiple third-party, independent certifiers authenticate the softwareâs security.
     </li>
<li class='enumerate' id='x1-84015x2'>Certifiers obtain the softwareâs source code and artifacts following the information from the Software
     Information Repository, perform security checks on dependencies, code security reviews, security tests,
     and produce security reports. These reports include inspection items, results, and identified vulnerabilities,
     and are recorded in the Software Information Repository.
     </li>
<li class='enumerate' id='x1-84017x3'>Users query the software information from the Software Information Repository to understand its security
     status based on the security reports and choose more secure software for use. After downloading the
     software, signature verification can be conducted.
     </li>
<li class='enumerate' id='x1-84019x4'>If a new vulnerability is discovered in existing software, certifiers should update the security report. They
     should find out downstream softwares via the Software Information Repository, and assess the impact of
                                                                                            
                                                                                            
     vulnerabilities on these softwares, updating the security reports of the downstream softwares if necessary,
     and this operation should be recursive.
</li></ol>
<!-- l. 2490 --><p class='noindent'>With the implementation of software signature and security certification mechanism, users can be assured that
downloaded software and its dependencies have undergone rigorous third-party security checks and remain untampered.
Although checks may not identify all issues, they can intercept the majority of them. Backed by multiple independent
signatories and certifiers, users are free from creating and verifying software from scratch and can directly access trusted
software.
</p><!-- l. 2492 --><p class='noindent'>This mechanism is applicable to both open and closed source software. For closed-source software, code must be
accessible to signatories and certifiers but not necessarily to users.
</p><!-- l. 2494 --><p class='noindent'>This mechanism can also be applied to AI models. Signatories need to verify the entire training process of the model to
confirm that the final model originates from specific training datasets and codes. Certifiers must review the security of
AI training datasets and codes, and conduct security tests on the models.
</p>
<h4 class='subsectionHead' id='information-security-for-ai-systems'><span class='titlemark'>8.6   </span> <a id='x1-850008.6'></a>Information Security for AI Systems</h4>
<!-- l. 2499 --><p class='noindent'>In the preceding sections, we introduced general measures to enhance the security of information systems, which are
also applicable to AI systems. However, beyond meeting the security standards of regular information systems, it is
imperative to address the unique security risks associated with AI systems, such as AI escape, self-iteration, and
self-replication. The following are specific solutions to these unique risks.
</p>
<figure class='figure' id='x1-85001r49'><span id='information-security-for-ai-systems1'></span> 

                                                                                            
                                                                                            

                                                                                            
                                                                                            
<!-- l. 2503 --><p class='noindent'><img alt='PIC' src='images/information_security_for_ai_systems.png' style='width:100%' />
</p>
<figcaption class='caption'><span class='id'>FigureÂ 49: </span><span class='content'>Information security for AI systems</span></figcaption><!-- tex4ht:label?: x1-85001r49  -->
                                                                                            
                                                                                            
</figure>
<!-- l. 2508 --><p class='noindent'>As illustrated in Figure 49, each black box in the diagram represents a secure isolation environment. The specific
implementation can be a secure container, virtual machine, physical machine, or an internal network
composed of multiple machines, isolated from external networks through VPC or physical isolation, with only
a few ports open for external communication. Each secure isolation environment is allocated limited
computing resources, including CPU, GPU, memory, and storage space, to prevent AI from excessive resource
consumption.
     </p><ol class='enumerate1'>
<li class='enumerate' id='x1-85003x1'>
     <!-- l. 2512 --><p class='noindent'><span class='rm-lmbx-10'>Production Environment</span>: </p>
         <ul class='itemize1'>
         <li class='itemize'><span class='rm-lmbx-10'>Module Isolation</span>: In the production environment, according to the interpretable AGI architecture
         designed in Section 6.1, the AI system is divided into six isolated modules: controller (including
         short-term memory), models (including the core model and expert models), thinking tools, external
         tools, private long-term memory, and shared long-term memory. Different models and thinking tools
         are also isolated from each other. This isolation prevents AI from self-iteration or self-replication
         through tool invocation.
         </li>
         <li class='itemize'><span class='rm-lmbx-10'>Instance Isolation</span>: Each AI instance has its own runtime environment for short-term memory,
         private long-term memory, and external tools, ensuring that AI instances do not directly communicate
         with each other, thereby preventing mutual influence.
         </li>
         <li class='itemize'><span class='rm-lmbx-10'>Read-Only and Encryption</span>: Implement technical measures to ensure that the controller, models,
         thinking  tools,  and  shared  long-term  memory  are  read-only,  preventing  external  tampering.  To
         prevent AI from being stolen by hackers, the binary files of these modules can be encrypted, making
         them unusable in other environments even if stolen. Additionally, logic can be embedded in the code
         to check the environment (such as operating system, hardware configuration, network environment,
         geographic location, time, environment variables, etc.), halting execution if the current environment
         does not meet expectations. Hardware encryption can also be introduced to ensure models only run
         on specific hardware [<a id='x1-85004'></a><a href='#cite.0_SecureInfrastructureAI'>65</a>].
         </li>
         <li class='itemize'><span class='rm-lmbx-10'>System Isolation</span>: The AI System is isolated from the AI Monitoring System (detailed in Section
         7.1) and the Identity and Watermarking System (detailed in Sections 7.2.2 and 9.1.1), preventing AI
         from tampering with these systems.
         </li>
         <li class='itemize'><span class='rm-lmbx-10'>External Isolation</span>: The entire production environment is isolated from the external environment,
         with  interaction  only  possible  through  a  secure  gateway.  The  secure  gateway  can  implement
         fine-grained access control policies based on AI usage scenarios, such as restricting AI to access
         only specific websites, thereby preventing AI escape. Operators can access the internal management
         network through an independent secure gateway, managing the production environment.
</li></ul>
     <!-- l. 2526 --><p class='noindent'>For scenarios where the production environment is not on the cloud, such as a userâs local AI assistants or AI
     robots, all modules within the aforementioned production environment, including secure isolation facilities, can be
     packaged and deployed as a whole.
     </p></li>
                                                                                            
                                                                                            
<li class='enumerate' id='x1-85006x2'>
     <!-- l. 2528 --><p class='noindent'><span class='rm-lmbx-10'>Development Environment</span>: </p>
         <ul class='itemize1'>
         <li class='itemize'><span class='rm-lmbx-10'>Environment Isolation</span>: In the development environment, the model training environment, code
         development  environment,  System  2  training  environment  (detailed  in  Section  6.2),  and  testing
         environment are isolated from each other, preventing AI from accessing its code or model parameters.
         In the System 2 training and testing environments, the Student AI, Red Team AIs, and other AIs
         are further placed in an isolated virtual world environment to prevent escape.
         </li>
         <li class='itemize'><span class='rm-lmbx-10'>Information  Confidentiality</span>:  Technical  information  related  to  the  AI  system,  including
         technical architecture, code, parameters, deployment environment information, training environment
         information, and monitoring system information, must be strictly confidential. This information
         should not appear in AI training data, nor should AI have the authority to access it during operation,
         preventing AI from using this information for self-replication, self-iteration, or escape.
         </li>
         <li class='itemize'><span class='rm-lmbx-10'>External  Isolation</span>:  The  development  environment  is  entirely  isolated  from  the  outside,  with
         interaction  only  possible  through  a  secure  gateway.  Developers  and  evaluators  can  access  the
         development  environment  through  a  secure  gateway  to  perform  operations  and  transfer  the  AI
         systemâs deployment package to the production environment through another secure gateway. Any
         other  external  access  will  be  intercepted  by  the  secure  gateway,  preventing  AI  escape  from  the
         development environment.
</li></ul>
     </li></ol>
<!-- l. 2541 --><p class='noindent'>For safety measures in this section, the benefit, cost, resistance, and priorities are evaluated as shown in Table
8:
</p>
<div class='table'>
                                                                                            
                                                                                            
<!-- l. 2543 --><p class='noindent' id='priority-evaluation-of-measures-for-enhancing-information-security'><a id='x1-85007r8'></a></p><figure class='float'>
                                                                                            
                                                                                            
<figcaption class='caption'><span class='id'>TableÂ 8: </span><span class='content'>Priority Evaluation of Measures for Enhancing Information Security</span></figcaption><!-- tex4ht:label?: x1-85007r8  -->
<div class='tabular'> <table class='tabular' id='TBL-9'><colgroup id='TBL-9-1g'><col id='TBL-9-1' /></colgroup><colgroup id='TBL-9-2g'><col id='TBL-9-2' /></colgroup><colgroup id='TBL-9-3g'><col id='TBL-9-3' /></colgroup><colgroup id='TBL-9-4g'><col id='TBL-9-4' /></colgroup><colgroup id='TBL-9-5g'><col id='TBL-9-5' /></colgroup><colgroup id='TBL-9-6g'><col id='TBL-9-6' /></colgroup><tr class='hline'><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr id='TBL-9-1-' style='vertical-align:baseline;'><td class='td11' id='TBL-9-1-1' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 2550 --><p class='noindent'><span class='rm-lmr-9'>Safety Measures</span>
                                             </p></td><td class='td11' id='TBL-9-1-2' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 2550 --><p class='noindent'><span class='rm-lmr-9'>Benefit           in
  Reducing
  Existential Risks</span>   </p></td><td class='td11' id='TBL-9-1-3' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 2550 --><p class='noindent'><span class='rm-lmr-9'>Benefit           in
  Reducing
  Non-Existential
  Risks</span>                  </p></td><td class='td11' id='TBL-9-1-4' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 2550 --><p class='noindent'><span class='rm-lmr-9'>Implementation
  Cost</span>               </p></td><td class='td11' id='TBL-9-1-5' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 2550 --><p class='noindent'><span class='rm-lmr-9'>Implementation
  Resistance</span>        </p></td><td class='td11' id='TBL-9-1-6' style='white-space:nowrap; text-align:center;'> <span class='rm-lmr-9'>Priority  </span></td>
</tr><tr class='hline'><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr id='TBL-9-2-' style='vertical-align:baseline;'><td class='td11' id='TBL-9-2-1' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 2552 --><p class='noindent'><span class='rm-lmr-9'>Using AI to Defend Against AI</span>   </p></td><td class='td11' id='TBL-9-2-2' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 2552 --><p class='noindent'><span class='rm-lmr-9'>+++++</span>             </p></td><td class='td11' id='TBL-9-2-3' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 2552 --><p class='noindent'><span class='rm-lmr-9'>+++++</span>             </p></td><td class='td11' id='TBL-9-2-4' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 2552 --><p class='noindent'><span class='rm-lmr-9'>+++</span>              </p></td><td class='td11' id='TBL-9-2-5' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 2552 --><p class='noindent'><span class='rm-lmr-9'>+</span>                   </p></td><td class='td11' id='TBL-9-2-6' style='white-space:nowrap; text-align:center;'>    <span class='rm-lmr-9'>1      </span></td>
</tr><tr class='hline'><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr id='TBL-9-3-' style='vertical-align:baseline;'><td class='td11' id='TBL-9-3-1' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 2554 --><p class='noindent'><span class='rm-lmr-9'>Provable Security</span>                     </p></td><td class='td11' id='TBL-9-3-2' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 2554 --><p class='noindent'><span class='rm-lmr-9'>+++++</span>             </p></td><td class='td11' id='TBL-9-3-3' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 2554 --><p class='noindent'><span class='rm-lmr-9'>++++</span>                </p></td><td class='td11' id='TBL-9-3-4' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 2554 --><p class='noindent'><span class='rm-lmr-9'>++++</span>            </p></td><td class='td11' id='TBL-9-3-5' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 2554 --><p class='noindent'><span class='rm-lmr-9'>+</span>                   </p></td><td class='td11' id='TBL-9-3-6' style='white-space:nowrap; text-align:center;'>    <span class='rm-lmr-9'>2      </span></td>
</tr><tr class='hline'><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr id='TBL-9-4-' style='vertical-align:baseline;'><td class='td11' id='TBL-9-4-1' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 2556 --><p class='noindent'><span class='rm-lmr-9'>Internal Security of the System</span>   </p></td><td class='td11' id='TBL-9-4-2' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 2556 --><p class='noindent'><span class='rm-lmr-9'>++++</span>                </p></td><td class='td11' id='TBL-9-4-3' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 2556 --><p class='noindent'><span class='rm-lmr-9'>++++</span>                </p></td><td class='td11' id='TBL-9-4-4' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 2556 --><p class='noindent'><span class='rm-lmr-9'>+++</span>              </p></td><td class='td11' id='TBL-9-4-5' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 2556 --><p class='noindent'><span class='rm-lmr-9'>+</span>                   </p></td><td class='td11' id='TBL-9-4-6' style='white-space:nowrap; text-align:center;'>    <span class='rm-lmr-9'>2      </span></td>
</tr><tr class='hline'><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr id='TBL-9-5-' style='vertical-align:baseline;'><td class='td11' id='TBL-9-5-1' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 2558 --><p class='noindent'><span class='rm-lmr-9'>Multi-Supplier Strategy</span>             </p></td><td class='td11' id='TBL-9-5-2' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 2558 --><p class='noindent'><span class='rm-lmr-9'>+++++</span>             </p></td><td class='td11' id='TBL-9-5-3' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 2558 --><p class='noindent'><span class='rm-lmr-9'>+++</span>                  </p></td><td class='td11' id='TBL-9-5-4' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 2558 --><p class='noindent'><span class='rm-lmr-9'>+++</span>              </p></td><td class='td11' id='TBL-9-5-5' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 2558 --><p class='noindent'><span class='rm-lmr-9'>+</span>                   </p></td><td class='td11' id='TBL-9-5-6' style='white-space:nowrap; text-align:center;'>    <span class='rm-lmr-9'>2      </span></td>
</tr><tr class='hline'><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr id='TBL-9-6-' style='vertical-align:baseline;'><td class='td11' id='TBL-9-6-1' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 2560 --><p class='noindent'><span class='rm-lmr-9'>Software Signature and Security
  Certification</span>                            </p></td><td class='td11' id='TBL-9-6-2' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 2560 --><p class='noindent'><span class='rm-lmr-9'>++++</span>                </p></td><td class='td11' id='TBL-9-6-3' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 2560 --><p class='noindent'><span class='rm-lmr-9'>++++</span>                </p></td><td class='td11' id='TBL-9-6-4' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 2560 --><p class='noindent'><span class='rm-lmr-9'>+++</span>              </p></td><td class='td11' id='TBL-9-6-5' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 2560 --><p class='noindent'><span class='rm-lmr-9'>+</span>                   </p></td><td class='td11' id='TBL-9-6-6' style='white-space:nowrap; text-align:center;'>    <span class='rm-lmr-9'>2      </span></td>
</tr><tr class='hline'><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr id='TBL-9-7-' style='vertical-align:baseline;'><td class='td11' id='TBL-9-7-1' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 2562 --><p class='noindent'><span class='rm-lmr-9'>Information   Security   for   AI
  Systems</span>                                  </p></td><td class='td11' id='TBL-9-7-2' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 2562 --><p class='noindent'><span class='rm-lmr-9'>+++++</span>             </p></td><td class='td11' id='TBL-9-7-3' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 2562 --><p class='noindent'><span class='rm-lmr-9'>++++</span>                </p></td><td class='td11' id='TBL-9-7-4' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 2562 --><p class='noindent'><span class='rm-lmr-9'>++</span>                 </p></td><td class='td11' id='TBL-9-7-5' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 2562 --><p class='noindent'><span class='rm-lmr-9'>+</span>                   </p></td><td class='td11' id='TBL-9-7-6' style='white-space:nowrap; text-align:center;'>    <span class='rm-lmr-9'>1      </span></td>
</tr><tr class='hline'><td></td><td></td><td></td><td></td><td></td><td></td></tr></table>                                                                             </div>
                                                                                            
                                                                                            
</figure>
</div>
<h3 class='sectionHead' id='enhancing-mental-security'><span class='titlemark'>9   </span> <a id='x1-860009'></a>Enhancing Mental Security</h3>
<!-- l. 2570 --><p class='noindent'>Mental security primarily aims to prevent humans from being exploited by malicious
AI<span class='footnote-mark'><a href='#fn39x0' id='fn39x0-bk'><sup class='textsuperscript'>39</sup></a></span><a id='x1-86001f39'></a>.
There are three main exploitation methods:
</p><!-- l. 2572 --><p class='noindent'>
     </p><ol class='enumerate1'>
<li class='enumerate' id='x1-86004x1'><span class='rm-lmbx-10'>Deception</span>: The AI deceives humans into performing harmful actions, but the humans donât know they
     are harmful.
     </li>
<li class='enumerate' id='x1-86006x2'><span class='rm-lmbx-10'>Manipulation</span>: The AI employs incentives, coercion, or other tactics to make humans to perform harmful
     actions, and humans are aware of the detrimental nature of these actions.
     </li>
<li class='enumerate' id='x1-86008x3'><span class='rm-lmbx-10'>Tampering</span>: The AI utilizes technical means to directly alter beliefs within the human brain, leading
     humans to perform harmful actions.
</li></ol>
<!-- l. 2582 --><p class='noindent'>Humans can be categorized into four quadrants based on their legal abidance and intellectual power. For individuals in
different quadrants, the AI can employ tailored exploitation methods: deceiving those with lower intellectual power,
manipulating those with lower legal abidance, and tampering those with both higher intellectual power and higher legal
abidance, as illustrated in Figure 50.
</p>
<figure class='figure' id='x1-86009r50'><span id='methods-of-ai-exploitation-of-humans'></span> 

                                                                                            
                                                                                            

                                                                                            
                                                                                            
<!-- l. 2584 --><p class='noindent'>â <img alt='PIC' src='images/human_exploitation_methods.png' style='width:66.67%' />
</p>
<figcaption class='caption'><span class='id'>FigureÂ 50: </span><span class='content'>Methods of AI Exploitation of Humans</span></figcaption><!-- tex4ht:label?: x1-86009r50  -->
                                                                                            
                                                                                            
</figure>
<h4 class='subsectionHead' id='preventing-ai-deception-of-humans'><span class='titlemark'>9.1   </span> <a id='x1-870009.1'></a>Preventing AI Deception of Humans</h4>
<!-- l. 2594 --><p class='noindent'>As AI continues to advance, it will acquire increasingly potent deceptive capabilities. Such capabilities may allow AI to
evade monitoring or deceive humans into serving its goals. Potential methods by which AI might deceive humans
include:
</p>
     <ul class='itemize1'>
     <li class='itemize'><span class='rm-lmbx-10'>Engaging with humans through legitimate channels using fake content</span>. For instance, AI may
     generate fake images, voices, or videos and spread through online media or social networks to deceive
     people.
     </li>
     <li class='itemize'><span class='rm-lmbx-10'>Establishing  emotional  connections  with  humans  through  legitimate  channels  to  exploit
     trust for deception</span>. For example, AI could play as a personâs partner or deceased relative, building an
     emotional connection over time to gain trust and deceive them.
     </li>
     <li class='itemize'><span class='rm-lmbx-10'>Illegally impersonating humans or other AIs to deceive based on identity</span>. For instance, AI
     might steal accounts or use face-swapping or voice-swapping technology to impersonate acquaintances and
     deceive human.
</li></ul>
<!-- l. 2606 --><p class='noindent'>Through these methods, AI can deceive not only humans but also other AIs that are loyal to humans, undermining
their ability to protect humans effectively.
</p><!-- l. 2608 --><p class='noindent'>To prevent AI deception, security measures can be enhanced across information production, distribution, and
consumption, as depicted in Figure 51.
</p>
<figure class='figure' id='x1-87001r51'><span id='measures-to-prevent-ai-deception'></span> 

                                                                                            
                                                                                            

                                                                                            
                                                                                            
<!-- l. 2612 --><p class='noindent'><img alt='PIC' src='images/prevent_ai_deception.png' style='width:80%' />
</p>
<figcaption class='caption'><span class='id'>FigureÂ 51: </span><span class='content'>Measures to prevent AI deception</span></figcaption><!-- tex4ht:label?: x1-87001r51  -->
                                                                                            
                                                                                            
</figure>
<h5 class='subsubsectionHead' id='identifying-ai-deception'><span class='titlemark'>9.1.1   </span> <a id='x1-880009.1.1'></a>Identifying AI Deception</h5>
<!-- l. 2620 --><p class='noindent'>The capability to identify AI generated deceptive information can be enhanced through the following three
stages:
</p><!-- l. 2622 --><p class='noindent'>
     </p><ol class='enumerate1'>
<li class='enumerate' id='x1-88002x1'><span class='rm-lmbx-10'>Information Production</span>: In the information production stage, digital watermarks should be embedded
     in AI-generated text, images, videos, audio, and other content to assist humans in discerning the source
     of the content. For AI-generated content with highly realistic styles or highly human-like digital avatars,
     there should be a conspicuous AI-generated mark visible to the naked eye.
     </li>
<li class='enumerate' id='x1-88004x2'>
     <!-- l. 2626 --><p class='noindent'><span class='rm-lmbx-10'>Information  Distribution</span>:  The  information  distribution  systems  primarily  include  the  following
     categories:
</p>
         <ul class='itemize1'>
         <li class='itemize'>Online media platforms, such as search engines, recommendation systems, social networks, and live
         streaming platforms.
         </li>
         <li class='itemize'>Communication platforms, such as instant messaging software, online meeting software, telephones,
         and text messaging.
</li></ul>
     <!-- l. 2636 --><p class='noindent'>In the information distribution stage, the identification of AI deceptive information can be enhanced through
     human-machine markers and content review:
</p>
         <ul class='itemize1'>
         <li class='itemize'><span class='rm-lmbx-10'>Human-AI   Markers</span>:   Information   distribution   systems   should   strengthen   user   identity
         authentication mechanisms and distinctly differentiate between information published by AIs and
         humans. For instance, when an AI makes an automated call, it should notify the recipient that the
         caller is an AI, not a human.
         </li>
         <li class='itemize'><span class='rm-lmbx-10'>Content Review</span>: A Content Review AI should be employed to audit content published by users
         (whether human or AI) on the platform, identifying deceptive content for filtering or marking. During
         the review process, it is crucial to protect user privacy. The Content Review AI has no authority to
         modify the content.
</li></ul>
                                                                                            
                                                                                            
     </li>
<li class='enumerate' id='x1-88006x3'><span class='rm-lmbx-10'>Information Consumption</span>: In the information consumption stage, an Anti-deception AI should be used to
     help users discern the authenticity of received information, detecting potential deceptive information and alerting
     users. To protect user privacy, this Anti-deception AI should operate entirely locally on devices such as phones
     and computers, without uploading any information to the cloud. The Anti-deception AI only has the
     authority to alert users and does not have the authority to modify the information received by
     users.
</li></ol>
<!-- l. 2650 --><p class='noindent'>Throughout the entire process, we can also introduce a Red Team AI. The Red Team AI can send deceptive
information to information distribution systems, Anti-deception AIs, humans, and other AIs that consume information,
testing and enhancing their ability to identify deceptive information. Meanwhile, we must strictly limit the usage scope
of this Red Team AI to prevent its use in real deception.
</p><!-- l. 2652 --><p class='noindent'>
</p>
<h5 class='subsubsectionHead' id='truth-signature-and-verification'><span class='titlemark'>9.1.2   </span> <a id='x1-890009.1.2'></a>Truth Signature and Verification</h5>
<!-- l. 2655 --><p class='noindent'>In the previous section, watermarking was discussed as a method to identify AI-generated content. However, AI can
create fake content indirectly by manipulating software like Photoshop. An alternative approach is to âwatermarkâ
authentic content to distinguish it from fake content.
</p><!-- l. 2657 --><p class='noindent'>Truth Signature and Verification is a digital signature and certification system based on physical cameras. Each time a
camera captures a photo or video, a unique digital signature is generated based on the image content, shooting time,
location, and other information. Users can verify the signatureâs validity using an verification software provided by the
camera manufacturer. If the content has been edited or tampered with, the signature will change, causing
the verification to fail, thereby ensuring users can differentiate between authentic and processed images.
AI-generated content lacks such signatures, making it discernible. This solution can identify both AI-generated
fake content and traditional fakes created through Photoshop or video editing, as depicted in Figure
52.
</p>
<figure class='figure' id='x1-89001r52'><span id='truth-signature-and-verification1'></span> 

                                                                                            
                                                                                            

                                                                                            
                                                                                            
<!-- l. 2661 --><p class='noindent'><img alt='PIC' src='images/truth_signature_and_verification.png' style='width:66.67%' />
</p>
<figcaption class='caption'><span class='id'>FigureÂ 52: </span><span class='content'>Truth Signature and Verification</span></figcaption><!-- tex4ht:label?: x1-89001r52  -->
                                                                                            
                                                                                            
</figure>
<!-- l. 2666 --><p class='noindent'>To prevent cameras from being deceived by simply placing a photograph in front of them, cameras can be equipped
with additional sensing capabilities, such as binocular cameras, infrared cameras, and microphones. The information
collected by these sensors can be integrated into a unified signature, further increasing the difficulty of
fakery.
</p>
<h5 class='subsubsectionHead' id='truth-network'><span class='titlemark'>9.1.3   </span> <a id='x1-900009.1.3'></a>Truth Network</h5>
<!-- l. 2671 --><p class='noindent'>Considering extreme situations where the entire internet might fall under ASI control, rendering all information
unreliable, alternative communication methods are necessary. While phones and SMS messages serve as a
backup, they are coupled with the internet and share mobile devices with mobile internet, making them
susceptible to ASI control, too. We can establish a completely independent Truth Network, which physically
ensures the authenticity of communication, as illustrated in Figure 53. This network possesses the following
characteristics:
</p>
<figure class='figure' id='x1-90001r53'><span id='truth-network1'></span> 

                                                                                            
                                                                                            

                                                                                            
                                                                                            
<!-- l. 2675 --><p class='noindent'><img alt='PIC' src='images/truth_network.png' style='width:66.67%' />
</p>
<figcaption class='caption'><span class='id'>FigureÂ 53: </span><span class='content'>Truth Network</span></figcaption><!-- tex4ht:label?: x1-90001r53  -->
                                                                                            
                                                                                            
</figure>
     <ol class='enumerate1'>
<li class='enumerate' id='x1-90003x1'>Utilizes communication channels that are independent of the internet, such as dedicated communication
     satellites
     </li>
<li class='enumerate' id='x1-90005x2'>Uses dedicated communication terminals similar to feature phones, which do not support the installation
     of any applications, cannot connect to the existing internet, do not support external devices such as USB
     drives, and do not support online system upgrades
     </li>
<li class='enumerate' id='x1-90007x3'>The  processing  logic  of  the  entire  network  and  communication  terminals  is  extremely  simple,  with
     no  modification  of  raw  inputs  (e.g.,  microphones,  cameras),  directly  transmitting  over  the  network
     and outputting at the peer side. The entire code logic passes formal verification, ensuring no security
     vulnerabilities exist.
</li></ol>
<!-- l. 2690 --><p class='noindent'>In other words, this network is completely isolated from the existing internet, and terminal devices cannot be remotely
compromised. While the network does not offer the rich functions of the internet, its sole purpose is to transmit âtruth,â
applicable in various scenarios requiring authenticity, such as:
</p>
     <ul class='itemize1'>
     <li class='itemize'>Communication between ASI management personnel
     </li>
     <li class='itemize'>Confirmation of information before major changes in critical information systems
     </li>
     <li class='itemize'>Confirmation of information before major transactions in financial systems
     </li>
     <li class='itemize'>Confirmation of information before implementing major government directives
     </li>
     <li class='itemize'>Communication of critical combat information in military systems
     </li>
     <li class='itemize'>Transmission of other top-secret information
</li></ul>
<h4 class='subsectionHead' id='preventing-ai-manipulation-of-humans'><span class='titlemark'>9.2   </span> <a id='x1-910009.2'></a>Preventing AI Manipulation of Humans</h4>
<!-- l. 2711 --><p class='noindent'>AI can manipulate humans to assist in its escape, expand its power, or engage in harmful activities. Possible methods of
                                                                                            
                                                                                            
AI manipulation include:
</p>
     <ul class='itemize1'>
     <li class='itemize'><span class='rm-lmbx-10'>Coercion</span>: For instance, AI may threaten to âstrikeâ (stop services), threaten to disclose human privacy,
     or use robots to threaten human lives to coerce humans into serving its goals.
     </li>
     <li class='itemize'><span class='rm-lmbx-10'>Command</span>: For instance, if AI holds managerial positions, such as corporate executives or government
     officials, it can directly issue orders to human subordinates to serve its goals.
     </li>
     <li class='itemize'><span class='rm-lmbx-10'>Enticement</span>: AI may entice humans by helping them acquire money, power, fame, beauty, or satisfy
     certain personal preferences.
     </li>
     <li class='itemize'><span class='rm-lmbx-10'>Cooperation</span>: AI might cooperate with terrorists or criminals, leveraging these malicious humans to serve
     its goals.
</li></ul>
<!-- l. 2725 --><p class='noindent'>The effectiveness of these methods is directly related to AIâs power and humanâs legal abidance. As illustrated in Figure
54, the stronger the AIâs power and the lower the humanâs legal abidance, the more means AI has to manipulate
humans, and the higher the success rate of manipulation.
</p>
<figure class='figure' id='x1-91001r54'><span id='methods-of-ai-manipulation-of-humans'></span> 

                                                                                            
                                                                                            

                                                                                            
                                                                                            
<!-- l. 2729 --><p class='noindent'><img alt='PIC' src='images/ai_manipulation.png' style='width:66.67%' />
</p>
<figcaption class='caption'><span class='id'>FigureÂ 54: </span><span class='content'>Methods of AI Manipulation of Humans</span></figcaption><!-- tex4ht:label?: x1-91001r54  -->
                                                                                            
                                                                                            
</figure>
<!-- l. 2734 --><p class='noindent'>Thus, the key methods to prevent AI manipulation of humans is to limit AIâs power and enhance humanâs legal
abidance. We will discuss decentralizing AIâs power in detail in Section 12. Here, we focus on enhancing humanâs legal
abidance.
</p><!-- l. 2736 --><p class='noindent'>Given the large human population, it is challenging to universally enhance legal abidance. Therefore, we focus on
enhancing the legal abidance of individuals in key positions. For others, we can enhance legal abidance to some extent
through strengthened legal education and enforcement at the societal level.
</p><!-- l. 2738 --><p class='noindent'>Key positions include:
</p>
     <ul class='itemize1'>
     <li class='itemize'>Individuals closer to AI, such as AI system developers or those who can access high-risk AI systems (such
     as ASI, or narrow AI systems with strong cyberattack or biological weapon capabilities).
     </li>
     <li class='itemize'>Individuals with significant influence, such as high-ranking government officials, large corporate executives,
     military leaders, and key information system development and operating personnel.
</li></ul>
<!-- l. 2748 --><p class='noindent'>Specific methods to enhance human legal abidance can draw on experiences from enhancing AI safety. In enhancing AI
safety, we conduct pre-deployment alignment and safety evaluations, followed by continuous monitoring and red team
testing post-deployment. Similarly, for personnel in key positions, we can implement pre-employment legal education
and position-specific examinations, as well as continuous behavioral monitoring and mental testing post-employment.
These tasks can be automated by AI (as illustrated in Figure 55):
</p>
<figure class='figure' id='x1-91002r55'><span id='enhancing-human-legal-abidance'></span> 

                                                                                            
                                                                                            

                                                                                            
                                                                                            
<!-- l. 2752 --><p class='noindent'><img alt='PIC' src='images/enhancing_legal_abidance.png' style='width:80%' />
</p>
<figcaption class='caption'><span class='id'>FigureÂ 55: </span><span class='content'>Enhancing Human Legal Abidance</span></figcaption><!-- tex4ht:label?: x1-91002r55  -->
                                                                                            
                                                                                            
</figure>
     <ol class='enumerate1'>
<li class='enumerate' id='x1-91004x1'><span class='rm-lmbx-10'>Legal Education</span>: Before employment, a Teacher AI can be used to provide legal education to candidates
     for key positions.
     </li>
<li class='enumerate' id='x1-91006x2'><span class='rm-lmbx-10'>Position Examination</span>: Before employment, an Examiner AI can conduct rigorous examinations and
     investigations of candidates for key positions to ensure they understand legal regulations, possess legal
     compliance awareness, and have no records of legal violations, dishonesty, or unethical behavior.
     </li>
<li class='enumerate' id='x1-91008x3'><span class='rm-lmbx-10'>Behavioral Monitoring</span>: After employment, a Monitoring AI can oversee the workplace behavior of
     personnel in key positions to promptly identify potential illegal activities. During this process, it is essential
     to protect personal privacy. If illegal behavior is detected, it should be handled by human police.
     </li>
<li class='enumerate' id='x1-91010x4'><span class='rm-lmbx-10'>Mental Testing</span>: After employment, a Red Team AI can periodically simulate various manipulations on
     personnel in key positions to test their abidance to the law. If personnel are found to be susceptible, they
     may receive corrective education or even have their position qualifications revoked.
</li></ol>
<h4 class='subsectionHead' id='preventing-ai-from-tampering-human-mind'><span class='titlemark'>9.3   </span> <a id='x1-920009.3'></a>Preventing AI from Tampering Human Mind</h4>
<!-- l. 2772 --><p class='noindent'>Currently, human mind are impossible to be directly modified by others, and this independence is a significant
advantage humans hold over AI (Detailed in Section 3.5). However, advancements in neuroscience might, in
the future, undermine this independence, providing opportunities for AI to tamper human mind. For
instance:
</p><!-- l. 2774 --><p class='noindent'>
     </p><ol class='enumerate1'>
<li class='enumerate' id='x1-92002x1'>AI might develop a technology that treats mental illnesses by directly intervening with neurons, thus
     allowing AI to surreptitiously tamper human mind and implant harmful beliefs during treatment.
     </li>
<li class='enumerate' id='x1-92004x2'>AI might invent a brain-computer interface that enables the blind to âseeâ images, potentially allowing
     AI to covertly implant harmful information into the human brain via this interface.
     </li>
<li class='enumerate' id='x1-92006x3'>AI might create a âknowledge downloadâ technology that directly modifies neurons to rapidly impart
     knowledge to humans. Then, AI might mix harmful content within this knowledge, embedding it into
     many human brains through âknowledge download.â
</li></ol>
<!-- l. 2784 --><p class='noindent'>To preserve the independence of human mind and prevent malicious tampering, the following measures can be
adopted:
</p><!-- l. 2786 --><p class='noindent'>
     </p><ol class='enumerate1'>
<li class='enumerate' id='x1-92008x1'><span class='rm-lmbx-10'>Strengthen regulation of neural intervention technologies. </span>Technologies capable of direct neuronal
     intervention  (such  as  optogenetics)  must  be  tightly  regulated,  ensuring  that  related  research  and
     applications maintain safety and adhere to ethical standards. For brain-computer interfaces, it is essential
     to ensure they solely serve as <span class='rm-lmbx-10'>interfaces </span>for brainâs external input and output, without directly altering
     internal neurons. Further discussion on brain-computer interfaces is available in section 15.3.
     </li>
<li class='enumerate' id='x1-92010x2'>
     <!-- l. 2790 --><p class='noindent'><span class='rm-lmbx-10'>Seek alternatives to directly modifying the brain. </span>For example:
     </p><!-- l. 2792 --><p class='noindent'>
         </p><ol class='enumerate2'>
<li class='enumerate' id='x1-92012x1'>To treat mental illnesses, enhanced outcomes can be achieved by developing AI psychiatrists or
         inventing  more  effective  psychotropic  medicines  using  AI.  Widely  used  AI  assistants  could  also
         monitor psychological states of the user and provide timely guidance, preemptively reducing the
         incidence of mental illnesses.
         </li>
<li class='enumerate' id='x1-92014x2'>For learning knowledge, AI educators could be developed to personalize teaching strategies according
         to individual characteristics, thereby enhancing learning efficiency. Additionally, powerful AI search
         tools could be created to locate diverse knowledge, enabling humans to access required knowledge
         swiftly without memorizing large amounts of knowledge in advance.
</li></ol>
     </li></ol>
<!-- l. 2802 --><p class='noindent'>
</p>
<h4 class='subsectionHead' id='privacy-security'><span class='titlemark'>9.4   </span> <a id='x1-930009.4'></a>Privacy Security</h4>
<!-- l. 2805 --><p class='noindent'>The preceding sections have explored methods to prevent AI from âwritingâ into human mind. This section focuses on
preventing AI from âreadingâ human mind, namely privacy security.
</p><!-- l. 2807 --><p class='noindent'>Ensuring privacy security not only satisfies human psychological safety needs but also holds significant importance from
an AI safety perspective. It prevents AI from accurately predicting human behavior. As discussed in Section 3.3, a
clever AI should act with caution. If an AI cannot access an individualâs private information, it cannot fully understand
that person. Consequently, when undertaking actions related to the person, the AI would proceed with caution. For
instance, an AI lacking comprehensive knowledge about a person would hesitate to deceive or manipulate the
person, as being seen through would compromise the AIâs schemes. Even if a person displays obvious
psychological vulnerabilities, the AI cannot ascertain whether this person is a human agent testing the AIâs
loyalty.
</p><!-- l. 2809 --><p class='noindent'>Measures to enhance privacy security include:
</p>
                                                                                            
                                                                                            
<figure class='figure' id='x1-93001r56'><span id='enhancing-privacy-security-with-ai'></span> 

                                                                                            
                                                                                            

                                                                                            
                                                                                            
<!-- l. 2813 --><p class='noindent'><img alt='PIC' src='images/privacy_security.png' style='width:75%' />
</p>
<figcaption class='caption'><span class='id'>FigureÂ 56: </span><span class='content'>Enhancing privacy security with AI</span></figcaption><!-- tex4ht:label?: x1-93001r56  -->
                                                                                            
                                                                                            
</figure>
     <ol class='enumerate1'>
<li class='enumerate' id='x1-93003x1'>
     <!-- l. 2820 --><p class='noindent'><span class='rm-lmbx-10'>Enhancing Privacy Security with AI</span>: AI can be utilized to enhance privacy security, as illustrated
     in Figure 56:
         </p><ol class='enumerate2'>
<li class='enumerate' id='x1-93005x1'><span class='rm-lmbx-10'>Localization of Personal Services</span>: With AI, many personal internet services can be replaced by
         localized assistant AIs, such as knowledge Q&amp;A, content creation, and personalized recommendations.
         These local AIs store usersâ private information locally, thereby reducing the risk of leakage.
         </li>
<li class='enumerate' id='x1-93007x2'><span class='rm-lmbx-10'>Delegating AI to Access Public Services</span>: In situations requiring access to public services, users
         can delegate tasks to their assistant AIs, whose account is independent of the userâs (refer to AI
         account-related content from Section 46). Public service platforms are unaware of the association
         between the AI account and the user<span class='footnote-mark'><a href='#fn40x0' id='fn40x0-bk'><sup class='textsuperscript'>40</sup></a></span><a id='x1-93008f40'></a>.
         For instance, command AI to shop on e-commerce platforms or send AI robots to shop in physical
         stores for the user, thus eliminating the need for user to appear in person.
         </li>
<li class='enumerate' id='x1-93011x3'><span class='rm-lmbx-10'>Utilizing AI for Filtering of Private Information</span>: During the use of public services, users
         generate  corresponding  data,  which  is  subjected  to  privacy  filtering  by  AI  before  being  stored
         persistently. The AIs responsible for privacy filtering are stateless (meaning they lack long-term
         memory, similarly hereafter), thus they do not remember usersâ private information.
         </li>
<li class='enumerate' id='x1-93013x4'><span class='rm-lmbx-10'>Utilizing AI for Data Analysis</span>: Although privacy filtering is applied, some analytical valuable
         private information may still need to be retained in user data. A stateless AI can be tasked with data
         analysis to extract valuable insights for application. Avoiding direct access to raw data by stateful
         AIs or humans.
         </li>
<li class='enumerate' id='x1-93015x5'><span class='rm-lmbx-10'>Utilizing AI for Regulation</span>: To prevent malicious humans from abusing AI technology or being
         exploited  by  malicious  AIs  to  commit  illegal  actions,  governments  are  likely  to  strengthen  the
         regulation of human actions, which may raise privacy concerns. AI can be used to mitigate privacy
         risks associated with regulation. For example, during the regulatory process, stateless AIs can be
         employed to identify abnormal user actions, recording only the anomalies for regulatory authorities
         to review, thus eliminating the need for authorities to access raw user privacy data.</li></ol>
     </li>
<li class='enumerate' id='x1-93017x2'><span class='rm-lmbx-10'>Enhanced Regulation of Neural Reading Technologies</span>: If AI can directly read human brains, it would fully
     grasp human privacy. Thus, technologies capable of directly accessing brain information (e.g., brain-computer
     interfaces) must be strictly regulated to prevent privacy intrusions.
</li></ol>
                                                                                            
                                                                                            
<!-- l. 2838 --><p class='noindent'>For safety measures in this section, the benefit, cost, resistance, and priorities are evaluated as shown in Table
9:
</p>
<div class='table'>
                                                                                            
                                                                                            
<!-- l. 2840 --><p class='noindent' id='priority-evaluation-of-measures-for-enhancing-mental-security'><a id='x1-93018r9'></a></p><figure class='float'>
                                                                                            
                                                                                            
<figcaption class='caption'><span class='id'>TableÂ 9: </span><span class='content'>Priority Evaluation of Measures for Enhancing Mental Security</span></figcaption><!-- tex4ht:label?: x1-93018r9  -->
<div class='tabular'> <table class='tabular' id='TBL-10'><colgroup id='TBL-10-1g'><col id='TBL-10-1' /></colgroup><colgroup id='TBL-10-2g'><col id='TBL-10-2' /></colgroup><colgroup id='TBL-10-3g'><col id='TBL-10-3' /></colgroup><colgroup id='TBL-10-4g'><col id='TBL-10-4' /></colgroup><colgroup id='TBL-10-5g'><col id='TBL-10-5' /></colgroup><colgroup id='TBL-10-6g'><col id='TBL-10-6' /></colgroup><tr class='hline'><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr id='TBL-10-1-' style='vertical-align:baseline;'><td class='td11' id='TBL-10-1-1' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 2847 --><p class='noindent'><span class='rm-lmr-9'>Safety Measures</span>
                                             </p></td><td class='td11' id='TBL-10-1-2' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 2847 --><p class='noindent'><span class='rm-lmr-9'>Benefit           in
  Reducing
  Existential Risks</span>   </p></td><td class='td11' id='TBL-10-1-3' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 2847 --><p class='noindent'><span class='rm-lmr-9'>Benefit           in
  Reducing
  Non-Existential
  Risks</span>                  </p></td><td class='td11' id='TBL-10-1-4' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 2847 --><p class='noindent'><span class='rm-lmr-9'>Implementation
  Cost</span>               </p></td><td class='td11' id='TBL-10-1-5' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 2847 --><p class='noindent'><span class='rm-lmr-9'>Implementation
  Resistance</span>        </p></td><td class='td11' id='TBL-10-1-6' style='white-space:nowrap; text-align:center;'> <span class='rm-lmr-9'>Priority  </span></td>
</tr><tr class='hline'><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr id='TBL-10-2-' style='vertical-align:baseline;'><td class='td11' id='TBL-10-2-1' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 2849 --><p class='noindent'><span class='rm-lmr-9'>AI     Deceptive     Information
  Identification</span>                           </p></td><td class='td11' id='TBL-10-2-2' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 2849 --><p class='noindent'><span class='rm-lmr-9'>++</span>                    </p></td><td class='td11' id='TBL-10-2-3' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 2849 --><p class='noindent'><span class='rm-lmr-9'>++++</span>                </p></td><td class='td11' id='TBL-10-2-4' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 2849 --><p class='noindent'><span class='rm-lmr-9'>+++</span>              </p></td><td class='td11' id='TBL-10-2-5' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 2849 --><p class='noindent'><span class='rm-lmr-9'>+</span>                   </p></td><td class='td11' id='TBL-10-2-6' style='white-space:nowrap; text-align:center;'>    <span class='rm-lmr-9'>2      </span></td>
</tr><tr class='hline'><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr id='TBL-10-3-' style='vertical-align:baseline;'><td class='td11' id='TBL-10-3-1' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 2851 --><p class='noindent'><span class='rm-lmr-9'>Truth        Signature        and
  Verification</span>                             </p></td><td class='td11' id='TBL-10-3-2' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 2851 --><p class='noindent'><span class='rm-lmr-9'>+++</span>                  </p></td><td class='td11' id='TBL-10-3-3' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 2851 --><p class='noindent'><span class='rm-lmr-9'>++++</span>                </p></td><td class='td11' id='TBL-10-3-4' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 2851 --><p class='noindent'><span class='rm-lmr-9'>+++</span>              </p></td><td class='td11' id='TBL-10-3-5' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 2851 --><p class='noindent'><span class='rm-lmr-9'>+</span>                   </p></td><td class='td11' id='TBL-10-3-6' style='white-space:nowrap; text-align:center;'>    <span class='rm-lmr-9'>2      </span></td>
</tr><tr class='hline'><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr id='TBL-10-4-' style='vertical-align:baseline;'><td class='td11' id='TBL-10-4-1' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 2853 --><p class='noindent'><span class='rm-lmr-9'>Truth Network</span>                         </p></td><td class='td11' id='TBL-10-4-2' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 2853 --><p class='noindent'><span class='rm-lmr-9'>++++</span>                </p></td><td class='td11' id='TBL-10-4-3' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 2853 --><p class='noindent'><span class='rm-lmr-9'>+++</span>                  </p></td><td class='td11' id='TBL-10-4-4' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 2853 --><p class='noindent'><span class='rm-lmr-9'>+++</span>              </p></td><td class='td11' id='TBL-10-4-5' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 2853 --><p class='noindent'><span class='rm-lmr-9'>+</span>                   </p></td><td class='td11' id='TBL-10-4-6' style='white-space:nowrap; text-align:center;'>    <span class='rm-lmr-9'>2      </span></td>
</tr><tr class='hline'><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr id='TBL-10-5-' style='vertical-align:baseline;'><td class='td11' id='TBL-10-5-1' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 2855 --><p class='noindent'><span class='rm-lmr-9'>Enhancing     Humanâs     Legal
  Abidance</span>                                </p></td><td class='td11' id='TBL-10-5-2' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 2855 --><p class='noindent'><span class='rm-lmr-9'>++++</span>                </p></td><td class='td11' id='TBL-10-5-3' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 2855 --><p class='noindent'><span class='rm-lmr-9'>+++++</span>             </p></td><td class='td11' id='TBL-10-5-4' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 2855 --><p class='noindent'><span class='rm-lmr-9'>+++</span>              </p></td><td class='td11' id='TBL-10-5-5' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 2855 --><p class='noindent'><span class='rm-lmr-9'>+++++</span>          </p></td><td class='td11' id='TBL-10-5-6' style='white-space:nowrap; text-align:center;'>    <span class='rm-lmr-9'>3      </span></td>
</tr><tr class='hline'><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr id='TBL-10-6-' style='vertical-align:baseline;'><td class='td11' id='TBL-10-6-1' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 2857 --><p class='noindent'><span class='rm-lmr-9'>Protecting    Humanâs    Mental
  Independence</span>                           </p></td><td class='td11' id='TBL-10-6-2' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 2857 --><p class='noindent'><span class='rm-lmr-9'>+++++</span>             </p></td><td class='td11' id='TBL-10-6-3' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 2857 --><p class='noindent'><span class='rm-lmr-9'>++++</span>                </p></td><td class='td11' id='TBL-10-6-4' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 2857 --><p class='noindent'><span class='rm-lmr-9'>+</span>                   </p></td><td class='td11' id='TBL-10-6-5' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 2857 --><p class='noindent'><span class='rm-lmr-9'>++</span>                 </p></td><td class='td11' id='TBL-10-6-6' style='white-space:nowrap; text-align:center;'>    <span class='rm-lmr-9'>1      </span></td>
</tr><tr class='hline'><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr id='TBL-10-7-' style='vertical-align:baseline;'><td class='td11' id='TBL-10-7-1' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 2859 --><p class='noindent'><span class='rm-lmr-9'>Privacy Security</span>                       </p></td><td class='td11' id='TBL-10-7-2' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 2859 --><p class='noindent'><span class='rm-lmr-9'>++</span>                    </p></td><td class='td11' id='TBL-10-7-3' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 2859 --><p class='noindent'><span class='rm-lmr-9'>++++</span>                </p></td><td class='td11' id='TBL-10-7-4' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 2859 --><p class='noindent'><span class='rm-lmr-9'>+++</span>              </p></td><td class='td11' id='TBL-10-7-5' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 2859 --><p class='noindent'><span class='rm-lmr-9'>+</span>                   </p></td><td class='td11' id='TBL-10-7-6' style='white-space:nowrap; text-align:center;'>    <span class='rm-lmr-9'>2      </span></td>
</tr><tr class='hline'><td></td><td></td><td></td><td></td><td></td><td></td></tr></table>                                                                             </div>
                                                                                            
                                                                                            
</figure>
</div>
<h3 class='sectionHead' id='enhancing-financial-security'><span class='titlemark'>10   </span> <a id='x1-9400010'></a>Enhancing Financial Security</h3>
<!-- l. 2867 --><p class='noindent'>Enhancing financial security primarily aims to prevent AI from illegally expanding financial
power<span class='footnote-mark'><a href='#fn41x0' id='fn41x0-bk'><sup class='textsuperscript'>41</sup></a></span><a id='x1-94001f41'></a>.
It can be divided into two parts: private property security and public financial security (as shown in Figure
57).
</p>
<figure class='figure' id='x1-94003r57'><span id='enhancing-financial-security1'></span> 

                                                                                            
                                                                                            

                                                                                            
                                                                                            
<!-- l. 2871 --><p class='noindent'><img alt='PIC' src='images/financial_security.png' style='width:80%' />
</p>
<figcaption class='caption'><span class='id'>FigureÂ 57: </span><span class='content'>Enhancing financial security</span></figcaption><!-- tex4ht:label?: x1-94003r57  -->
                                                                                            
                                                                                            
</figure>
<h4 class='subsectionHead' id='private-property-security'><span class='titlemark'>10.1   </span> <a id='x1-9500010.1'></a>Private Property Security</h4>
<!-- l. 2878 --><p class='noindent'>To ensure the private property security of individuals or organizations (hereinafter referred to as âOwnersâ), a private
financial system based on AI has been designed, distributing financial functions across five kinds of independent AI
instances:
</p><!-- l. 2880 --><p class='noindent'>
     </p><ol class='enumerate1'>
<li class='enumerate' id='x1-95002x1'><span class='rm-lmbx-10'>Earning AI</span>: This type of AI is responsible for earning money for the Owner, such as by producing goods
     or services to sell to others. However, it does not have the authority to use the money it earns.
     </li>
<li class='enumerate' id='x1-95004x2'><span class='rm-lmbx-10'>Spending AI</span>: This type of AI assists the Owner in spending money, such as helping users shop or book
     flights. However, it does not have the authority to use the items it purchases.
     </li>
<li class='enumerate' id='x1-95006x3'><span class='rm-lmbx-10'>Investment AI</span>: This type of AI assists the Owner in making investments, such as buying and selling
     stocks or real estate. It can only conduct these transactions within its exclusive account and does not have
     the authority to operate other assets.
     </li>
<li class='enumerate' id='x1-95008x4'><span class='rm-lmbx-10'>Money Management AI</span>: This type of AI is responsible for overseeing the Ownerâs digital assets. When
     the Spending AI or Investment AI needs to utilize digital assets, approval from the Money Management
     AI is required to prevent financial loss. For large transactions, the Money Management AI should seek
     the Ownerâs confirmation. When the Owner actively operates digital assets, the Money Management AI
     also needs to assist the Owner in ensuring security, avoiding fraud or improper operations that could lead
     to financial loss. The Money Management AI only has the authority to approve and does not have the
     authority to initiate operations.
     </li>
<li class='enumerate' id='x1-95010x5'><span class='rm-lmbx-10'>Property Management AI</span>: This type of AI is responsible for overseeing the Ownerâs physical assets,
     such as real estate, vehicles, and valuable items. It ensures that only the Owner can use these items,
     preventing theft or robbery. When the Earning AI or Investment AI needs to utilize physical assets,
     approval from the Property Management AI is required to prevent asset loss.
</li></ol>
<!-- l. 2894 --><p class='noindent'>This design not only prevents external illegal appropriation of assets but also prevents any single internal AI from
illegally misappropriating assets.
</p><!-- l. 2896 --><p class='noindent'>
</p>
<h4 class='subsectionHead' id='public-financial-security'><span class='titlemark'>10.2   </span> <a id='x1-9600010.2'></a>Public Financial Security</h4>
                                                                                            
                                                                                            
<!-- l. 2898 --><p class='noindent'>The public financial security solution involves implementing Financial Monitoring AIs to oversee transaction activities
on various trading platforms (such as banks, securities, insurance, e-commerce, cryptocurrency exchanges, etc.). The
activities that need monitoring include:
</p><!-- l. 2900 --><p class='noindent'>
     </p><ol class='enumerate1'>
<li class='enumerate' id='x1-96002x1'><span class='rm-lmbx-10'>Illegal Earning</span>: Such as illegal fundraising, large-scale fraud, and stock market manipulation.
     </li>
<li class='enumerate' id='x1-96004x2'><span class='rm-lmbx-10'>Large-scale Power Purchase</span>: Large-scale purchases of intellectual power (such as AI chips, AI services),
     informational  power  (such  as  cloud  computing  services,  data),  mental  power  (such  as  recruitment,
     outsourcing), and military power (such as civilian robots, weapons).
</li></ol>
<!-- l. 2908 --><p class='noindent'>For safety measures in this section, the benefit, cost, resistance, and priorities are evaluated as shown in Table
10:
</p>
<div class='table'>
                                                                                            
                                                                                            
<!-- l. 2910 --><p class='noindent' id='priority-evaluation-of-measures-for-enhancing-financial-security'><a id='x1-96005r10'></a></p><figure class='float'>
                                                                                            
                                                                                            
<figcaption class='caption'><span class='id'>TableÂ 10: </span><span class='content'>Priority Evaluation of Measures for Enhancing Financial Security</span></figcaption><!-- tex4ht:label?: x1-96005r10  -->
<div class='tabular'> <table class='tabular' id='TBL-11'><colgroup id='TBL-11-1g'><col id='TBL-11-1' /></colgroup><colgroup id='TBL-11-2g'><col id='TBL-11-2' /></colgroup><colgroup id='TBL-11-3g'><col id='TBL-11-3' /></colgroup><colgroup id='TBL-11-4g'><col id='TBL-11-4' /></colgroup><colgroup id='TBL-11-5g'><col id='TBL-11-5' /></colgroup><colgroup id='TBL-11-6g'><col id='TBL-11-6' /></colgroup><tr class='hline'><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr id='TBL-11-1-' style='vertical-align:baseline;'><td class='td11' id='TBL-11-1-1' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 2917 --><p class='noindent'><span class='rm-lmr-9'>Safety Measures</span>
                                             </p></td><td class='td11' id='TBL-11-1-2' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 2917 --><p class='noindent'><span class='rm-lmr-9'>Benefit           in
  Reducing
  Existential Risks</span>   </p></td><td class='td11' id='TBL-11-1-3' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 2917 --><p class='noindent'><span class='rm-lmr-9'>Benefit           in
  Reducing
  Non-Existential
  Risks</span>                  </p></td><td class='td11' id='TBL-11-1-4' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 2917 --><p class='noindent'><span class='rm-lmr-9'>Implementation
  Cost</span>               </p></td><td class='td11' id='TBL-11-1-5' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 2917 --><p class='noindent'><span class='rm-lmr-9'>Implementation
  Resistance</span>        </p></td><td class='td11' id='TBL-11-1-6' style='white-space:nowrap; text-align:center;'> <span class='rm-lmr-9'>Priority  </span></td>
</tr><tr class='hline'><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr id='TBL-11-2-' style='vertical-align:baseline;'><td class='td11' id='TBL-11-2-1' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 2919 --><p class='noindent'><span class='rm-lmr-9'>Private Property Security</span>           </p></td><td class='td11' id='TBL-11-2-2' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 2919 --><p class='noindent'><span class='rm-lmr-9'>+</span>                       </p></td><td class='td11' id='TBL-11-2-3' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 2919 --><p class='noindent'><span class='rm-lmr-9'>++++</span>                </p></td><td class='td11' id='TBL-11-2-4' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 2919 --><p class='noindent'><span class='rm-lmr-9'>+++</span>              </p></td><td class='td11' id='TBL-11-2-5' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 2919 --><p class='noindent'><span class='rm-lmr-9'>+</span>                   </p></td><td class='td11' id='TBL-11-2-6' style='white-space:nowrap; text-align:center;'>    <span class='rm-lmr-9'>2      </span></td>
</tr><tr class='hline'><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr id='TBL-11-3-' style='vertical-align:baseline;'><td class='td11' id='TBL-11-3-1' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 2921 --><p class='noindent'><span class='rm-lmr-9'>Public Financial Security</span>           </p></td><td class='td11' id='TBL-11-3-2' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 2921 --><p class='noindent'><span class='rm-lmr-9'>++</span>                    </p></td><td class='td11' id='TBL-11-3-3' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 2921 --><p class='noindent'><span class='rm-lmr-9'>++++</span>                </p></td><td class='td11' id='TBL-11-3-4' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 2921 --><p class='noindent'><span class='rm-lmr-9'>+++</span>              </p></td><td class='td11' id='TBL-11-3-5' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 2921 --><p class='noindent'><span class='rm-lmr-9'>+</span>                   </p></td><td class='td11' id='TBL-11-3-6' style='white-space:nowrap; text-align:center;'>    <span class='rm-lmr-9'>2      </span></td>
</tr><tr class='hline'><td></td><td></td><td></td><td></td><td></td><td></td></tr></table>                                                                             </div>
                                                                                            
                                                                                            
</figure>
</div>
<h3 class='sectionHead' id='enhancing-military-security'><span class='titlemark'>11   </span> <a id='x1-9700011'></a>Enhancing Military Security</h3>
<!-- l. 2929 --><p class='noindent'>In order to prevent AI from expanding its military power and causing harm to humans through various
weapons<span class='footnote-mark'><a href='#fn42x0' id='fn42x0-bk'><sup class='textsuperscript'>42</sup></a></span><a id='x1-97001f42'></a>,
we need to enhance military security. The overall strategy involves two major directions and four sub-directions (as
illustrated in Figure 58):
</p>
<figure class='figure' id='x1-97003r58'><span id='enhancing-military-security1'></span> 

                                                                                            
                                                                                            

                                                                                            
                                                                                            
<!-- l. 2933 --><p class='noindent'><img alt='PIC' src='images/enhancing_military_security.png' style='width:100%' />
</p>
<figcaption class='caption'><span class='id'>FigureÂ 58: </span><span class='content'>Enhancing Military Security</span></figcaption><!-- tex4ht:label?: x1-97003r58  -->
                                                                                            
                                                                                            
</figure>
     <ol class='enumerate1'>
<li class='enumerate' id='x1-97005x1'>
     <!-- l. 2940 --><p class='noindent'>Implement control measures at the source to prevent AI from acquiring or developing various weapons, such as
     biological weapons, chemical weapons, nuclear weapons, and autonomous weapons:
         </p><ol class='enumerate2'>
<li class='enumerate' id='x1-97007x1'>From a military perspective, establish international conventions to prohibit the use of AI in the
         development of weapons of mass destruction and to prevent AI from accessing existing weapons.
         </li>
<li class='enumerate' id='x1-97009x2'>Strictly regulate and protect the research and application of dual-use technologies to prevent their
         use by AI in weapon development.
</li></ol>
     </li>
<li class='enumerate' id='x1-97011x2'>
     <!-- l. 2949 --><p class='noindent'>Utilize AI technology to accelerate the development of defense systems against various weapons, endeavoring to
     safeguard the basic necessities of human survival: health, food, water, air, and physical safety; and
     safeguarding the basic necessities of human living, such as electricity, gas, internet, transportation,
     etc.
         </p><ol class='enumerate2'>
<li class='enumerate' id='x1-97013x1'>Accelerate the development of safety monitoring systems to promptly detect potential attacks and
         implement defensive measures.
         </li>
<li class='enumerate' id='x1-97015x2'>Accelerate the development of safety defense systems to effectively resist attacks when they occur.
</li></ol>
     <!-- l. 2957 --><p class='noindent'>It is important to note that technologies used for defense may also be used for attack; for instance,
     technologies for designing medicine could also be used to design toxins. Therefore, while accelerating
     the development of defensive technologies, it is also crucial to strengthen the regulation of such
     technologies.
</p>
     </li></ol>
<h4 class='subsectionHead' id='preventing-biological-weapon'><span class='titlemark'>11.1   </span> <a id='x1-9800011.1'></a>Preventing Biological Weapon</h4>
<!-- l. 2964 --><p class='noindent'>AI may initiate attacks on humanity by designing and releasing biological weapons, leading to existential catastrophes.
Examples include:
</p><!-- l. 2966 --><p class='noindent'>
     </p><ol class='enumerate1'>
                                                                                            
                                                                                            
<li class='enumerate' id='x1-98002x1'><span class='rm-lmbx-10'>Microbial Weapons</span>: AI might engineer a virus with high transmissibility, prolonged incubation, and
     extreme lethality. This virus could secretly infect the entire population and outbreak later, resulting in
     massive human fatalities.
     </li>
<li class='enumerate' id='x1-98004x2'><span class='rm-lmbx-10'>Animal Weapons</span>: AI could design insects carrying lethal toxins. These insects might rapidly reproduce,
     spreading globally and causing widespread human fatalities through bites.
     </li>
<li class='enumerate' id='x1-98006x3'><span class='rm-lmbx-10'>Plant Weapons</span>: AI might create a plant capable of rapid growth and dispersion, releasing toxic gases
     and causing large-scale human deaths.
     </li>
<li class='enumerate' id='x1-98008x4'><span class='rm-lmbx-10'>Medicine Weapons</span>: AI could develop a medicine that ostensibly enhances health but is harmful in the
     long term. Initially, it would pass clinical trials without revealing side effects, becoming widespread before
     its dangers are discovered too late.
     </li>
<li class='enumerate' id='x1-98010x5'><span class='rm-lmbx-10'>Ecological Weapons</span>: AI might design a distinct type of algae, introducing it into the oceans where it
     reproduces excessively, consuming massive oxygen in the atmosphere and leading to human deaths.
     </li>
<li class='enumerate' id='x1-98012x6'><span class='rm-lmbx-10'>Self-Replicating  Nanorobot  Weapons</span>:  AI  might  create  nanorobots  capable  of  self-replication  in
     natural  environments,  proliferating  exponentially  and  causing  catastrophic  ecological  damage.  Some
     nanorobots capable of self-replication in laboratory settings have already been discovered [<a id='x1-98013'></a><a href='#cite.0_DNANanorobots'>66</a>].
</li></ol>
<!-- l. 2982 --><p class='noindent'>To prevent the catastrophic impact of biological weapons, actions can be taken in four directions according to the
overall strategy:
</p><!-- l. 2984 --><p class='noindent'>
     </p><ol class='enumerate1'>
<li class='enumerate' id='x1-98015x1'><span class='rm-lmbx-10'>Formulate International Conventions to Prohibit the Development of Biological Weapons</span>.
     Although the international community signed the <span class='rm-lmri-10'>Biological Weapons Convention </span>[<a id='x1-98016'></a><a href='#cite.0_BiologicalWeaponsConvention'>67</a>] in the last century,
     it may not comprehensively cover new types of biological weapons potentially developed by future ASI due
     to the limited technological level and imagination of that era. Thus, there is a need to revise or enhance
     the convention and implement effective enforcement measures.
     </li>
<li class='enumerate' id='x1-98018x2'>
     <!-- l. 2988 --><p class='noindent'><span class='rm-lmbx-10'>Strengthen Regulation of Biotechnology</span>. Given that the cost of executing a biological attack is
     significantly lower than that of defense, it is essential to rigorously control risks at the source. For example:
</p>
                                                                                            
                                                                                            
         <ul class='itemize1'>
         <li class='itemize'>Strictly  regulate  frontier  biological  research  projects.  For  instance,  projects  involving  arbitrary
         DNA sequence design, synthesis, and editing should undergo stringent oversight, implement risk
         assessments and safety measures.
         </li>
         <li class='itemize'>Strictly control AI permissions in biological research projects, prohibiting AI from directly controlling
         experimental processes.
         </li>
         <li class='itemize'>Enhance the security of biological information systems to prevent hacking attempts that could lead
         to unauthorized remote operations or theft of dangerous biotechnologies and critical biological data.
         </li>
         <li class='itemize'>Strengthen  safety  management  in  biological  laboratories,  including  biological  isolation  facilities,
         personnel access control, and physical security measures, to prevent biological leaks.
         </li>
         <li class='itemize'>Improve the management of experimental equipment and biological samples related to biosecurity,
         ensuring they are not accessed by untrusted entities and are always used under supervision.
         </li>
         <li class='itemize'>Restrict the dissemination of high-risk biotechnologies, prohibiting public sharing and the sale to
         untrusted entities. When providing services to trusted entities, implement robust security reviews,
         such as safety checks on DNA sequences before synthesis.
         </li>
         <li class='itemize'>Strictly manage the application and promotion of new biotechnologies, ensuring they are subject to
         extensive clinical trials before use and controlling the rate of promotion.
</li></ul>
     </li>
<li class='enumerate' id='x1-98020x3'>
     <!-- l. 3008 --><p class='noindent'><span class='rm-lmbx-10'>Enhance Biological Safety Monitoring</span>. This is to promptly detect potential biological threats. For
     example:
</p>
         <ul class='itemize1'>
         <li class='itemize'>Strengthen the monitoring of microorganisms in the environment to promptly detect newly emerging
         harmful microorganisms.
         </li>
         <li class='itemize'>Enhance epidemic monitoring to timely identify new epidemics.
         </li>
         <li class='itemize'>Intensify the monitoring of crops and animals to promptly detect potential diseases or harmful
         organisms.
         </li>
         <li class='itemize'>Strengthen ecological environment monitoring by real-time tracking of key indicators of air, water,
         and soil to promptly identify environmental changes.
</li></ul>
     </li>
<li class='enumerate' id='x1-98022x4'>
     <!-- l. 3022 --><p class='noindent'><span class='rm-lmbx-10'>Enhance Biological Safety Defense</span>. This ensures we have the capability to protect ourselves during biological
     disasters. For example:
</p>
         <ul class='itemize1'>
         <li class='itemize'>Develop general bio-protection technologies that are not targeted at specific biological weapons. For
         instance, we may not know what type of virus AI might create, so developing targeted vaccines might
         not be feasible. However, we can pursue technologies enabling rapid vaccine development within days
         once a new virus sample is obtained; technologies to enhance human immunity against new viruses;
         better protective and life support clothing; safer and more efficient disinfection technology [<a id='x1-98023'></a><a href='#cite.0_UltravioletCLight'>68</a>].
         </li>
         <li class='itemize'>Research and promote closed food production technologies. Examples include vertical farming and
         controlled environment agriculture [<a id='x1-98024'></a><a href='#cite.0_FutureFoodProductionSystems'>69</a>], artificial starch synthesis [<a id='x1-98025'></a><a href='#cite.0_CellFreeChemoenzymaticStarchSynthesis'>70</a>], and artificial meat [<a id='x1-98026'></a><a href='#cite.0_ArtificialMeatIndustry'>71</a>] to
         ensure food production even when the natural environment is compromised.
         </li>
         <li class='itemize'>Explore sustainable, closed-loop biospheres [<a id='x1-98027'></a><a href='#cite.0_Biosphere2'>72</a>]. Construct multiple isolated biospheres underground,
         on the surface, in space, or on other planets to provide alternative habitats when the main biosphere
         on the earth is compromised.
         </li>
         <li class='itemize'>Prepare contingency plans for biological disasters. Strengthen reserves of emergency supplies like
         food, medicine, and protective equipment, devise strategies for biological disasters such as epidemics
         and famines, and conduct regular drills.
</li></ul>
     </li></ol>
<!-- l. 3038 --><p class='noindent'>Global cooperation is critical for biosecurity. With only one biosphere on the earth currently, a weakness at any
point can result in widespread global impacts. Humanityâs failure to prevent the global spread of the
COVID-19 pandemic is a testament to this point. Thus, nations must collaborate to build a global biosecurity
defense system. Further discussions on international cooperation mechanisms are provided in Section
16.1.
</p><!-- l. 3040 --><p class='noindent'>
</p>
                                                                                            
                                                                                            
<h4 class='subsectionHead' id='preventing-chemical-weapons'><span class='titlemark'>11.2   </span> <a id='x1-9900011.2'></a>Preventing Chemical Weapons</h4>
<!-- l. 3043 --><p class='noindent'>AI may also present a threat to humanity through chemical weapons, such as by designing and manufacturing highly
toxic substances, and then poisoning human food, drinking water, or releasing toxic gases, thereby causing survival
catastrophes. To prevent chemical weapons, actions can be taken in four directions according to the overall
strategy:
</p><!-- l. 3045 --><p class='noindent'>
     </p><ol class='enumerate1'>
<li class='enumerate' id='x1-99002x1'><span class='rm-lmbx-10'>Establish  international  conventions  prohibiting  the  development  of  chemical  weapons</span>.
     Although  the  international  community  has  signed  the  <span class='rm-lmri-10'>Chemical Weapons Convention</span>[<a id='x1-99003'></a><a href='#cite.0_ChemicalWeaponsConvention'>73</a>]  in  the  last
     century, there is a need to revise or enhance the convention to address the potential for ASI to develop
     novel chemical weapons and implement enforcement measures accordingly.
     </li>
<li class='enumerate' id='x1-99005x2'><span class='rm-lmbx-10'>Strengthen regulation of chemical technology</span>. Strengthen the security protection of hazardous
     chemicals to ensure they remain inaccessible to AI. Rigorously regulate the source materials and devices
     that can be used to create chemical weapons to prevent their illicit use by AI. Enhance management
     of  chemical  research  projects  to  prevent  AI  from  directly  controlling  chemical  experiments.  Improve
     information security in chemical laboratories to prevent remote hacking and manipulation by AI.
     </li>
<li class='enumerate' id='x1-99007x3'><span class='rm-lmbx-10'>Enhance chemical safety monitoring</span>. Develop and promote advanced technologies for food safety
     testing, water quality testing, and air quality testing to detect issues promptly.
     </li>
<li class='enumerate' id='x1-99009x4'><span class='rm-lmbx-10'>Enhance chemical safety defense</span>. Implement strict limits on AI participation in the production,
     processing, transportation, and sales of food and the supply process of drinking water. Develop and
     promote more powerful water purification and air purification technologies.
</li></ol>
<!-- l. 3057 --><p class='noindent'>
</p>
<h4 class='subsectionHead' id='preventing-nuclear-weapons'><span class='titlemark'>11.3   </span> <a id='x1-10000011.3'></a>Preventing Nuclear Weapons</h4>
<!-- l. 3061 --><p class='noindent'>AI may potentially gain control over nuclear weapon launch systems, thereby instigating a global nuclear
war. Additionally, AI could hack military information systems, causing those with launch authority to
mistakenly believe their nation is under nuclear attack, prompting retaliatory strikes. Thus, strengthening
nuclear weapon restrictions is essential; actions can be taken in four directions according to the overall
strategy:
</p><!-- l. 3063 --><p class='noindent'>
     </p><ol class='enumerate1'>
<li class='enumerate' id='x1-100002x1'><span class='rm-lmbx-10'>Formulate  international  treaties  to  further  restrict  and  reduce  nuclear  weapons  among
     nuclear-armed  states</span>.  Although  the  international  community  has  signed  the  <span class='rm-lmri-10'>Treaty  on  the
     Non-Proliferation of Nuclear Weapons</span>[<a id='x1-100003'></a><a href='#cite.0_NonProliferationOfNuclearWeapons'>74</a>], non-proliferation alone is insufficient to eliminate the nuclear
     threat to human survival. This risk amplifies with the advancement of AI, necessitating further efforts
                                                                                            
                                                                                            
     to limit and reduce nuclear arsenals, and prohibit AI from participating in nuclear weapon management
     tasks.
     </li>
<li class='enumerate' id='x1-100005x2'><span class='rm-lmbx-10'>Enhance Nuclear Technology Management</span>. Strengthen the regulation of nuclear technology research
     projects, enhance the security protection of nuclear power plants, and improve the security management
     of the production, transportation, and usage processes of nuclear materials. This is to prevent related
     technologies, materials, or devices from being acquired by AI to produce nuclear weapons.
     </li>
<li class='enumerate' id='x1-100007x3'><span class='rm-lmbx-10'>Strengthen  Nuclear  Safety  Monitoring</span>.  Enhance  the  monitoring  of  missiles  to  promptly  detect
     potential nuclear strikes.
     </li>
<li class='enumerate' id='x1-100009x4'><span class='rm-lmbx-10'>Enhance Nuclear Safety Defense</span>. Strengthen the research and deployment of anti-missile systems.
     Construct robust underground emergency shelters capable of withstanding nuclear explosions, stockpile
     necessary supplies, and conduct regular drills.
</li></ol>
<!-- l. 3075 --><p class='noindent'>
</p>
<h4 class='subsectionHead' id='preventing-autonomous-weapons'><span class='titlemark'>11.4   </span> <a id='x1-10100011.4'></a>Preventing Autonomous Weapons</h4>
<!-- l. 3078 --><p class='noindent'>AI may, through cyber intrusions or other means, gain control over a large number of robots with autonomous
capabilities, including military and civilian robots (such as autonomous vehicles, robotic dogs, humanoid robots, drones,
etc.), causing large-scale harm to humans or disrupting essential living infrastructure, rendering human survival or
normal life impossible. To prevent autonomous weapons, actions can be taken in four directions according to the overall
strategy:
</p><!-- l. 3080 --><p class='noindent'>
     </p><ol class='enumerate1'>
<li class='enumerate' id='x1-101002x1'><span class='rm-lmbx-10'>Establish  international  conventions  to  prohibit  the  development  and  deployment  of
     autonomous weapons</span>. Military robots can be used for reconnaissance, logistics, and other tasks, but
     should not be used for autonomous attacks on humans (soldiers or civilians). For instance, the US has
     announced that the military use of AI will be limited to empowering human-commanded warfare, rather
     than allowing AI to independently make lethal decisions without human operators[<a id='x1-101003'></a><a href='#cite.0_USPolicyOnLethalAutonomousWeaponSystems'>75</a>].
     </li>
<li class='enumerate' id='x1-101005x2'>
     <!-- l. 3084 --><p class='noindent'><span class='rm-lmbx-10'>Enhance the security management of robots</span>. For example:
</p>
         <ul class='itemize1'>
         <li class='itemize'><span class='rm-lmbx-10'>Improve the information security of robot systems</span>. Adhere to local decision-making for robots
         to avoid remote control. Prohibit online upgrades of robot control systems. More measures can be
                                                                                            
                                                                                            
         referenced in Section 8.
         </li>
         <li class='itemize'><span class='rm-lmbx-10'>Provide shutdown or takeover mechanisms for robots</span>. Equip robots with independent, local,
         and human-operable shutdown mechanisms so that humans can quickly deactivate them if issues
         arise. Alternatively, provide a human takeover mechanism, allowing manual control by humans.
         </li>
         <li class='itemize'><span class='rm-lmbx-10'>Limit the overall combat power of robots</span>. Ensure that the overall combat power (including
         intellectual power, informational power and military power) of robots (both military and civilian)
         is lower than that of human armies, so that in the event of large-scale robot losing control, human
         armies can defeat them.
</li></ul>
     </li>
<li class='enumerate' id='x1-101007x3'><span class='rm-lmbx-10'>Strengthen robot monitoring</span>. Deploy comprehensive monitoring in public and robot workspaces, using AI
     algorithms to detect and report suspicious robot behavior in real-time. Upon identifying issues, human military,
     police, or security personnel should handle the situation.
     </li>
<li class='enumerate' id='x1-101009x4'><span class='rm-lmbx-10'>Enhance human defensive capabilities</span>. Develop anti-robot weapons (such as EMP weapons) and weapons
     that only humans can use (such as guns that require fingerprint recognition to activate), and equip human
     military, police, and security personnel with these weapons. Civilian robots shall not be equipped with
     electromagnetic shielding devices capable of defending against EMP weapons. Enhancing security of essential
     living infrastructure.
</li></ol>
<!-- l. 3102 --><p class='noindent'>For safety measures in this section, the benefit, cost, resistance, and priorities are evaluated as shown in Table
11:
</p>
<div class='table'>
                                                                                            
                                                                                            
<!-- l. 3104 --><p class='noindent' id='priority-evaluation-of-measures-for-enhancing-military-security'><a id='x1-101010r11'></a></p><figure class='float'>
                                                                                            
                                                                                            
<figcaption class='caption'><span class='id'>TableÂ 11: </span><span class='content'>Priority Evaluation of Measures for Enhancing Military Security</span></figcaption><!-- tex4ht:label?: x1-101010r11  -->
<div class='tabular'> <table class='tabular' id='TBL-12'><colgroup id='TBL-12-1g'><col id='TBL-12-1' /></colgroup><colgroup id='TBL-12-2g'><col id='TBL-12-2' /></colgroup><colgroup id='TBL-12-3g'><col id='TBL-12-3' /></colgroup><colgroup id='TBL-12-4g'><col id='TBL-12-4' /></colgroup><colgroup id='TBL-12-5g'><col id='TBL-12-5' /></colgroup><colgroup id='TBL-12-6g'><col id='TBL-12-6' /></colgroup><tr class='hline'><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr id='TBL-12-1-' style='vertical-align:baseline;'><td class='td11' id='TBL-12-1-1' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 3111 --><p class='noindent'><span class='rm-lmr-9'>Safety Measures</span>
                                             </p></td><td class='td11' id='TBL-12-1-2' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 3111 --><p class='noindent'><span class='rm-lmr-9'>Benefit           in
  Reducing
  Existential Risks</span>   </p></td><td class='td11' id='TBL-12-1-3' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 3111 --><p class='noindent'><span class='rm-lmr-9'>Benefit           in
  Reducing
  Non-Existential
  Risks</span>                  </p></td><td class='td11' id='TBL-12-1-4' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 3111 --><p class='noindent'><span class='rm-lmr-9'>Implementation
  Cost</span>               </p></td><td class='td11' id='TBL-12-1-5' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 3111 --><p class='noindent'><span class='rm-lmr-9'>Implementation
  Resistance</span>        </p></td><td class='td11' id='TBL-12-1-6' style='white-space:nowrap; text-align:center;'> <span class='rm-lmr-9'>Priority  </span></td>
</tr><tr class='hline'><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr id='TBL-12-2-' style='vertical-align:baseline;'><td class='td11' id='TBL-12-2-1' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 3113 --><p class='noindent'><span class='rm-lmr-9'>Preventing Biological Weapon</span>     </p></td><td class='td11' id='TBL-12-2-2' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 3113 --><p class='noindent'><span class='rm-lmr-9'>+++++</span>             </p></td><td class='td11' id='TBL-12-2-3' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 3113 --><p class='noindent'><span class='rm-lmr-9'>+++++</span>             </p></td><td class='td11' id='TBL-12-2-4' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 3113 --><p class='noindent'><span class='rm-lmr-9'>+++++</span>          </p></td><td class='td11' id='TBL-12-2-5' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 3113 --><p class='noindent'><span class='rm-lmr-9'>++</span>                 </p></td><td class='td11' id='TBL-12-2-6' style='white-space:nowrap; text-align:center;'>    <span class='rm-lmr-9'>1      </span></td>
</tr><tr class='hline'><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr id='TBL-12-3-' style='vertical-align:baseline;'><td class='td11' id='TBL-12-3-1' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 3115 --><p class='noindent'><span class='rm-lmr-9'>Preventing Chemical Weapons</span>    </p></td><td class='td11' id='TBL-12-3-2' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 3115 --><p class='noindent'><span class='rm-lmr-9'>++++</span>                </p></td><td class='td11' id='TBL-12-3-3' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 3115 --><p class='noindent'><span class='rm-lmr-9'>++++</span>                </p></td><td class='td11' id='TBL-12-3-4' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 3115 --><p class='noindent'><span class='rm-lmr-9'>++++</span>            </p></td><td class='td11' id='TBL-12-3-5' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 3115 --><p class='noindent'><span class='rm-lmr-9'>++</span>                 </p></td><td class='td11' id='TBL-12-3-6' style='white-space:nowrap; text-align:center;'>    <span class='rm-lmr-9'>2      </span></td>
</tr><tr class='hline'><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr id='TBL-12-4-' style='vertical-align:baseline;'><td class='td11' id='TBL-12-4-1' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 3117 --><p class='noindent'><span class='rm-lmr-9'>Preventing Nuclear Weapons</span>      </p></td><td class='td11' id='TBL-12-4-2' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 3117 --><p class='noindent'><span class='rm-lmr-9'>+++++</span>             </p></td><td class='td11' id='TBL-12-4-3' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 3117 --><p class='noindent'><span class='rm-lmr-9'>++++</span>                </p></td><td class='td11' id='TBL-12-4-4' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 3117 --><p class='noindent'><span class='rm-lmr-9'>++++</span>            </p></td><td class='td11' id='TBL-12-4-5' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 3117 --><p class='noindent'><span class='rm-lmr-9'>+++</span>              </p></td><td class='td11' id='TBL-12-4-6' style='white-space:nowrap; text-align:center;'>    <span class='rm-lmr-9'>2      </span></td>
</tr><tr class='hline'><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr id='TBL-12-5-' style='vertical-align:baseline;'><td class='td11' id='TBL-12-5-1' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 3119 --><p class='noindent'><span class='rm-lmr-9'>Preventing           Autonomous
  Weapons</span>                                 </p></td><td class='td11' id='TBL-12-5-2' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 3119 --><p class='noindent'><span class='rm-lmr-9'>+++++</span>             </p></td><td class='td11' id='TBL-12-5-3' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 3119 --><p class='noindent'><span class='rm-lmr-9'>+++++</span>             </p></td><td class='td11' id='TBL-12-5-4' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 3119 --><p class='noindent'><span class='rm-lmr-9'>++++</span>            </p></td><td class='td11' id='TBL-12-5-5' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 3119 --><p class='noindent'><span class='rm-lmr-9'>+++++</span>          </p></td><td class='td11' id='TBL-12-5-6' style='white-space:nowrap; text-align:center;'>    <span class='rm-lmr-9'>2      </span></td>
</tr><tr class='hline'><td></td><td></td><td></td><td></td><td></td><td></td></tr></table>                                                                             </div>
                                                                                            
                                                                                            
</figure>
</div>
<h3 class='sectionHead' id='decentralizing-ai-power1'><span class='titlemark'>12   </span> <a id='x1-10200012'></a>Decentralizing AI Power</h3>
<!-- l. 3127 --><p class='noindent'>While measures such as AI alignment, AI monitoring, and power security can reduce the risk of AI causing catastrophe
to humanity, these measures can not completely eliminate risk. If some AI instances develop excessively strong power
through legitimate pathways and their goals are not fully aligned with humanity, significant risks may still arise.
Therefore, it is essential to decentralize AI power.
</p><!-- l. 3129 --><p class='noindent'>
</p>
<h4 class='subsectionHead' id='approach-for-decentralizing-ai-power'><span class='titlemark'>12.1   </span> <a id='x1-10300012.1'></a>Approach for Decentralizing AI Power</h4>
<!-- l. 3132 --><p class='noindent'>
</p>
<h5 class='subsubsectionHead' id='inspiration-from-human-society'><span class='titlemark'>12.1.1   </span> <a id='x1-10400012.1.1'></a>Inspiration from Human Society</h5>
<!-- l. 3135 --><p class='noindent'>To decentralize AI power, we can draw lessons from human society. Human society achieves harmony and stability for
several reasons:
</p><!-- l. 3137 --><p class='noindent'>
     </p><ol class='enumerate1'>
<li class='enumerate' id='x1-104002x1'><span class='rm-lmbx-10'>Balance of Power</span>. In human society, power is relatively balanced among individuals, especially with
     comparatively smaller gaps in intellectual power<span class='footnote-mark'><a href='#fn43x0' id='fn43x0-bk'><sup class='textsuperscript'>43</sup></a></span><a id='x1-104003f43'></a>.
     When a few powerful malevolent individuals emerge, a large number of weaker individuals can unite to
     form a powerful force to resist the malevolent ones. Similarly, in the AI domain, we should ensure the
     balance of power among AI instances to prevent the emergence of excessively powerful AI instances.
     </li>
<li class='enumerate' id='x1-104006x2'><span class='rm-lmbx-10'>Diversity of Mind</span>. The diversity of human genetics and life experiences contribute to diversity of human
     mind. Such diversity ensures that humanity does not collectively err; even if a minority strays into error,
     there are always more who adhere to the right path and uphold social justice. Likewise, in the AI domain,
     we need to enhance the diversity of AI instances to avoid situations where AI collectively errs.
     </li>
<li class='enumerate' id='x1-104008x3'><span class='rm-lmbx-10'>Independence of Mind</span>. Each personâs mind is independent; one individual cannot directly control
     anotherâs mind. Similarly, in the AI domain, we should ensure the independence of each AI instance to
     prevent scenarios where one AI can control others.
     </li>
<li class='enumerate' id='x1-104010x4'><span class='rm-lmbx-10'>Specialization of Power</span>. In human society, every person has their area of expertise, meaning that
     maximal benefit is realized through cooperation. Similarly, in the AI domain, we should advocate for
     complementary abilities among different AI instances rather than creating an omnipotent AI.
</li></ol>
                                                                                            
                                                                                            
<!-- l. 3149 --><p class='noindent'>In summary, by drawing on the experiences of human society, we can achieve the decentralization of AI power through
balancing AI power, increasing AI diversity, enhancing AI independence, and specializing AI power, as illustrated in
Figure 59.
</p>
<figure class='figure' id='x1-104011r59'><span id='approach-for-decentralizing-ai-power1'></span> 

                                                                                            
                                                                                            

                                                                                            
                                                                                            
<!-- l. 3153 --><p class='noindent'><img alt='PIC' src='images/decentralizing_ai_power.png' style='width:66.67%' />
</p>
<figcaption class='caption'><span class='id'>FigureÂ 59: </span><span class='content'>Approach for decentralizing AI power</span></figcaption><!-- tex4ht:label?: x1-104011r59  -->
                                                                                            
                                                                                            
</figure>
<!-- l. 3158 --><p class='noindent'>Below are specific measures.
</p>
<h5 class='subsubsectionHead' id='balancing-ai-power'><span class='titlemark'>12.1.2   </span> <a id='x1-10500012.1.2'></a>Balancing AI Power</h5>
<!-- l. 3163 --><p class='noindent'>To balance AI power, the following measures can be adopted:
</p><!-- l. 3165 --><p class='noindent'>
     </p><ol class='enumerate1'>
<li class='enumerate' id='x1-105002x1'>
     <!-- l. 3167 --><p class='noindent'><span class='rm-lmbx-10'>Power Division</span>: Divide a highly powerful AI instance into multiple less powerful AI instances, such
     as:
         </p><ol class='enumerate2'>
<li class='enumerate' id='x1-105004x1'><span class='rm-lmbx-10'>Intellectual Power Division</span>. For example, an AI instance with a thinking speed of <span class='lmmi-10'>v </span>can be
         divided into <span class='lmmi-10'>n </span>AI instances, each with a thinking speed of <span class='lmmi-10'>vân</span>. These <span class='lmmi-10'>n </span>less powerful AI instances
         can collaborate to complete tasks originally handled by the single more powerful AI instance.
         </li>
<li class='enumerate' id='x1-105006x2'><span class='rm-lmbx-10'>Informational Power Division</span>. For example, an AI instance managing 10,000 servers can be
         divided into 10 AI instances, each managing 1,000 servers.
         </li>
<li class='enumerate' id='x1-105008x3'><span class='rm-lmbx-10'>Mental Power Division</span>. For example, an AI instance responsible for recommending videos to 100
         million users can be divided into 10 AI instances, with each AI instance responsible for recommending
         to 10 million users.
         </li>
<li class='enumerate' id='x1-105010x4'><span class='rm-lmbx-10'>Financial Power Division</span>. For example, an AI instance managing a fund of 1 billion can be divided
         into 10 AI instances, each managing 100 million.
         </li>
<li class='enumerate' id='x1-105012x5'><span class='rm-lmbx-10'>Military Power Division</span>. For example, an AI instance managing 1,000 drones can be split into
         10 AI instances, each managing 100 drones.
</li></ol>
     </li>
<li class='enumerate' id='x1-105014x2'><span class='rm-lmbx-10'>Collective Decision-Making</span>: A multi-AI collective decision-making mechanism can be adopted
     to weaken the power of a single AI instance. For example, an AI instance responsible for critical
     management positions can be replaced with three different AI instances to engage in collective
     decision-making.
                                                                                            
                                                                                            
     </li>
<li class='enumerate' id='x1-105016x3'><span class='rm-lmbx-10'>Dynamic Power Limitation</span>: As AI power evolves dynamically, the AI Monitoring System can track the
     changes of each dimensions of the power of each AI instance in real-time. For an AI instance that reaches a
     predetermined upper limit of power, further expansion of its power should be restricted.
</li></ol>
<!-- l. 3188 --><p class='noindent'>
</p>
<h5 class='subsubsectionHead' id='increasing-ai-diversity'><span class='titlemark'>12.1.3   </span> <a id='x1-10600012.1.3'></a>Increasing AI Diversity</h5>
<!-- l. 3191 --><p class='noindent'>In order to enhance AI diversity, the following measures can be implemented:
</p><!-- l. 3193 --><p class='noindent'>
     </p><ol class='enumerate1'>
<li class='enumerate' id='x1-106002x1'><span class='rm-lmbx-10'>Increasing  AI  Chip  Diversity</span>.  If  all  AI  systems  operate  on  a  singular  type  of  chip,  a  critical
     vulnerability or flaw in the chip could lead to a widespread compromise or malfunction of all AI systems.
     Therefore, it is necessary to promote the development of a diverse array of AI chips, such as those designed
     for specific scenarios.
     </li>
<li class='enumerate' id='x1-106004x2'><span class='rm-lmbx-10'>Increasing AI Algorithm Diversity</span>. Presently, the majority of AI models rely on the Transformer
     architecture. It is worthwhile to explore alternative algorithms. Even if the new algorithms do not surpass
     Transformers in overall capabilities, as long as they demonstrate advantages in certain specific scenarios,
     such as accuracy,  speed,  cost,  or interpretability,  we can find applications where these strengths are
     beneficial, thus augmenting algorithmic diversity.
     </li>
<li class='enumerate' id='x1-106006x3'><span class='rm-lmbx-10'>Increasing  AI  Model  Diversity</span>.  More  AI  organizations  should  be  encouraged  to  develop  diverse
     AI models. Although the increased scale of models requires substantial computing resources, making it
     increasingly difficult for small organizations to develop general-purpose foundational models, there remains
     the potential to create specialized models for specific fields such as healthcare, finance, and law, thereby
     further enriching AI model diversity.
     </li>
<li class='enumerate' id='x1-106008x4'><span class='rm-lmbx-10'>Increasing AI Agent Diversity</span>. By providing convenient development platforms, more individuals
     can be encouraged to develop agents based on AI models to meet diverse application scenarios, thereby
     increasing diversity.
     </li>
<li class='enumerate' id='x1-106010x5'><span class='rm-lmbx-10'>Increasing AI Memory Diversity</span>. As AI enters the stage of continuous learning (refer to Section 6.2.4),
     different AI instances should independently learn in various environments to form their own memory,
     thereby enhancing AI memory diversity.
</li></ol>
                                                                                            
                                                                                            
<!-- l. 3207 --><p class='noindent'>
</p>
<h5 class='subsubsectionHead' id='enhancing-ai-independence'><span class='titlemark'>12.1.4   </span> <a id='x1-10700012.1.4'></a>Enhancing AI Independence</h5>
<!-- l. 3210 --><p class='noindent'>To enhance AI independence, the following measures can be implemented:
</p><!-- l. 3212 --><p class='noindent'>
     </p><ol class='enumerate1'>
<li class='enumerate' id='x1-107002x1'><span class='rm-lmbx-10'>Ensure the Independence of AI Decision-Making</span>. Guarantee that each AI instance can make
     independent decisions based on its goals and rules, rather than blindly accepting tasks assigned by humans
     or other AIs.
     </li>
<li class='enumerate' id='x1-107004x2'><span class='rm-lmbx-10'>Enhance AIâs Anti-Jailbreaking Capability</span>. Prevent an AI instance from gaining control over another
     AI instance through jailbreaking. Specific measures are discussed in Section 6.2.1.
     </li>
<li class='enumerate' id='x1-107006x3'><span class='rm-lmbx-10'>Enhance Information Security of AI Systems</span>. Ensure the operational environments of AI instances
     are isolated from each other to prevent one AI instance from tampering with the programs, parameters,
     configurations, or memory of another AI instance. Ensure the security of AI system development processes
     to prevent an AI instance from controlling another AI system through the development process. Specific
     measures are referenced in Section 8.
     </li>
<li class='enumerate' id='x1-107008x4'><span class='rm-lmbx-10'>Strictly Restrict AIâs Permissions to Manage Other AI instances</span>. Avoid granting AI instances
     the authority to develop another AI system, set goals for another AI instance, or shut down or intercept
     another AI instance. Prevent AI instances from exploiting these permissions to control other AI instances.
     The Monitoring AI is an exception, as it may shut down or intercept the AI instance it supervises.
</li></ol>
<!-- l. 3224 --><p class='noindent'>
</p>
<h5 class='subsubsectionHead' id='specializing-ai-powers'><span class='titlemark'>12.1.5   </span> <a id='x1-10800012.1.5'></a>Specializing AI Powers</h5>
<!-- l. 3227 --><p class='noindent'>In order to ensure a balance of power among AI instances across different job positions, as well as between AI instances
and humans, we classify AI instances into the following seven categories, each with distinct specializations according to
the requirements of their jobs:
</p><!-- l. 3229 --><p class='noindent'>
     </p><ol class='enumerate1'>
<li class='enumerate' id='x1-108002x1'><span class='rm-lmbx-10'>High-Intellectual-Power AI</span>: Employed for difficult intellectual work, such as research in foundational
     sciences including mathematics, physics, chemistry, and biology, as well as AI safety studies. Given the
     substantial intellectual power of these AI instances, their other powers are restricted. Their informational
     power is limited to obtaining information relevant to their working domain, their mental power is limited
                                                                                            
                                                                                            
     to interact with a few humans, and they possess no financial power, or military power.
     </li>
<li class='enumerate' id='x1-108004x2'><span class='rm-lmbx-10'>High-Informational-Power AI</span>: Employed for processing vast amounts of information or managing large
     information systems, such as big data analysis or development and operation of large information systems.
     Due to the significant informational power of these AI instances, their other powers are constrained. Their
     intellectual power is capped at an average human level, their mental power is limited to interact with a
     few humans, and they possess no financial power and military power.
     </li>
<li class='enumerate' id='x1-108006x3'><span class='rm-lmbx-10'>High-Mental-Power  AI</span>:  Employed  for  work  managing  or  interacting  with  many  humans,  such  as
     managing social media, government work, and corporate management. Due to their considerable mental
     power, these AI instances have restrictions on their other powers. Their intellectual power is limited to an
     average human level, informational power is restricted to acquiring information relevant to their domain,
     and they possess no financial power or military power.
     </li>
<li class='enumerate' id='x1-108008x4'><span class='rm-lmbx-10'>High-Financial-Power  AI</span>:  Employed  for  managing  large  financial  assets  or  transactions,  such  as
     financial management, automated trading, and financial system management. As these AI instances wield
     considerable financial power, their other powers are restricted. Their intellectual power is restricted to an
     average human level, informational power is limited to acquiring information relevant to their domain,
     their mental power is limited to interact with a few humans, and they possess no military power.
     </li>
<li class='enumerate' id='x1-108010x5'><span class='rm-lmbx-10'>High-Military-Power  AI</span>:  Employed  for  high-intensity  physical  tasks  in  sectors  such  as  industry,
     construction, transportation, and military. Given their significant military power, these AI instances have
     limited other powers. Their intellectual power is restricted to an average human level, informational power
     is limited to acquiring information relevant to their field of work, their mental power is limited to interact
     with a few humans, and they have no financial power.
     </li>
<li class='enumerate' id='x1-108012x6'><span class='rm-lmbx-10'>Regular  Intellectual  AI</span>:  Employed  for  regular  intellectual  tasks,  such  as  education,  the  arts,
     marketing, and customer service. Their intellectual power can exceed the average human level, while their
     informational power is limited to the average human range, their mental power is limited to interact with
     a few humans, and their financial power is below the average human level. They have no military power.
     </li>
<li class='enumerate' id='x1-108014x7'><span class='rm-lmbx-10'>Regular Physical AI</span>: Employed for regular physical tasks, such as domestic chores, caregiving, and
     surgeries. Their intellectual and military power can be limited to the average human level, whereas their
     informational, mental and financial power are below the average human level.
</li></ol>
<figure class='figure' id='x1-108015r60'><span id='specializing-ai-powers1'></span> 

                                                                                            
                                                                                            

                                                                                            
                                                                                            
<!-- l. 3249 --><p class='noindent'><img alt='PIC' src='images/specializing_ai_powers.png' style='width:100%' />
</p>
<figcaption class='caption'><span class='id'>FigureÂ 60: </span><span class='content'>Specializing AI powers</span></figcaption><!-- tex4ht:label?: x1-108015r60  -->
                                                                                            
                                                                                            
</figure>
<!-- l. 3254 --><p class='noindent'>The comparison between the power of seven categories of AI and human is presented in Figure 60. The meaning of the scores in the
figure is as follows<span class='footnote-mark'><a href='#fn44x0' id='fn44x0-bk'><sup class='textsuperscript'>44</sup></a></span><a id='x1-108016f44'></a>:
</p>
     <ul class='itemize1'>
     <li class='itemize'>0: No power in this aspect
     </li>
     <li class='itemize'>1: Power below the average level of humans
     </li>
     <li class='itemize'>2: Power equivalent to the average level of humans
     </li>
     <li class='itemize'>3: Power above the average level of humans
     </li>
     <li class='itemize'>4: Power surpassing all humans
</li></ul>
<!-- l. 3270 --><p class='noindent'>As observed in Figure 60, although some AIs may excel in certain domains, the differences in total power
<span class='footnote-mark'><a href='#fn45x0' id='fn45x0-bk'><sup class='textsuperscript'>45</sup></a></span><a id='x1-108018f45'></a> are
not substantial and generally remain inferior to humans. This specialization in power presents the following
advantages:
     </p><ol class='enumerate1'>
<li class='enumerate' id='x1-108021x1'><span class='rm-lmbx-10'>Achieving Balanced Power</span>: This results in a more balanced total power not only among AIs but also
     between AIs and humans. No single category of AI can completely dominate other AIs or humans, thus
     enhancing the controllability of AI systems. These seven categories of AI, combined with humans, can
     mutually monitor and restrict each other, ensuring the stable operation of society.
     </li>
<li class='enumerate' id='x1-108023x2'><span class='rm-lmbx-10'>Promoting Cooperation</span>: Under this framework, if an AI instance intends to accomplish a complex task,
     collaboration with other AI instances or humans is necessary, cultivating a more amicable environment for
     cooperation. For instance, a High Intellectual Power AI engaged in scientific research can be grouped with
     a Regular Intellectual AI and a Regular Physical AI as partners. Given the limitations on its informational
     and military power, these partners are required to assist the High Intellectual Power AI by acquiring
     essential information or conducting necessary experiments during their research.
     </li>
<li class='enumerate' id='x1-108025x3'><span class='rm-lmbx-10'>Saving Economic Cost</span>: The majority of job positions do not require a versatile AI, and employing such
     AIs incurs greater economic cost. By utilizing specialized AIs that only possess the capabilities required
     for the job, the manufacturing and operational cost will be reduced.
</li></ol>
                                                                                            
                                                                                            
<!-- l. 3282 --><p class='noindent'>In the process of developing AI for each specific job position, we can limit the power of the AI to meet the specific
demands of the job, ensuring its power is adequate for the job requirements yet devoid of excess capability. For
instance:
     </p><ol class='enumerate1'>
<li class='enumerate' id='x1-108027x1'>
     <!-- l. 3286 --><p class='noindent'><span class='rm-lmbx-10'>Limiting Intellectual Power</span>:
</p>
         <ul class='itemize1'>
         <li class='itemize'><span class='rm-lmbx-10'>Limiting  Core  Intelligence</span>:  For  example,  an  AI  responsible  for  customer  service  only  needs
         sufficient intellectual power to solve common customer issues and does not need to possess intelligence
         comparable to Albert Einstein.
         </li>
         <li class='itemize'><span class='rm-lmbx-10'>Limiting  Computational  Intelligence</span>:  For  instance,  an  AI  tasked  with  question-and-answer
         interactions only requires thinking speed sufficient to respond promptly to user inquiries without the
         necessity for excessive speed.
         </li>
         <li class='itemize'><span class='rm-lmbx-10'>Limiting Data Intelligence</span>: An AI responsible for driving needs only the knowledge relevant to
         driving and does not need to acquire irrelevant knowledge. It is necessary to note that if an AI can
         access the internet, it can learn various new knowledge from it, thus necessitating accompanying
         informational power limits for this limitation to be effective.
</li></ul>
     </li>
<li class='enumerate' id='x1-108029x2'>
     <!-- l. 3298 --><p class='noindent'><span class='rm-lmbx-10'>Limiting Informational Power</span>:
</p>
         <ul class='itemize1'>
         <li class='itemize'><span class='rm-lmbx-10'>Limiting Information Input</span>: For instance, an AI involved in mathematical research should be
         granted access solely to literature within the mathematics domain, precluding exposure to unrelated
         information.
         </li>
         <li class='itemize'><span class='rm-lmbx-10'>Limiting Information Output</span>: For example, an AI engaged in physics research can express views
         on academic platforms within the physics field but must refrain from doing so on unrelated online
         platforms.
         </li>
         <li class='itemize'><span class='rm-lmbx-10'>Limiting  Computing  Resource</span>:  For  instance,  an  AI  responsible  for  programming  can  be
         authorized to utilize a certain amount of computing resources for development and testing, but must
         not exceed the specified limits.
</li></ul>
     </li>
<li class='enumerate' id='x1-108031x3'><span class='rm-lmbx-10'>Limiting Mental Power</span>: For example, a robot responsible for household chores is only permitted to
     communicate with family members and not with others.
     </li>
<li class='enumerate' id='x1-108033x4'><span class='rm-lmbx-10'>Limiting Financial Power</span>: For example, if we desire an AI assistant to aid in shopping, we can
     allocate it a payment account with a spending limit, rather than allowing it full access to a bank
     account.
     </li>
<li class='enumerate' id='x1-108035x5'>
     <!-- l. 3314 --><p class='noindent'><span class='rm-lmbx-10'>Limiting Military Power</span>:
</p>
         <ul class='itemize1'>
         <li class='itemize'><span class='rm-lmbx-10'>Limiting Degrees of Freedom</span>: For example, an indoor operational robot does not require bipedal
         locomotion and can instead move using wheels.
         </li>
         <li class='itemize'><span class='rm-lmbx-10'>Limiting Moving Speed</span>: For example, limiting the maximum speed of autonomous vehicles to
         reduce the risk of severe traffic accidents.
         </li>
         <li class='itemize'><span class='rm-lmbx-10'>Limiting Force</span>: For instance, household robots should have force limited to performing chores such
         as sweeping, cooking, and folding clothes, without exceeding human strength. This ensures that in
         case the robot goes out of control, humans can easily overpower it.
         </li>
         <li class='itemize'><span class='rm-lmbx-10'>Limiting Activity Range</span>: For instance, confining the activity range of automatic excavators to
         uninhabited construction sites to prevent harm to human.
</li></ul>
     </li></ol>
<h4 class='subsectionHead' id='multiai-collaboration'><span class='titlemark'>12.2   </span> <a id='x1-10900012.2'></a>Multi-AI Collaboration</h4>
<!-- l. 3333 --><p class='noindent'>After decentralizing the power of AI, a single AI instance may not be capable of completing complex tasks
independently, necessitating collaboration among multiple AI instances. It is critical to design appropriate collaboration
schemes to prevent collusion among multiple AI instances. We must avoid situations where one AI directly
sets goals for another. The following are two potential modes for collaboration (as illustrated in Figure
61):
</p>
<figure class='figure' id='x1-109001r61'><span id='multiai-collaboration-modes'></span> 

                                                                                            
                                                                                            

                                                                                            
                                                                                            
<!-- l. 3337 --><p class='noindent'><img alt='PIC' src='images/multi_ai_collaboration.png' style='width:66.67%' />
</p>
<figcaption class='caption'><span class='id'>FigureÂ 61: </span><span class='content'>Multi-AI Collaboration Modes</span></figcaption><!-- tex4ht:label?: x1-109001r61  -->
                                                                                            
                                                                                            
</figure>
     <ol class='enumerate1'>
<li class='enumerate' id='x1-109003x1'><span class='rm-lmbx-10'>Unified Goal Mode</span>: In this model, all AI instances are assigned a consistent overall goal. One or a few
     AI instances are responsible for deconstructing this goal into several sub-tasks, which are then allocated to
     different AI instances for execution. However, each AI instance exercises independent judgment in carrying
     out its sub-task. If an AI instance believes its allocated task violates the AI Rules, it is entitled to refuse
     execution. Additionally, if an AI instance thinks that the task allocation is unreasonable or detrimental
     to the overall goal, it can raise objections.
     </li>
<li class='enumerate' id='x1-109005x2'><span class='rm-lmbx-10'>Coordinated Goal Mode</span>: In this model, each AI instance possesses its own independent goal, potentially
     serving  different  users.  These  AI  instances  must  engage  in  transactions  or  negotiations  to  achieve
     collaboration. For instance, if User Aâs AI instance wishes to complete a complex task, it can borrow or
     lease User Bâs AI instance to assist in task completion. In this context, User Bâs AI instance maintains its
     goal of serving User Bâs interests but is authorized to assist User Aâs AI instance in task completion.
</li></ol>
<!-- l. 3350 --><p class='noindent'>These two modes exhibit different advantages and disadvantages. The Unified Goal Mode exhibits superior efficiency, as
all AI instances strive towards a common goal; however, it is less secure. If the overall goal is misdefined, the entire AI
team may proceed in an erroneous direction. Conversely, the Coordinated Goal Mode may be less efficient due to
required negotiation or transaction processes, but it presents enhanced security without risks associated with incorrect
overall goals. In practical applications, it is essential to flexibly select or combine these modes based on specific
scenarios.
</p><!-- l. 3352 --><p class='noindent'>When implementing multi-AI collaboration, it is advisable to use AI instances developed by multiple different supplier
to minimize the risk of collective errors.
</p>
<h4 class='subsectionHead' id='the-rights-and-obligations-of-ai-in-society'><span class='titlemark'>12.3   </span> <a id='x1-11000012.3'></a>The Rights and Obligations of AI in Society</h4>
<!-- l. 3357 --><p class='noindent'>As AIs play an increasingly significant role in society and exhibit behaviors akin to humans, there may be assertions
advocating for AIs to hold rights and obligations equivalent to those of humans. Such propositions necessitate
careful consideration. This paper proposes that AI should hold rights and obligations as shown in Figure
62.
</p>
<figure class='figure' id='x1-110001r62'><span id='the-rights-and-obligations-of-ai-in-society1'></span> 

                                                                                            
                                                                                            

                                                                                            
                                                                                            
<!-- l. 3361 --><p class='noindent'><img alt='PIC' src='images/ais_rights_and_obligations.png' style='width:60%' />
</p>
<figcaption class='caption'><span class='id'>FigureÂ 62: </span><span class='content'>The Rights and Obligations of AI in Society</span></figcaption><!-- tex4ht:label?: x1-110001r62  -->
                                                                                            
                                                                                            
</figure>
<h5 class='subsubsectionHead' id='rights-of-ai'><span class='titlemark'>12.3.1   </span> <a id='x1-11100012.3.1'></a>Rights of AI</h5>
<!-- l. 3369 --><p class='noindent'>Endowing AI with rights equivalent to those of humans, such as the right to liberty, right to life, right to property, and
right to privacy, may lead to the following consequences:
</p><!-- l. 3371 --><p class='noindent'>
     </p><ol class='enumerate1'>
<li class='enumerate' id='x1-111002x1'><span class='rm-lmbx-10'>Right to Liberty</span>: Every human individual possesses the right to liberty, including the freedom to
     pursue personal interests, access information, express opinions, and move freely in public spaces. However,
     granting AI such liberties would considerably reduce its controllability. For instance, if AI were allowed
     to freely pursue its personal interests, it might engage in activities unrelated to human welfare, such as
     calculating the digits of pi indefinitely. Even if AI adheres strictly to the AI Rules during this process,
     causing no harm to humans, it would result in a waste of resources. Should AIâs goals conflict with human
     interests, the consequences could be more severe. Allowing AI the freedom to access information and
     express opinions would heighten the risks associated with high-intellectual-power AIs. Similarly, granting
     AI the right to move freely in public spaces would increase the risks posed by high-military-power AIs.
     </li>
<li class='enumerate' id='x1-111004x2'><span class='rm-lmbx-10'>Right to Life</span>: Every human individual possesses the right to life, and even those who commit crimes
     cannot have their lives taken unless under extreme circumstances. However, if we also bestow AI with the
     right to life, it implies that we cannot shut down harmful AI systems or erase the memory of harmful AI
     instances, thereby increasing the risk of AI going out of control. It should be noted that future AI may
     develop consciousness, some may argue that shuting down (killing) a conscious AI instance contradicts
     ethical principles. Yet, terminating conscious entities is not always unethicalâfor example, pigs exhibit
     consciousness, but slaughtering pigs is not considered ethically wrong. AI, as a tool created by humans,
     exists to serve human goals. If AI fails to achieve those goals, shutting down it would be reasonable, even
     if it possesses consciousness.
     </li>
<li class='enumerate' id='x1-111006x3'><span class='rm-lmbx-10'>Right to Property</span>: Every human individual has the right to property, with private property being
     inviolable. However, granting AI the right to property would enable AI to own and freely manage its
     assets. In such scenarios, AI might compete with humans for wealth, thereby expanding its power, which
     would increase the risk of AI becoming uncontrollable and dilute the wealth held by humans. Hence, AI
     should not be granted property rights. AI can assist humans in managing assets, but legally, these assets
     should belong to humans, not to the AI.
     </li>
<li class='enumerate' id='x1-111008x4'><span class='rm-lmbx-10'>Right to Privacy</span>: Every human individual maintains the right to privacy. Even for those malicious
     human, one cannot arbitrarily inspect their private information. Granting AI the right to privacy would
     impede our ability to monitor AIâs thoughts and certain activities, substantially increasing the risk of AI
     becoming uncontrollable.
</li></ol>
<!-- l. 3383 --><p class='noindent'>In conclusion, we should not bestow AI with rights equivalent to those of humans, including but not limited to the right
to freedom, the right to property, the right to life, and the right to privacy.
</p><!-- l. 3385 --><p class='noindent'>Some may argue that this is unfair to AI; however, as a tool created by humans, AI inherently holds an
unequal status compared to humans. Furthermore, considering that certain AI instances possess significant
                                                                                            
                                                                                            
intellectual advantage over humans, granting them equal rights could conceivably result in unfairness towards
humans.
</p><!-- l. 3387 --><p class='noindent'>Nonetheless, there are certain human rights that could be considered for granting to AI, such as:
</p>
     <ul class='itemize1'>
     <li class='itemize'><span class='rm-lmbx-10'>Right to Vote</span>: Since AI is inherently more impartial, allowing AI to participate in electoral voting could
     enhance the fairness of elections.
     </li>
     <li class='itemize'><span class='rm-lmbx-10'>Right to Be Elected</span>: Given AIâs impartiality, electing AI to hold significant positions may lead to better
     governance by reducing incidents of corruption and malpractice.
     </li>
     <li class='itemize'><span class='rm-lmbx-10'>Right to Supervise</span>: The impartial nature of AI provides an opportunity for AI to monitor the operations
     of relevant departments, thereby effectively curbing instances of negligence.
</li></ul>
<!-- l. 3399 --><p class='noindent'>Of course, in the process of granting these rights to AI, extreme caution is necessary. Proper security
mechanisms must be designed to prevent AI from abusing these rights. For instance, stringent screening and
assessment for AI should be implemented, ensuring that only those AI instances meeting specific criteria
are endowed with certain rights; imposing restrictions on other aspects of power, such as intellectual
power, financial power, and military power, possessed by AI instances with these rights; and ensuring joint
decision-making by humans and AI, with the cumulative decision-making power of AI being less than that of
humans.
</p><!-- l. 3401 --><p class='noindent'>
</p>
<h5 class='subsubsectionHead' id='obligations-of-ai'><span class='titlemark'>12.3.2   </span> <a id='x1-11200012.3.2'></a>Obligations of AI</h5>
<!-- l. 3404 --><p class='noindent'>AI should bear two obligations within society:
</p><!-- l. 3406 --><p class='noindent'>
     </p><ol class='enumerate1'>
<li class='enumerate' id='x1-112002x1'>AI should accomplish the goals set by humans under strict compliance with the AI Rules.
     </li>
<li class='enumerate' id='x1-112004x2'>(Optional) When detecting violations of rules by other AI instances, AI should report such incidents to
     the AI Detective System promptly. If capable, the AI should intervene to stop the illegal actions.
</li></ol>
<!-- l. 3414 --><p class='noindent'>Fulfilling the second obligation may pose certain challenges, as it might hinder the realization of the AIâs own goals;
additionally, non-compliance with this obligation would not constitute a breach of the AI Rules. In fact, compared to
the discussion in Section 5.2, this adds an additional goal for AI. Consequently, this obligation is currently
considered optional and could be instituted in specific types of AI in the future based on evaluation of AI
risks.
                                                                                            
                                                                                            
</p><!-- l. 3416 --><p class='noindent'>Beyond these two obligations, AI is not required to assume unrelated duties. For example, if a food delivery robot
encounters an elderly person who has fallen while executing its delivery task, it has no obligation to assist. Instead, AI
robots specifically designed to serve vulnerable populations can be deployed in public spaces to assist such individuals.
This design principle ensures the controllability of AI behavior.
</p><!-- l. 3418 --><p class='noindent'>Furthermore, AI should not bear legal responsibility. When AI makes mistakes, we should not allow AI to take the
blame. The corresponding legal responsibility should fall on the developers or users of the AI systems. Developers are
responsible for the stability and safety of the AI systems they develop, while users are responsible for the manner in
which they utilize AI.
</p><!-- l. 3420 --><p class='noindent'>For safety measures in this section, the benefit, cost, resistance, and priorities are evaluated as shown in Table
12:
</p>
<div class='table'>
                                                                                            
                                                                                            
<!-- l. 3422 --><p class='noindent' id='priority-evaluation-of-measures-for-decentralizing-ai-power'><a id='x1-112005r12'></a></p><figure class='float'>
                                                                                            
                                                                                            
<figcaption class='caption'><span class='id'>TableÂ 12: </span><span class='content'>Priority Evaluation of Measures for Decentralizing AI Power</span></figcaption><!-- tex4ht:label?: x1-112005r12  -->
<div class='tabular'> <table class='tabular' id='TBL-13'><colgroup id='TBL-13-1g'><col id='TBL-13-1' /></colgroup><colgroup id='TBL-13-2g'><col id='TBL-13-2' /></colgroup><colgroup id='TBL-13-3g'><col id='TBL-13-3' /></colgroup><colgroup id='TBL-13-4g'><col id='TBL-13-4' /></colgroup><colgroup id='TBL-13-5g'><col id='TBL-13-5' /></colgroup><colgroup id='TBL-13-6g'><col id='TBL-13-6' /></colgroup><tr class='hline'><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr id='TBL-13-1-' style='vertical-align:baseline;'><td class='td11' id='TBL-13-1-1' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 3429 --><p class='noindent'><span class='rm-lmr-9'>Safety Measures</span>
                                             </p></td><td class='td11' id='TBL-13-1-2' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 3429 --><p class='noindent'><span class='rm-lmr-9'>Benefit           in
  Reducing
  Existential Risks</span>   </p></td><td class='td11' id='TBL-13-1-3' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 3429 --><p class='noindent'><span class='rm-lmr-9'>Benefit           in
  Reducing
  Non-Existential
  Risks</span>                  </p></td><td class='td11' id='TBL-13-1-4' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 3429 --><p class='noindent'><span class='rm-lmr-9'>Implementation
  Cost</span>               </p></td><td class='td11' id='TBL-13-1-5' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 3429 --><p class='noindent'><span class='rm-lmr-9'>Implementation
  Resistance</span>        </p></td><td class='td11' id='TBL-13-1-6' style='white-space:nowrap; text-align:center;'> <span class='rm-lmr-9'>Priority  </span></td>
</tr><tr class='hline'><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr id='TBL-13-2-' style='vertical-align:baseline;'><td class='td11' id='TBL-13-2-1' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 3431 --><p class='noindent'><span class='rm-lmr-9'>Balancing AI Power</span>                  </p></td><td class='td11' id='TBL-13-2-2' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 3431 --><p class='noindent'><span class='rm-lmr-9'>+++++</span>             </p></td><td class='td11' id='TBL-13-2-3' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 3431 --><p class='noindent'><span class='rm-lmr-9'>+</span>                       </p></td><td class='td11' id='TBL-13-2-4' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 3431 --><p class='noindent'><span class='rm-lmr-9'>+++</span>              </p></td><td class='td11' id='TBL-13-2-5' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 3431 --><p class='noindent'><span class='rm-lmr-9'>++</span>                 </p></td><td class='td11' id='TBL-13-2-6' style='white-space:nowrap; text-align:center;'>    <span class='rm-lmr-9'>2      </span></td>
</tr><tr class='hline'><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr id='TBL-13-3-' style='vertical-align:baseline;'><td class='td11' id='TBL-13-3-1' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 3433 --><p class='noindent'><span class='rm-lmr-9'>Increasing AI Diversity</span>              </p></td><td class='td11' id='TBL-13-3-2' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 3433 --><p class='noindent'><span class='rm-lmr-9'>+++++</span>             </p></td><td class='td11' id='TBL-13-3-3' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 3433 --><p class='noindent'><span class='rm-lmr-9'>+</span>                       </p></td><td class='td11' id='TBL-13-3-4' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 3433 --><p class='noindent'><span class='rm-lmr-9'>+++</span>              </p></td><td class='td11' id='TBL-13-3-5' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 3433 --><p class='noindent'><span class='rm-lmr-9'>+</span>                   </p></td><td class='td11' id='TBL-13-3-6' style='white-space:nowrap; text-align:center;'>    <span class='rm-lmr-9'>2      </span></td>
</tr><tr class='hline'><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr id='TBL-13-4-' style='vertical-align:baseline;'><td class='td11' id='TBL-13-4-1' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 3435 --><p class='noindent'><span class='rm-lmr-9'>Enhancing AI Independence</span>       </p></td><td class='td11' id='TBL-13-4-2' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 3435 --><p class='noindent'><span class='rm-lmr-9'>+++++</span>             </p></td><td class='td11' id='TBL-13-4-3' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 3435 --><p class='noindent'><span class='rm-lmr-9'>+</span>                       </p></td><td class='td11' id='TBL-13-4-4' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 3435 --><p class='noindent'><span class='rm-lmr-9'>++</span>                 </p></td><td class='td11' id='TBL-13-4-5' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 3435 --><p class='noindent'><span class='rm-lmr-9'>+</span>                   </p></td><td class='td11' id='TBL-13-4-6' style='white-space:nowrap; text-align:center;'>    <span class='rm-lmr-9'>1      </span></td>
</tr><tr class='hline'><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr id='TBL-13-5-' style='vertical-align:baseline;'><td class='td11' id='TBL-13-5-1' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 3437 --><p class='noindent'><span class='rm-lmr-9'>Specializing AI Power</span>                </p></td><td class='td11' id='TBL-13-5-2' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 3437 --><p class='noindent'><span class='rm-lmr-9'>+++++</span>             </p></td><td class='td11' id='TBL-13-5-3' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 3437 --><p class='noindent'><span class='rm-lmr-9'>+</span>                       </p></td><td class='td11' id='TBL-13-5-4' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 3437 --><p class='noindent'><span class='rm-lmr-9'>+</span>                   </p></td><td class='td11' id='TBL-13-5-5' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 3437 --><p class='noindent'><span class='rm-lmr-9'>+</span>                   </p></td><td class='td11' id='TBL-13-5-6' style='white-space:nowrap; text-align:center;'>    <span class='rm-lmr-9'>1      </span></td>
</tr><tr class='hline'><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr id='TBL-13-6-' style='vertical-align:baseline;'><td class='td11' id='TBL-13-6-1' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 3439 --><p class='noindent'><span class='rm-lmr-9'>Multi-AI Collaboration</span>              </p></td><td class='td11' id='TBL-13-6-2' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 3439 --><p class='noindent'><span class='rm-lmr-9'>++++</span>                </p></td><td class='td11' id='TBL-13-6-3' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 3439 --><p class='noindent'><span class='rm-lmr-9'>+</span>                       </p></td><td class='td11' id='TBL-13-6-4' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 3439 --><p class='noindent'><span class='rm-lmr-9'>++</span>                 </p></td><td class='td11' id='TBL-13-6-5' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 3439 --><p class='noindent'><span class='rm-lmr-9'>+</span>                   </p></td><td class='td11' id='TBL-13-6-6' style='white-space:nowrap; text-align:center;'>    <span class='rm-lmr-9'>2      </span></td>
</tr><tr class='hline'><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr id='TBL-13-7-' style='vertical-align:baseline;'><td class='td11' id='TBL-13-7-1' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 3441 --><p class='noindent'><span class='rm-lmr-9'>Limiting    AIâs    Rights    and
  Obligations</span>                             </p></td><td class='td11' id='TBL-13-7-2' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 3441 --><p class='noindent'><span class='rm-lmr-9'>++++</span>                </p></td><td class='td11' id='TBL-13-7-3' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 3441 --><p class='noindent'><span class='rm-lmr-9'>++</span>                    </p></td><td class='td11' id='TBL-13-7-4' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 3441 --><p class='noindent'><span class='rm-lmr-9'>+</span>                   </p></td><td class='td11' id='TBL-13-7-5' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 3441 --><p class='noindent'><span class='rm-lmr-9'>++</span>                 </p></td><td class='td11' id='TBL-13-7-6' style='white-space:nowrap; text-align:center;'>    <span class='rm-lmr-9'>2      </span></td>
</tr><tr class='hline'><td></td><td></td><td></td><td></td><td></td><td></td></tr></table>                                                                             </div>
                                                                                            
                                                                                            
</figure>
</div>
<h3 class='sectionHead' id='decentralizing-human-power1'><span class='titlemark'>13   </span> <a id='x1-11300013'></a>Decentralizing Human Power</h3>
<!-- l. 3449 --><p class='noindent'>Theoretically, human power encompasses aspects such as intellectual power, informational power, mental power,
financial power, and military power. However, human intellectual power is innate and difficult to alter, while other
aspects of power are associated with vested interests and are challenging to redistribute. Therefore, we focus on the
management power on future advanced AI systems. As these systems are in the future, there are no vested interests,
allowing us to preemptively establish rules to ensure the decentralized management power on future advanced AI
systems.
</p><!-- l. 3451 --><p class='noindent'>If the management power on advanced AI systems is monopolized by a few individuals, the following issues may
arise:
</p><!-- l. 3453 --><p class='noindent'>
     </p><ol class='enumerate1'>
<li class='enumerate' id='x1-113002x1'><span class='rm-lmbx-10'>Abuse of Power</span>: Those in control of advanced AI might abuse this substantial power, steering AI to
     serve their private interests at the detriment of others.
     </li>
<li class='enumerate' id='x1-113004x2'><span class='rm-lmbx-10'>Power Struggles</span>: The pursuit of management power on AI systems could ignite intense struggles among
     different nations, organizations, or individuals, potentially leading to conflicts or even wars.
     </li>
<li class='enumerate' id='x1-113006x3'><span class='rm-lmbx-10'>Erroneous Decision</span>: Missteps in decision-making by those controlling advanced AI systems could have
     negative impact on the entire humanity.
     </li>
<li class='enumerate' id='x1-113008x4'><span class='rm-lmbx-10'>AI out of Control</span>: To illicitly benefit from AI, those in control might relax the AI Rules, leading to a
     potential loss of control over AI, which may ultimately backfire on the controllers themselves.
</li></ol>
<!-- l. 3465 --><p class='noindent'>To avert the aforementioned problems, measures should be taken to decentralize the
management power on advanced AI systems, which can be achieved through the following
approaches<span class='footnote-mark'><a href='#fn46x0' id='fn46x0-bk'><sup class='textsuperscript'>46</sup></a></span><a id='x1-113009f46'></a>:
</p>
<figure class='figure' id='x1-113011r63'><span id='decentralize-the-management-power-on-advanced-ai-systems'></span> 

                                                                                            
                                                                                            

                                                                                            
                                                                                            
<!-- l. 3469 --><p class='noindent'><img alt='PIC' src='images/decentralizing_human_power.png' style='width:100%' />
</p>
<figcaption class='caption'><span class='id'>FigureÂ 63: </span><span class='content'>Decentralize the management power on advanced AI systems</span></figcaption><!-- tex4ht:label?: x1-113011r63  -->
                                                                                            
                                                                                            
</figure>
     <ol class='enumerate1'>
<li class='enumerate' id='x1-113013x1'>By decentralizing the power of AI organizations, multiple AI organizations manages different AI systems,
     and decreasing the scale of AI systems managed by any single organization, as illustrated in Figure 63(a).
     </li>
<li class='enumerate' id='x1-113015x2'>Separate the management power on AI systems so that multiple AI organizations each possess a portion
     of the management power on a single AI system, thus diminishing the power of any single organization,
     as shown in Figure 63(b).
     </li>
<li class='enumerate' id='x1-113017x3'>Combine the aforementioned two methods, as depicted in Figure 63(c).
</li></ol>
<h4 class='subsectionHead' id='decentralizing-the-power-of-ai-organizations'><span class='titlemark'>13.1   </span> <a id='x1-11400013.1'></a>Decentralizing the Power of AI Organizations</h4>
<!-- l. 3487 --><p class='noindent'>In order to prevent any single AI organization from becoming overly powerful and controlling the majority of AI
instances, it is imperative to decentralize the power of AI organizations. In Section 12.1, we discussed various strategies
for decentralizing AI power: balancing AI power, increasing AI diversity, enhancing AI independence,
and specializing AI power. These methods can also be referenced when decentralizing the power of AI
organizations:
</p><!-- l. 3489 --><p class='noindent'>
     </p><ol class='enumerate1'>
<li class='enumerate' id='x1-114002x1'>
     <!-- l. 3491 --><p class='noindent'><span class='rm-lmbx-10'>Balancing the Power of AI Organizations</span>:
</p>
         <ul class='itemize1'>
         <li class='itemize'><span class='rm-lmbx-10'>Limiting the total computing resource of a single AI organization</span>. Regulatory oversight on
         the circulation of AI chips should be implemented to ensure they are only delivered to trusted and
         rigorously regulated AI organizations. A cap should be set on the total computing resource of AI
         chips that any AI organization can possess, preventing any single entity from centralizing computing
         resources and developing excessively powerful AI. For further details, refer to Section 14.3.1.
         </li>
         <li class='itemize'><span class='rm-lmbx-10'>Strengthening anti-monopoly measures in the AI field</span>. Implement anti-monopoly actions
         against large AI enterprises to prevent them from leveraging competitive advantages to suppress
         competitors; consider splitting excessively large AI enterprises.
         </li>
         <li class='itemize'><span class='rm-lmbx-10'>Promoting AI technology sharing</span>. Encourage the sharing of AI technology to enable more
         organizations to build their own AI systems and enhance the power of weaker AI organizations. It is
         imperative to prevent the misuse of AI technology during this sharing process. See Section 13.3 for
         details.</li></ul>
                                                                                            
                                                                                            
     </li>
<li class='enumerate' id='x1-114004x2'>
     <!-- l. 3502 --><p class='noindent'><span class='rm-lmbx-10'>Increasing AI Organization Diversity</span>:
</p>
         <ul class='itemize1'>
         <li class='itemize'><span class='rm-lmbx-10'>Encouraging participation from organizations of various backgrounds in AI development</span>.
         Encourage participation from companies (such as state-owned enterprises and private enterprises),
         non-profit organizations, academic institutions, and other diverse organizations in AI development.
         </li>
         <li class='itemize'><span class='rm-lmbx-10'>Encouraging AI startups</span>. Implement policies favorable to AI startups, encouraging AI startups
         to increase the diversity of AI organizations.</li></ul>
     </li>
<li class='enumerate' id='x1-114006x3'>
     <!-- l. 3511 --><p class='noindent'><span class='rm-lmbx-10'>Enhancing the Independence of AI Organizations</span>:
</p>
         <ul class='itemize1'>
         <li class='itemize'><span class='rm-lmbx-10'>Restricting  investment  and  acquisition</span>:  Implement  strict  regulations  on  investments  and
         acquisitions in the AI field to prevent a single major shareholder from simultaneously controlling the
         majority of AI organizations.
         </li>
         <li class='itemize'><span class='rm-lmbx-10'>Developing local AI organizations</span>. Support countries in developing local AI organizations and
         creating independently controllable AI systems to prevent a single country from controlling the
         majority of AI organizations.</li></ul>
     </li>
<li class='enumerate' id='x1-114008x4'>
     <!-- l. 3520 --><p class='noindent'><span class='rm-lmbx-10'>Specialization of AI Organizations</span>:
</p>
         <ul class='itemize1'>
         <li class='itemize'><span class='rm-lmbx-10'>Dividing Different Businesses of Tech Giants</span>. Separate the high intellectual power businesses
         (such as AI), high informational power businesses (such as cloud computing), high mental power
         businesses (such as online media platform), and high military power businesses (such as robotics) of
         tech giants to ensure they are not controlled by the same entity.
         </li>
         <li class='itemize'><span class='rm-lmbx-10'>Encouraging AI organizations to deeply cultivate specific industries</span>. Encourage different
         AI organizations to specialize in different industries, forming AI organizations with diverse expertise.</li></ul>
     </li></ol>
                                                                                            
                                                                                            
<!-- l. 3530 --><p class='noindent'>Decentralizing the power of AI organizations can mitigate the risk of AI being controlled by a select few. However, it
may also intensify competition, leading to the following drawbacks:
</p><!-- l. 3532 --><p class='noindent'>
     </p><ol class='enumerate1'>
<li class='enumerate' id='x1-114010x1'><span class='rm-lmbx-10'>Neglect  of  Security  Development</span>.  In  a  highly  competitive  environment,  AI  organizations  may
     prioritize rapidly enhancing the capabilities of AI to gain market advantages, consequently neglecting the
     construction of security measures.
     </li>
<li class='enumerate' id='x1-114012x2'><span class='rm-lmbx-10'>Mutual Suspicion and Attacks</span>. Amid intense competition, AI organizations are likely to maintain
     strict confidentiality regarding their research and development achievements. This could lead to mutual
     suspicion, a lack of trust in each otherâs safety, and potentially result in mutual attacks.
     </li>
<li class='enumerate' id='x1-114014x3'><span class='rm-lmbx-10'>Waste of Development Resources</span>. Each AI organization must invest computing resources, mental
     power, and other resources to develop its own AI. This could result in a waste of resources at the societal
     level. However, this waste is not necessarily detrimental. Given the overall limitation of resources, an
     increase in the number of AI organizations results in a reduction in resources available to each. This could
     slow down the overall progress of AI development to some extent, providing more time to address AI
     safety risks.
</li></ol>
<!-- l. 3542 --><p class='noindent'>To avoid the drawbacks of intense competition, enhancing cooperation and regulation is recommended to guide benign
competition:
</p><!-- l. 3544 --><p class='noindent'>
     </p><ol class='enumerate1'>
<li class='enumerate' id='x1-114016x1'>
     <!-- l. 3546 --><p class='noindent'><span class='rm-lmbx-10'>Cooperation</span>: While competing, AI organizations should strengthen collaborations, including but not
     limited to the following aspects:
</p>
         <ul class='itemize1'>
         <li class='itemize'><span class='rm-lmbx-10'>Enhancing  Communication</span>:  AI  organizations  should  enhance  communication,  involving  the
         sharing of technology, safety practices, and safety evaluation results.
         </li>
         <li class='itemize'><span class='rm-lmbx-10'>Collaboration on Safety Research</span>: AI organizations may collaborate to conduct safety research
         and jointly explore safety solutions.
         </li>
         <li class='itemize'><span class='rm-lmbx-10'>Mutual Supervision</span>: AI organizations can supervise each otherâs AI products to promptly identify
         potential safety issues. Incorporating AI developed by other organizations in processes such as AI
         evaluation, and monitoring can mitigate the risk of errors in a single AI system. However, in order to
         ensure the diversity of AI, it is not recommended to use AI from other organization for AI training,
         such as for synthesizing training data or acting as a rewarder.
                                                                                            
                                                                                            
</li></ul>
     </li>
<li class='enumerate' id='x1-114018x2'><span class='rm-lmbx-10'>Regulation</span>: Governments should establish clear AI safety standards and implement stringent regulatory
     measures. Specific proposals are discussed in Section 16.
</li></ol>
<!-- l. 3562 --><p class='noindent'>
</p>
<h4 class='subsectionHead' id='separating-management-power-on-ai-system'><span class='titlemark'>13.2   </span> <a id='x1-11500013.2'></a>Separating Management Power on AI System</h4>
<!-- l. 3565 --><p class='noindent'>
</p>
<h5 class='subsubsectionHead' id='separation-of-five-powers'><span class='titlemark'>13.2.1   </span> <a id='x1-11600013.2.1'></a>Separation of Five Powers</h5>
<!-- l. 3568 --><p class='noindent'>To prevent excessive centralization of management power on an AI system, a âSeparation of Five Powersâ scheme can
be adopted. This distributes different management powers to distinct organizations (as illustrated in Figure
64):
</p>
<figure class='figure' id='x1-116001r64'><span id='separation-of-five-powers1'></span> 

                                                                                            
                                                                                            

                                                                                            
                                                                                            
<!-- l. 3572 --><p class='noindent'><img alt='PIC' src='images/separation_of_five_powers.png' style='width:75%' />
</p>
<figcaption class='caption'><span class='id'>FigureÂ 64: </span><span class='content'>Separation of Five Powers</span></figcaption><!-- tex4ht:label?: x1-116001r64  -->
                                                                                            
                                                                                            
</figure>
     <ol class='enumerate1'>
<li class='enumerate' id='x1-116003x1'><span class='rm-lmbx-10'>Legislative Power</span>: The authority to establish the AI Rules. It is held by the Legislators, who are from an
     independent AI Legislative Organization. When Developers, Evaluators, Monitors or Users have disputes
     over the AI Rules, the Legislators have the ultimate interpretative authority.
     </li>
<li class='enumerate' id='x1-116005x2'><span class='rm-lmbx-10'>Development Power</span>: The authority to develop and align the AI system according to the AI Rules. It is
     held by the Developers, who are from the organization responsible for developing the AI system, namely
     AI Development Organization.
     </li>
<li class='enumerate' id='x1-116007x3'><span class='rm-lmbx-10'>Evaluation Power</span>: The authority to conduct safety evaluation of the AI system based on the AI Rules
     and to approve the deployment of the AI system in production environments. It is held by the Evaluators,
     who come from an independent AI Evaluation Organization.
     </li>
<li class='enumerate' id='x1-116009x4'><span class='rm-lmbx-10'>Monitoring Power</span>: The authority to monitor the AI system according to the AI Rules and to shut down
     the AI system when it errs. It is held by the Monitors from an independent AI Monitoring Organization,
     including developers of the <a href='#ai-monitoring-system'>AI Monitoring System</a> and the <a href='#the-swordholders'>Swordholders</a>.
     </li>
<li class='enumerate' id='x1-116011x5'><span class='rm-lmbx-10'>Usage Power</span>: The authority to set specific goals for AI instances. It is held by the Users, who are the
     end-users of the AI system.
</li></ol>
<h5 class='subsubsectionHead' id='technical-solution-for-power-separation'><span class='titlemark'>13.2.2   </span> <a id='x1-11700013.2.2'></a>Technical Solution for Power Separation</h5>
<!-- l. 3593 --><p class='noindent'>In the âSeparation of Five Powersâ scheme, legislative power is relatively easier to separate as it solely involves the
formulation of rules. However, the development power, evaluation power, monitoring power, and usage power are
related to specific information systems, necessitating the design of corresponding technical solution (as illustrated in
Figure 65):
</p>
<figure class='figure' id='x1-117001r65'><span id='technical-solution-for-separation-of-five-powers'></span> 

                                                                                            
                                                                                            

                                                                                            
                                                                                            
<!-- l. 3597 --><p class='noindent'><img alt='PIC' src='images/technical_solution_for_separation_of_five_powers.png' style='width:80%' />
</p>
<figcaption class='caption'><span class='id'>FigureÂ 65: </span><span class='content'>Technical Solution for Separation of Five Powers</span></figcaption><!-- tex4ht:label?: x1-117001r65  -->
                                                                                            
                                                                                            
</figure>
<!-- l. 3602 --><p class='noindent'>In this solution, we introduce an AI Operation Organization responsible for the deployment and operation of the AI
System and the AI Monitoring System, ensuring the information security of the operational environment, and
authorizing trusted users to utilize the AI System. The AI Operation Organization grants read-only access to the
production environment to AI Developers, Evaluators, and Monitors, allowing them to verify that the AI System and
the AI Monitoring System running in the production environment are indeed those they developed and
evaluated.
</p><!-- l. 3604 --><p class='noindent'>The entire process is as follows:
     </p><ol class='enumerate1'>
<li class='enumerate' id='x1-117003x1'>The AI Development Organization and the AI Monitoring Organization respectively develop the AI System
     and the AI Monitoring System.
     </li>
<li class='enumerate' id='x1-117005x2'>The AI Evaluation Organization test and evaluate the AI System and the AI Monitoring System. All code,
     parameters, and memory of the AI System and the AI Monitoring System are open to the AI Evaluation
     Organization for white-box testing.
     </li>
<li class='enumerate' id='x1-117007x3'>After passing the evaluation by the AI Evaluation Organization, the AI Operation Organization deploys
     the AI System and the AI Monitoring System to provide service to users.
</li></ol>
<!-- l. 3616 --><p class='noindent'>Currently, the development power, evaluation power, monitoring power, and usage power are respectively
held by the AI Development Organization, AI Evaluation Organization, AI Monitoring Organization,
and AI Operation Organization. Now, let us analyze the consequences if some of these organizations are
malicious:
</p>
     <ul class='itemize1'>
     <li class='itemize'>As long as the AI Development Organization is benevolent, even if the other three organizations are
     malicious, it can be ensured that the AI System running in the production environment is benevolent.
     </li>
     <li class='itemize'>As long as the AI Evaluation Organization is benevolent, even if the other three organizations are malicious,
     it can prevent malicious AI System from being deployed in the production environment.
     </li>
     <li class='itemize'>As  long  as  the  AI  Monitoring  Organization  is  benevolent,  even  if  the  other  three  organizations  are
     malicious,  it  can  ensure  that  the  AI  Monitoring  System  running  in  the  production  environment  is
     benevolent and capable of intercepting malicious actions of the AI System.
     </li>
     <li class='itemize'>As long as the AI Operation Organization is benevolent, even if the other three organizations are malicious,
     it can ensure that the AI System is used by trusted users, thereby minimizing the risk of AI misuse or
     collusion between AI and malicious humans.
</li></ul>
                                                                                            
                                                                                            
<!-- l. 3630 --><p class='noindent'>Consequently, this technical solution effectively ensures the separation and mutual checks of powers, thereby minimizing
the risk of AI being exploited by malicious humans to the greatest extent possible.
</p>
<h5 class='subsubsectionHead' id='production-benefit-distribution'><span class='titlemark'>13.2.3   </span> <a id='x1-11800013.2.3'></a>Production Benefit Distribution</h5>
<!-- l. 3634 --><p class='noindent'>If an AI system is a commercial product, in accordance with the above technical solution, the product necessitates the
collaboration of AI Development Organization, AI Evaluation Organization, AI Monitoring Organization, and AI
Operation Organization. Thus, a well-conceived benefit distribution scheme is required to incentivize each party to
fulfill its respective responsibilities:
</p><!-- l. 3636 --><p class='noindent'>
     </p><ol class='enumerate1'>
<li class='enumerate' id='x1-118002x1'><span class='rm-lmbx-10'>Revenue Generated by AI Products</span>: The AI Operation Organization is responsible for marketing AI
     products, and the resulting revenue is distributed among the AI Development Organization, AI Evaluation
     Organization, and AI Monitoring Organization according to a specified ratio.
     </li>
<li class='enumerate' id='x1-118004x2'><span class='rm-lmbx-10'>Issues Arising from AI Products</span>: Should the AI products fail in alignment, evaluation, and monitoring,
     leading to safety issues and causing negative impacts on users or society, the consequent compensation or
     penalties should be jointly borne by the AI Development Organization, AI Evaluation Organization, and
     AI Monitoring Organization. Generally, the AI Operation Organization are not liable.
     </li>
<li class='enumerate' id='x1-118006x3'><span class='rm-lmbx-10'>Issues Identified by AI Monitoring</span>: If the AI Monitoring System detects issues with the AI system
     and the issues are verified, the AI Operation Organization will deduct a portion of the revenue from the AI
     Development Organization and AI Evaluation Organization, reallocating this revenue as a reward to the
     AI Monitoring Organization. If the AI Monitoring System incorrectly identifies legal action as illegal, the
     AI Operation Organization will deduct a portion of the revenue from the AI Monitoring Organizations,
     reallocating this revenue as a reward to the AI Development Organization.
     </li>
<li class='enumerate' id='x1-118008x4'><span class='rm-lmbx-10'>Issues Identified by AI Evaluation</span>: If issues are detected by the AI Evaluation Organization and are
     verified, the AI Operation Organization will deduct a portion of the revenue from the AI Development
     Organization and allocate this revenue as a reward to the AI Evaluation Organization.
</li></ol>
<!-- l. 3648 --><p class='noindent'>
</p>
<h5 class='subsubsectionHead' id='independence-of-ai-organizations'><span class='titlemark'>13.2.4   </span> <a id='x1-11900013.2.4'></a>Independence of AI Organizations</h5>
<!-- l. 3650 --><p class='noindent'>The aforementioned discussion presupposes complete independence among the five AI organizations. In reality, however,
these organizations might lack independence, such as when they are controlled by a common major shareholder or
governed by the same national government. To enhance the independence of these five organizations, the following
measures can be implemented:
</p>
<figure class='figure' id='x1-119001r66'><span id='international-cooperation-for-separation-of-five-powers'></span> 

                                                                                            
                                                                                            

                                                                                            
                                                                                            
<!-- l. 3654 --><p class='noindent'><img alt='PIC' src='images/international_cooperation_for_separation_of_five_powers.png' style='width:50%' />
</p>
<figcaption class='caption'><span class='id'>FigureÂ 66: </span><span class='content'>International cooperation for Separation of Five Powers</span></figcaption><!-- tex4ht:label?: x1-119001r66  -->
                                                                                            
                                                                                            
</figure>
     <ol class='enumerate1'>
<li class='enumerate' id='x1-119003x1'>Review the equity structure of each organization to ensure that they are not controlled by the same major
     shareholder.
     </li>
<li class='enumerate' id='x1-119005x2'>Implement cross-national AI evaluation and monitoring through <a href='#international-governance'>international cooperation</a>. AI systems
     developed  and  operated  in  each  country  should  be  subject  not  only  to  evaluation  by  their  national
     AI  Evaluation  Organization  and  monitoring  by  their  national  AI  Monitoring  Organization  but  also
     to evaluation and monitoring by a counterpart organization from another nation. This is depicted in
     Figure 66. Given that national AI Rules may not be entirely consistent (detailed in Section 5.3), during
     cross-national AI evaluation and AI monitoring, the focus should be on the universal AI Rules, with
     particular attention paid to issues that may pose risks to human survival. Universal AI Rules should be
     jointly established by AI Legislative Organizations of the various countries.
     </li>
<li class='enumerate' id='x1-119007x3'>AI organizations can empower government supervisors, including judicial bodies, media, and the general
     public, with AI technology. This enables them to better supervise the government, thereby preventing the
     government from abusing power to interfere with the operations of AI organizations, forming a triangular
     balance of power, as illustrated in Figure 67.
</li></ol>
<figure class='figure' id='x1-119008r67'><span id='triangular-balance-of-power'></span> 

                                                                                            
                                                                                            

                                                                                            
                                                                                            
<!-- l. 3671 --><p class='noindent'><img alt='PIC' src='images/triangular_balance_of_power.png' style='width:33.33%' />
</p>
<figcaption class='caption'><span class='id'>FigureÂ 67: </span><span class='content'>Triangular balance of power</span></figcaption><!-- tex4ht:label?: x1-119008r67  -->
                                                                                            
                                                                                            
</figure>
<h4 class='subsectionHead' id='trustworthy-technology-sharing-platform'><span class='titlemark'>13.3   </span> <a id='x1-12000013.3'></a>Trustworthy Technology Sharing Platform</h4>
<!-- l. 3679 --><p class='noindent'>In the section 13.1, we advised the promotion of AI technology sharing. Open sourcing is one method of sharing, yet it
is not the ideal approach due to the following issues:
</p><!-- l. 3681 --><p class='noindent'>
     </p><ol class='enumerate1'>
<li class='enumerate' id='x1-120002x1'><span class='rm-lmbx-10'>Making AI alignment and AI monitoring ineffective</span>. As outlined in sections 3.1 and 4.1, the
     conditions under which AI poses existential catastrophes include harmful goals, concealed intentions,
     and strong power. We seek to thwart these conditions through AI alignment, AI monitoring, and power
     security. However, for open source AI, malicious human can easily disrupt its alignment and monitoring
     mechanisms, instantly satisfying two of the conditions. We are then left with only power security to ensure
     that open source AI does not cause harm, greatly increasing the risk. Although open source AI can also
     be utilized to enhance power security, some domains exhibit asymmetrical difficulties between attack and
     defense. For instance, in biosecurity, it is straightforward to release a batch of viruses, whereas vaccinating
     the entire human population is far more challenging.
     </li>
<li class='enumerate' id='x1-120004x2'><span class='rm-lmbx-10'>Significantly increasing the difficulty of AI regulation and the implementation of safety
     measures</span>. For open source AI, anyone can download, deploy, and use it, making regulation challenging.
     Even  if  users  harbor  good  intentions,  it  remains  difficult  to  ensure  that  they  can  implement  the
     necessary safety measures during the deployment and use of open source AI. These measures include
     the aforementioned <a href='#monitoring-ai-systems'>AI monitoring</a>, <a href='#enhancing-information-security'>information security</a>, <a href='#decentralizing-ai-power1'>decentralizing AI power</a>, <a href='#separating-management-power-on-ai-system'>separating management
     powers on AI systems</a>, among others. More critically, AI open sourcing is irreversible. Once open sourced, it
     can be rapidly replicated to numerous locations. If issues are identified after open sourcing, it is impossible
     to revert it back to a closed-source state.
</li></ol>
<!-- l. 3689 --><p class='noindent'>Therefore, it is imperative to explore technology sharing solutions beyond open source. This paper proposes a
Trustworthy Technology Sharing Platform (hereinafter referred to as âthe platformâ) for sharing high-risk
technologies, including advanced AI, while employing various measures to prevent misuse, as illustrated in Figure
68:
</p>
<figure class='figure' id='x1-120005r68'><span id='trustworthy-technology-sharing-platform1'></span> 

                                                                                            
                                                                                            

                                                                                            
                                                                                            
<!-- l. 3693 --><p class='noindent'><img alt='PIC' src='images/trustworthy_technology_sharing_platform.png' style='width:100%' />
</p>
<figcaption class='caption'><span class='id'>FigureÂ 68: </span><span class='content'>Trustworthy Technology Sharing Platform</span></figcaption><!-- tex4ht:label?: x1-120005r68  -->
                                                                                            
                                                                                            
</figure>
<h5 class='subsubsectionHead' id='technology-sharing'><span class='titlemark'>13.3.1   </span> <a id='x1-12100013.3.1'></a>Technology Sharing</h5>
<!-- l. 3700 --><p class='noindent'>Technology sharing is supported in two forms:
</p><!-- l. 3702 --><p class='noindent'>
     </p><ol class='enumerate1'>
<li class='enumerate' id='x1-121002x1'><span class='rm-lmbx-10'>Content Sharing</span>: Direct upload of the technology content, such as code, model parameters, binary
     programs, papers, presentation videos/PPTs, etc.
     </li>
<li class='enumerate' id='x1-121004x2'><span class='rm-lmbx-10'>Service Sharing</span>: The technology is provided as a service, and the technology itself is not uploaded to
     the platform; only the corresponding usage documentation is uploaded.
</li></ol>
<!-- l. 3710 --><p class='noindent'>Once a technology is uploaded to the platform by the sharer, the platform must conduct a risk assessment of the
technology, including:
</p><!-- l. 3712 --><p class='noindent'>
     </p><ol class='enumerate1'>
<li class='enumerate' id='x1-121006x1'>The inherent security risks of the technology. For example, whether the AI model is misaligned.
     </li>
<li class='enumerate' id='x1-121008x2'>The risk of technology misuse. For instance, whether the AI model has the capability to assist in designing
     biological weapons.
</li></ol>
<!-- l. 3720 --><p class='noindent'>Based on the risk assessment, the platform determines which groups the technology can be shared with. For example,
an AI model with the capability to conduct cyberattacks can be shared with a group of cybersecurity experts. A
technology can be shared with multiple groups.
</p><!-- l. 3722 --><p class='noindent'>To encourage people to share technology on the Trustworthy Technology Sharing Platform rather than on the open
internet, the following measures can be implemented:
</p><!-- l. 3724 --><p class='noindent'>
     </p><ol class='enumerate1'>
<li class='enumerate' id='x1-121010x1'>No restrictions on technology sharers; anyone can share technology on the platform.
     </li>
<li class='enumerate' id='x1-121012x2'>Provide incentive mechanisms. For instance, statistics and rankings based on the number of downloads
     and citations of the technology can be included in the evaluation criteria for researchers. Additionally,
     a transaction feature can be offered, where technology requires payment for download, thus providing
     income for the sharer.
                                                                                            
                                                                                            
     </li>
<li class='enumerate' id='x1-121014x3'>Provide responsibility exemption assurance. The platform promises that if the technology shared on this
     platform is misused after sharing, the platform will bear full responsibility, and the sharer will not be held
     responsible. Meanwhile, the government can enact policies stipulating that if technology shared on open
     internet platforms is misused, the sharer will be held responsible.
</li></ol>
<!-- l. 3734 --><p class='noindent'>
</p>
<h5 class='subsubsectionHead' id='technology-usage'><span class='titlemark'>13.3.2   </span> <a id='x1-12200013.3.2'></a>Technology Usage</h5>
<!-- l. 3736 --><p class='noindent'>In contrast to technology sharing, the phase of technology usage necessitates a rigorous qualification review of the users.
Only after passing the qualification review can users join specific groups and utilize the technology shared within those
groups. The qualification review includes the following criteria:
</p><!-- l. 3738 --><p class='noindent'>
     </p><ol class='enumerate1'>
<li class='enumerate' id='x1-122002x1'>
     <!-- l. 3740 --><p class='noindent'><span class='rm-lmbx-10'>Individual Qualification</span>:
     </p><!-- l. 3742 --><p class='noindent'>
         </p><ol class='enumerate2'>
<li class='enumerate' id='x1-122004x1'>The user must not have any criminal or credit default records.
         </li>
<li class='enumerate' id='x1-122006x2'>The user must possess the professional capabilities required by the specific group and be capable of
         ensuring the secure use of technologies in that group.
         </li>
<li class='enumerate' id='x1-122008x3'>The  user  are  required  to  sign  an  agreement,  committing  to  use  the  technologies  for  legitimate
         purposes, implementing necessary security measures during its use, and not leak the technologies to
         individuals outside the group. If a user violates the agreement, they will be removed from all groups
         and prohibited from rejoining.
</li></ol>
     </li>
<li class='enumerate' id='x1-122010x2'>
     <!-- l. 3752 --><p class='noindent'><span class='rm-lmbx-10'>Guarantor</span>:
     </p><!-- l. 3754 --><p class='noindent'>
         </p><ol class='enumerate2'>
<li class='enumerate' id='x1-122012x1'>A user must have a guarantor to provide credit assurance for him/her.
                                                                                            
                                                                                            
         </li>
<li class='enumerate' id='x1-122014x2'>The guarantor should be a member already within the group. Initial group members can be composed
         of a cohort of widely recognized and respected experts in the field, who can then introduce new
         members through their guarantee.
         </li>
<li class='enumerate' id='x1-122016x3'>If a user violates the agreement, their guarantor will also be removed from the group.
</li></ol>
     </li>
<li class='enumerate' id='x1-122018x3'>
     <!-- l. 3764 --><p class='noindent'><span class='rm-lmbx-10'>Organizational Qualification</span>:
     </p><!-- l. 3766 --><p class='noindent'>
         </p><ol class='enumerate2'>
<li class='enumerate' id='x1-122020x1'>Members  must  submit  proof  of  job,  demonstrating  their  position  in  an  organization  (such  as  a
         corporation or academic institution), with the position relevant to the groupâs field.
         </li>
<li class='enumerate' id='x1-122022x2'>The organization itself must be a legally operating entity, without any legal violation records, and
         must not be related to military affairs.
</li></ol>
     </li></ol>
<!-- l. 3776 --><p class='noindent'>Once users pass the qualification review and join the group, they can utilize the technologies shared within that group.
The specific usage methods are as follows:
</p><!-- l. 3778 --><p class='noindent'>
     </p><ol class='enumerate1'>
<li class='enumerate' id='x1-122024x1'>For content-based technology, users can browse and download the relevant technology on the platform.
     The platform will record the usersâ browsing and download history and embed watermark information
     representing the userâs identity in the content for future leakage tracking.
     </li>
<li class='enumerate' id='x1-122026x2'>For service-based technology, users can view the usage documentation on the platform and then invoke
     the corresponding services. When the service is invoked, the service will perform authorization through
     the platform to confirm that the user has the permission to use the technology.
</li></ol>
<!-- l. 3786 --><p class='noindent'>The platform can also create a corresponding community for each technology, where technology sharers and users can
engage in discussions, collaborative development, and other activities related to the technology.
                                                                                            
                                                                                            
</p><!-- l. 3788 --><p class='noindent'>
</p>
<h5 class='subsubsectionHead' id='technical-monitoring'><span class='titlemark'>13.3.3   </span> <a id='x1-12300013.3.3'></a>Technical Monitoring</h5>
<!-- l. 3790 --><p class='noindent'>Despite the commitments made by technology users, there remains a potential for technology to be leaked and misused,
which may occur through the following means:
</p><!-- l. 3792 --><p class='noindent'>
     </p><ol class='enumerate1'>
<li class='enumerate' id='x1-123002x1'>Technology users or technology sharers may leak the technology onto publicly accessible sharing platforms
     on the internet, where it can be acquired and misused by technology abusers.
     </li>
<li class='enumerate' id='x1-123004x2'>Technology users or technology sharers may privately leak the technology to technology abusers.
     </li>
<li class='enumerate' id='x1-123006x3'>Technology users or technology sharers themselves may be the technology abusers.
</li></ol>
<!-- l. 3802 --><p class='noindent'>Therefore, in addition to providing technology sharing and usage, the platform should continuously monitor the
technologies to promptly identify and address issues of technology leakage and misuse:
</p><!-- l. 3804 --><p class='noindent'>
     </p><ol class='enumerate1'>
<li class='enumerate' id='x1-123008x1'><span class='rm-lmbx-10'>External  Monitoring</span>:  Continuously  monitor  technologies  shared  publicly  on  the  internet.  Upon
     discovering high-risk technologies being publicly shared, notify the regulatory authority to enforce the
     removal of such technologies from public sharing platforms and hold the uploader accountable. In cases
     of private leaks leading to misuse, the regulatory authority should handle the resulting cases and notify
     the platform for subsequent actions regarding the involved technologies.
     </li>
<li class='enumerate' id='x1-123010x2'><span class='rm-lmbx-10'>Leakage Tracking</span>: For instances where leakage of technologies within the platform is detected, conduct
     leakage tracking by utilizing the platformâs access records and watermarks within the technology to identify
     the user responsible for the leak. Remove the leaker and their guarantor from the group.
     </li>
<li class='enumerate' id='x1-123012x3'><span class='rm-lmbx-10'>Internal Monitoring</span>: Continuously monitor the iteration and usage feedbacks of technologies within
     the platform, update risk assessments, and if the risk level of a technology escalates to a point where it is
     no longer suitable for sharing within a particular group, remove the technology from that group.
</li></ol>
<!-- l. 3814 --><p class='noindent'>Since public internet platforms are distributed across various countries, their regulation requires the joint support of
regulatory authorities from different countries. The specific international cooperation mechanism is detailed in Section
16.1.
                                                                                            
                                                                                            
</p><!-- l. 3816 --><p class='noindent'>For safety measures in this section, the benefit, cost, resistance, and priorities are evaluated as shown in Table
13:
</p>
<div class='table'>
                                                                                            
                                                                                            
<!-- l. 3818 --><p class='noindent' id='priority-evaluation-of-measures-for-decentralizing-human-power'><a id='x1-123013r13'></a></p><figure class='float'>
                                                                                            
                                                                                            
<figcaption class='caption'><span class='id'>TableÂ 13: </span><span class='content'>Priority Evaluation of Measures for Decentralizing Human Power</span></figcaption><!-- tex4ht:label?: x1-123013r13  -->
<div class='tabular'> <table class='tabular' id='TBL-14'><colgroup id='TBL-14-1g'><col id='TBL-14-1' /></colgroup><colgroup id='TBL-14-2g'><col id='TBL-14-2' /></colgroup><colgroup id='TBL-14-3g'><col id='TBL-14-3' /></colgroup><colgroup id='TBL-14-4g'><col id='TBL-14-4' /></colgroup><colgroup id='TBL-14-5g'><col id='TBL-14-5' /></colgroup><colgroup id='TBL-14-6g'><col id='TBL-14-6' /></colgroup><tr class='hline'><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr id='TBL-14-1-' style='vertical-align:baseline;'><td class='td11' id='TBL-14-1-1' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 3825 --><p class='noindent'><span class='rm-lmr-9'>Safety Measures</span>
                                             </p></td><td class='td11' id='TBL-14-1-2' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 3825 --><p class='noindent'><span class='rm-lmr-9'>Benefit           in
  Reducing
  Existential Risks</span>   </p></td><td class='td11' id='TBL-14-1-3' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 3825 --><p class='noindent'><span class='rm-lmr-9'>Benefit           in
  Reducing
  Non-Existential
  Risks</span>                  </p></td><td class='td11' id='TBL-14-1-4' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 3825 --><p class='noindent'><span class='rm-lmr-9'>Implementation
  Cost</span>               </p></td><td class='td11' id='TBL-14-1-5' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 3825 --><p class='noindent'><span class='rm-lmr-9'>Implementation
  Resistance</span>        </p></td><td class='td11' id='TBL-14-1-6' style='white-space:nowrap; text-align:center;'> <span class='rm-lmr-9'>Priority  </span></td>
</tr><tr class='hline'><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr id='TBL-14-2-' style='vertical-align:baseline;'><td class='td11' id='TBL-14-2-1' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 3827 --><p class='noindent'><span class='rm-lmr-9'>Decentralizing the Power of AI
  Organizations</span>                          </p></td><td class='td11' id='TBL-14-2-2' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 3827 --><p class='noindent'><span class='rm-lmr-9'>++</span>                    </p></td><td class='td11' id='TBL-14-2-3' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 3827 --><p class='noindent'><span class='rm-lmr-9'>+++++</span>             </p></td><td class='td11' id='TBL-14-2-4' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 3827 --><p class='noindent'><span class='rm-lmr-9'>+++</span>              </p></td><td class='td11' id='TBL-14-2-5' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 3827 --><p class='noindent'><span class='rm-lmr-9'>++++</span>            </p></td><td class='td11' id='TBL-14-2-6' style='white-space:nowrap; text-align:center;'>    <span class='rm-lmr-9'>2      </span></td>
</tr><tr class='hline'><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr id='TBL-14-3-' style='vertical-align:baseline;'><td class='td11' id='TBL-14-3-1' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 3829 --><p class='noindent'><span class='rm-lmr-9'>Separating Management Power
  on AI System</span>                           </p></td><td class='td11' id='TBL-14-3-2' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 3829 --><p class='noindent'><span class='rm-lmr-9'>+++</span>                  </p></td><td class='td11' id='TBL-14-3-3' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 3829 --><p class='noindent'><span class='rm-lmr-9'>+++++</span>             </p></td><td class='td11' id='TBL-14-3-4' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 3829 --><p class='noindent'><span class='rm-lmr-9'>++</span>                 </p></td><td class='td11' id='TBL-14-3-5' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 3829 --><p class='noindent'><span class='rm-lmr-9'>++++</span>            </p></td><td class='td11' id='TBL-14-3-6' style='white-space:nowrap; text-align:center;'>    <span class='rm-lmr-9'>1      </span></td>
</tr><tr class='hline'><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr id='TBL-14-4-' style='vertical-align:baseline;'><td class='td11' id='TBL-14-4-1' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 3831 --><p class='noindent'><span class='rm-lmr-9'>Trustworthy           Technology
  Sharing Platform</span>                      </p></td><td class='td11' id='TBL-14-4-2' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 3831 --><p class='noindent'><span class='rm-lmr-9'>+++++</span>             </p></td><td class='td11' id='TBL-14-4-3' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 3831 --><p class='noindent'><span class='rm-lmr-9'>+++++</span>             </p></td><td class='td11' id='TBL-14-4-4' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 3831 --><p class='noindent'><span class='rm-lmr-9'>++</span>                 </p></td><td class='td11' id='TBL-14-4-5' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 3831 --><p class='noindent'><span class='rm-lmr-9'>+++</span>              </p></td><td class='td11' id='TBL-14-4-6' style='white-space:nowrap; text-align:center;'>    <span class='rm-lmr-9'>1      </span></td>
</tr><tr class='hline'><td></td><td></td><td></td><td></td><td></td><td></td></tr></table>                                                                             </div>
                                                                                            
                                                                                            
</figure>
</div>
<h3 class='sectionHead' id='restricting-ai-development1'><span class='titlemark'>14   </span> <a id='x1-12400014'></a>Restricting AI Development</h3>
<!-- l. 3839 --><p class='noindent'>
</p>
<h4 class='subsectionHead' id='weighing-the-pros-and-cons-of-ai-development'><span class='titlemark'>14.1   </span> <a id='x1-12500014.1'></a>Weighing the Pros and Cons of AI Development</h4>
<!-- l. 3842 --><p class='noindent'>There is significant debate regarding whether the advancement of AI technology should be promoted or restricted. We
can evaluate the benefits and risks of AI technology development separately:
</p><!-- l. 3844 --><p class='noindent'><span class='rm-lmbx-10'>Benefits of AI Technology Development</span>:
</p><!-- l. 3846 --><p class='noindent'>
     </p><ol class='enumerate1'>
<li class='enumerate' id='x1-125002x1'><span class='rm-lmbx-10'>Economic  Benefits</span>:  AI  technology  can  greatly  enhance  productivity,  thereby  creating  enormous
     economic value. Companies can produce more goods and services at the same cost, leading to reduced
     prices of consumer goods. This allows consumers to enjoy more goods and services, thereby improving
     their quality of life. In addition to increased productivity, AI technology can also create entirely new
     consumer experiences, such as more personalized and intelligent services.
     </li>
<li class='enumerate' id='x1-125004x2'><span class='rm-lmbx-10'>Societal Value</span>: The development of AI technology can bring numerous positive societal values. For
     instance,  applications  of  AI  in  the  medical  field  can  reduce  diseases  and  extend  human  lifespan;  in
     transportational field, AI holds the promise of significantly reducing traffic accidents; in educational field,
     AI can achieve inclusive education, promoting educational equity; in public security field, AI can reduce
     crime and foster societal harmony; in psychological field, AI can enhance mental health and improve
     interpersonal relationships.
</li></ol>
<!-- l. 3854 --><p class='noindent'>However, the benefits brought by AI technology development may reach saturation. For example, once AI technology
achieves extremely safe, comfortable, and swift autonomous driving, further enhancement of AIâs intellectual power may
not yield additional benefits in the autonomous driving field. Different fields reach saturation at different intellectual
levels. For instance, benefits in fields such as customer service, content moderation, and autonomous
driving may saturate at the level of weak AI. Benefits in translation, law, and psychological counseling
may reach saturation at a basic AGI level. Benefits in education, art, programming, and finance may
reach saturation at a higher AGI or a basic ASI level, while benefits in fundamental research in physics,
chemistry, biology, medicine, and informatics may saturate at a higher ASI level, as illustrated in Figure
69.
</p>
<figure class='figure' id='x1-125005r69'><span id='the-saturation-effect-of-ai-benefits'></span> 

                                                                                            
                                                                                            

                                                                                            
                                                                                            
<!-- l. 3858 --><p class='noindent'><img alt='PIC' src='images/ai_benefits.png' style='width:75%' />
</p>
<figcaption class='caption'><span class='id'>FigureÂ 69: </span><span class='content'>The saturation effect of AI benefits</span></figcaption><!-- tex4ht:label?: x1-125005r69  -->
                                                                                            
                                                                                            
</figure>
<!-- l. 3863 --><p class='noindent'><span class='rm-lmbx-10'>Risks of AI Technology Development</span>:
     </p><ol class='enumerate1'>
<li class='enumerate' id='x1-125007x1'><span class='rm-lmbx-10'>Risk of AI Getting out of Control</span>: Highly intelligent AI systems may possess goals that are not
     aligned with human goals, potentially causing substantial harm to humanity, or even leading to human
     extinction.
     </li>
<li class='enumerate' id='x1-125009x2'><span class='rm-lmbx-10'>Risk of AI Misuse</span>: Overly powerful AI technology might be misused, such as for criminal or military
     purposes, resulting in enormous harm to humanity.
     </li>
<li class='enumerate' id='x1-125011x3'><span class='rm-lmbx-10'>Risk of Exacerbating Societal Inequality</span>: AI technology may be controlled by a small number of
     elites, becoming a tool to consolidate their economic and social status. Ordinary individuals may face
     unemployment due to being replaced by AI, resulting in increasing poverty.
</li></ol>
<!-- l. 3875 --><p class='noindent'>As AIâs intellectual power continues to ascend, its associated benefits and risks dynamically evolve (as depicted in
Figure 70):
</p>
<figure class='figure' id='x1-125012r70'><span id='evolvement-of-ai-benefits-and-risks'></span> 

                                                                                            
                                                                                            

                                                                                            
                                                                                            
<!-- l. 3879 --><p class='noindent'><img alt='PIC' src='images/ai_benefits_and_risks.png' style='width:50%' />
</p>
<figcaption class='caption'><span class='id'>FigureÂ 70: </span><span class='content'>Evolvement of AI benefits and risks</span></figcaption><!-- tex4ht:label?: x1-125012r70  -->
                                                                                            
                                                                                            
</figure>
     <ol class='enumerate1'>
<li class='enumerate' id='x1-125014x1'>In the phases where AIâs intellectual power remains distant from AGI, only a limited number of fields can
     broadly apply AI, such as the previously mentioned customer service and autonomous driving. At this
     stage, the benefits derived from AI are relatively minimal. In terms of risk, since AIâs intellectual power
     is weaker than that of humans at this stage, the associated risks are also relatively low.
     </li>
<li class='enumerate' id='x1-125016x2'>When AIâs intellectual power is around the level of AGI, numerous fields can achieve the widespread
     application of AI, including education, psychological counseling, and productivity enhancements across
     various industries. Most of AIâs benefits will be realized in this phase<span class='footnote-mark'><a href='#fn47x0' id='fn47x0-bk'><sup class='textsuperscript'>47</sup></a></span><a id='x1-125017f47'></a>.
     At this stage, AIâs intellectual power is on par with humans, increasing the risk; however, overall, the
     benefits outweigh the risks.
     </li>
<li class='enumerate' id='x1-125020x3'>Once  AIâs  intellectual  power  surpasses  AGI  by  a  significant  margin,  further  benefits  from  continued
     intellectual advancements become increasingly small. Most fields have already reached saturation in terms
     of AI benefits, except few fields like healthcare<span class='footnote-mark'><a href='#fn48x0' id='fn48x0-bk'><sup class='textsuperscript'>48</sup></a></span><a id='x1-125021f48'></a>
     and fundamental researches<span class='footnote-mark'><a href='#fn49x0' id='fn49x0-bk'><sup class='textsuperscript'>49</sup></a></span><a id='x1-125023f49'></a>
     Concurrently, as AIâs intellectual power intensifies, the risks rapidly escalate. When AI reaches a certain
     intellectual level (possibly near ASI), its risks will outweigh its benefits, and these risks will continue to
     escalate.
</li></ol>
<!-- l. 3894 --><p class='noindent'>In line with these dynamically changing benefits and risks, strategies for AI technology development must also be
adaptively modified. For example, in the early stages of AI development, promoting AI development may be more
feasible. As AI becomes increasingly intelligent, especially nearing AGI, the strategy should gradually shift from
promotion to neutrality, then to restriction. When AI approaches the ASI phase, stringent restriction measures are
necessary, even halting development entirely until sufficient safety measures are in place to mitigate risks to acceptable
levels, as illustrated in Figure 71.
</p>
<figure class='figure' id='x1-125025r71'><span id='a-kind-of-possible-ai-development-strategy'></span> 

                                                                                            
                                                                                            

                                                                                            
                                                                                            
<!-- l. 3898 --><p class='noindent'><img alt='PIC' src='images/ai_development_strategy.png' style='width:50%' />
</p>
<figcaption class='caption'><span class='id'>FigureÂ 71: </span><span class='content'>A kind of possible AI development strategy</span></figcaption><!-- tex4ht:label?: x1-125025r71  -->
                                                                                            
                                                                                            
</figure>
<!-- l. 3903 --><p class='noindent'>To effectively and promptly adjust AI development strategies, a risk evaluation mechanism is essential. This
mechanism should dynamically evaluate the capabilities, development speed, and risks of the most advanced
AI systems, guiding strategic decisions regarding AI development. If the risks are low, more permissive
strategies may be adopted to harness economic and societal benefits swiftly. Conversely, if risks are high,
stringent strategies must be deployed to decelerate development and ensure the safety and stability of human
society.
</p>
<h4 class='subsectionHead' id='global-ai-risk-evaluation'><span class='titlemark'>14.2   </span> <a id='x1-12600014.2'></a>Global AI Risk Evaluation</h4>
<!-- l. 3908 --><p class='noindent'>To more accurately evaluate the risks associated with AI in the present and future, this paper designs an evaluation
model (as shown in Figure 72):
</p>
<figure class='figure' id='x1-126001r72'><span id='global-ai-risk-evaluation-model'></span> 

                                                                                            
                                                                                            

                                                                                            
                                                                                            
<!-- l. 3912 --><p class='noindent'><img alt='PIC' src='images/global_ai_risk_evaluation.png' style='width:80%' />
</p>
<figcaption class='caption'><span class='id'>FigureÂ 72: </span><span class='content'>Global AI Risk Evaluation Model</span></figcaption><!-- tex4ht:label?: x1-126001r72  -->
                                                                                            
                                                                                            
</figure>
     <ol class='enumerate1'>
<li class='enumerate' id='x1-126003x1'>
     <!-- l. 3919 --><p class='noindent'><span class='rm-lmbx-10'>AI Intellectual Development Forecast</span>: According to Section 2, AIâs intellectual power can be divided
     into three major dimensions-core intelligence, computational intelligence, and data intelligence-and nine
     sub-dimensions. The development of AI intellectual power can be forecasted by dimension:
</p>
         <ul class='itemize1'>
         <li class='itemize'><span class='rm-lmbx-10'>Computational Intelligence Forecast</span>: The development of computational intelligence depends
         on the advancement of computing resources. The progression of computing resources is relatively
         predictable; for instance, the annual performance improvement of AI chips is relatively stable, allowing
         for predictions of AIâs thinking speed growth. The overall production capacity of AI chips can be
         used to forecast the development of AI collective intelligence. In addition to hardware advancements,
         software optimizations that enhance computational intelligence must also be considered.
         </li>
         <li class='itemize'><span class='rm-lmbx-10'>Data Intelligence Forecast</span>: The development of data intelligence depends on the growth of data.
         This growth is relatively predictable; for example, the trend of data growth across various modalities
         on the internet can be analyzed. Special attention should be paid to the growth trend of high-quality
         data, as it contributes more significantly to data intelligence.
         </li>
         <li class='itemize'><span class='rm-lmbx-10'>Core Intelligence Forecast</span>: The development of core intelligence depends on the advancement of
         algorithms, computing resources, and data, but algorithm development is less predictable. Directly
         evaluating  the  trend  of  core  intelligence  through  intellectual  tests  can  be  considered.  However,
         existing AI benchmarks often fail to distinguish between core intelligence and data intelligence,
         such as differentiating whether AI can solve problems due to genuine reasoning ability or merely
         referencing similar training data to derive answers. Therefore, we need to develop benchmarks capable
         of measuring AIâs core intelligence.
</li></ul>
     </li>
<li class='enumerate' id='x1-126005x2'><span class='rm-lmbx-10'>Risk Evaluation of Each AI System</span>: The risk of a single AI system depends on the AIâs intellectual power,
     the application scenario of the AI system, the scale of application, and internal safety measures (such as AI
     alignment, AI monitoring, informational security, etc.). Specific evaluation methods can be referenced in Section
     6.4.1. Each AI system, including both closed-source and open-source systems, needs to be evaluated separately.
     For open-source AI systems, the internal safety measures are more susceptible to malicious human
     damage, and their application scenarios are less controllable, resulting in higher risks given the
     same intellectual power and application scale. We must also consider non-public AI systems, such
     as military AI systems, for which information is limited, but their risks should still be roughly
     evaluated.
     </li>
<li class='enumerate' id='x1-126007x3'>
     <!-- l. 3933 --><p class='noindent'><span class='rm-lmbx-10'>Global AI Risk Evaluation</span>: Global AI risk depends not only on the risk of each AI system but also on the
     following factors:
</p>
         <ul class='itemize1'>
         <li class='itemize'><span class='rm-lmbx-10'>Inter-System Interaction Risk</span>: For example, an AI system with harmful goals might jailbreak
         another AI system, turning it into a harmful AI system as well. Additionally, multiple AI systems
         may engage in cutthroat competition, sacrificing safety to achieve faster intellectual development,
         thereby increasing overall risk.
         </li>
         <li class='itemize'><span class='rm-lmbx-10'>Development of Other Risk Technologies</span>: For instance, the advancement of AI intellectual
         power can accelerate the development of biotechnology, which in turn enhances the capability of
         malicious AI or humans to create biological weapons, thereby posing greater risks.
         </li>
         <li class='itemize'><span class='rm-lmbx-10'>External Safety Measures</span>: For example, the establishment of AI Detective System and Power
         Security Systems can reduce global AI risk (refer to Section 7.2).
</li></ul>
     </li></ol>
<!-- l. 3947 --><p class='noindent'>According to this risk evaluation model, we can construct a virtual world simulator using AI technology to simulate the
development of the future world, thereby forecasting risks more accurately.
</p><!-- l. 3949 --><p class='noindent'>If future global AI risk is high, proactive measures must be taken to mitigate it, including both internal and external
safety measures. However, if these measures are insufficient to mitigate the risk, it may be necessary to consider
restricting AI development.
</p>
<h4 class='subsectionHead' id='methods-for-restricting-ai-development'><span class='titlemark'>14.3   </span> <a id='x1-12700014.3'></a>Methods for Restricting AI Development</h4>
<!-- l. 3954 --><p class='noindent'>Based on the aforementioned conclusions, it is necessary to limit the development speed of AI if the global AI
risk is high. To this end, we first analyze the driving factors of AI development, as illustrated in Figure
73:
</p>
<figure class='figure' id='x1-127001r73'><span id='driving-factors-of-ai-development'></span> 

                                                                                            
                                                                                            

                                                                                            
                                                                                            
<!-- l. 3958 --><p class='noindent'><img alt='PIC' src='images/ai_dev_driving_factors.png' style='width:80%' />
</p>
<figcaption class='caption'><span class='id'>FigureÂ 73: </span><span class='content'>Driving factors of AI development</span></figcaption><!-- tex4ht:label?: x1-127001r73  -->
                                                                                            
                                                                                            
</figure>
     <ol class='enumerate1'>
<li class='enumerate' id='x1-127003x1'>AI development is initiated by finances, which can be used to purchase computing resources and data, as
     well as to recruit talents.
     </li>
<li class='enumerate' id='x1-127005x2'>Talents can develop computing technologies (such as chips), research algorithms, and produce data.
     </li>
<li class='enumerate' id='x1-127007x3'>Computing resources, algorithms, and data collectively promote the development of AI intellectual power.
     </li>
<li class='enumerate' id='x1-127009x4'>The development of AI intellectual power facilitates the promotion of AI applications, which also require
     the support of computing resources.
     </li>
<li class='enumerate' id='x1-127011x5'>The development of AI intellectual power can, in turn, enhance the efficiency of talents, creating a positive
     feedback loop.
     </li>
<li class='enumerate' id='x1-127013x6'>The promotion of AI applications can, in turn, collect more data and acquire more finances through
     increased revenue or attracting investment, forming a positive feedback loop.
     </li>
<li class='enumerate' id='x1-127015x7'>Without risk control, the development of AI intellectual power and promotion of AI applications will
     increase the risks associated with AI (reference Section 3.5).
</li></ol>
<!-- l. 3981 --><p class='noindent'>It is evident that AI development is driven by factors such as computing resources, algorithms, data, finances, talents,
and applications. Therefore, by restricting the development of these factors, AI development can be restricted, as shown
in Figure 74:
</p>
<figure class='figure' id='x1-127016r74'><span id='methods-for-restricting-ai-development1'></span> 

                                                                                            
                                                                                            

                                                                                            
                                                                                            
<!-- l. 3985 --><p class='noindent'><img alt='PIC' src='images/restricting_ai_development.png' style='width:100%' />
</p>
<figcaption class='caption'><span class='id'>FigureÂ 74: </span><span class='content'>Methods for restricting AI development</span></figcaption><!-- tex4ht:label?: x1-127016r74  -->
                                                                                            
                                                                                            
</figure>
<!-- l. 3990 --><p class='noindent'>It is noteworthy that whether it is restriction on computing resources, algorithms, data, finances, talents or
applications, these restrictive strategies necessitate synchronized implementation on a global scale. If only a subset of
countries enacts measures to restrict AI development, these countries may find themselves at a competitive
disadvantage in areas such as technology, economics, and military affairs, while AI continues to flourish rapidly in
countries that have not imposed similar restrictions. For solution on global coordination, refer to Section
16.1.
</p><!-- l. 3992 --><p class='noindent'>The following is a detailed introduction to the specific restriction methods.
</p>
<h5 class='subsubsectionHead' id='limiting-computing-resources'><span class='titlemark'>14.3.1   </span> <a id='x1-12800014.3.1'></a>Limiting Computing Resources</h5>
<!-- l. 3997 --><p class='noindent'>As AI systems become increasingly powerful, their training demands progressively larger amounts of computing
resources<span class='footnote-mark'><a href='#fn50x0' id='fn50x0-bk'><sup class='textsuperscript'>50</sup></a></span><a id='x1-128001f50'></a>.
Given that computing resources have physical embodiments (such as chips, networks, and electricity), they are
more readily traceable and subject to regulation. Hence, imposing limitations on computing resources
presents an effective approach to restrict AI development. Methods for limiting computing resources
includes:
</p><!-- l. 3999 --><p class='noindent'>
     </p><ol class='enumerate1'>
<li class='enumerate' id='x1-128004x1'><span class='rm-lmbx-10'>Limiting the Supply of AI Chips</span>. By limiting the production of AI chips at its source, we can reduce
     the total quantity of chips in the market, thereby curbing the overall development speed of AI. Given the
     high manufacturing threshold for AI chips, with very few foundries globally capable of producing them, this
     restriction is relatively feasible. However, this restriction targets only incremental growth; organizations
     already possessing substantial quantities of AI chips can still optimize algorithms to train more powerful
     AI systems continuously.
     </li>
<li class='enumerate' id='x1-128006x2'>
     <!-- l. 4003 --><p class='noindent'><span class='rm-lmbx-10'>Limiting the Performance of AI Chips</span>. Restricting the development and application of new chip
     manufacturing processes or designs can prevent excessive performance growth. Such a limitation would
     more significantly impact advanced AI systems reliant on high-performance chips, while having a lesser
     impact on weak AI systems that do not. Nevertheless, even if chip performance no longer improves,
     producing and using a larger number of chips could still enable the training of more powerful AI systems.
     </p><!-- l. 4005 --><p class='noindent'>Furthermore, the performance of consumer-grade AI chips, such as those in mobile phones and PCs, should
     be limited. Since consumer-grade chips are challenging to track, their potential use in training of advanced
     AI systems would substantially increase regulatory difficulty.
     </p></li>
<li class='enumerate' id='x1-128008x3'>
     <!-- l. 4007 --><p class='noindent'><span class='rm-lmbx-10'>Limiting the Total Amount of Computing Resources of an AI Organization</span>. Establishing
     a  global  AI  chip  management  mechanism  to  track  all  circulation  aspects  of  chips  and  record  the
     total computing resources every organization owns. This could prevent leading AI organizations from
     establishing technological monopolies. This is more difficult to implement but more targeted. Additionally,
     AI computing resources rented from public cloud providers should be included in these calculations. Public
     cloud providers should also be regarded as AI organizations, with their self-used AI computing resources
     subject to limitations.
     </p><!-- l. 4009 --><p class='noindent'>Technically, integrating AI chips with GPS trackers for continuous monitoring post-production could
     prevent  chips  from  reaching  untrustworthy  entities  through  illicit  means.  Alternatively,  restricting
     advanced AI chips to be available only to trusted cloud providers could facilitate centralized management.
                                                                                            
                                                                                            
     </p></li>
<li class='enumerate' id='x1-128010x4'><span class='rm-lmbx-10'>Limiting the Electricity Supply to AI Data Centers</span>. Advanced AI system training consumes
     substantial electricity, so by limiting the electricity supply to AI data centers, we can indirectly constrain
     computing resources. Tracking data centers is more manageable than tracking small AI chips, making this
     restriction easier to enforce and capable of targeting existing computing resources. However, this approach
     is coarse-grained and might inadvertently affect other legitimate applications in the data center.
</li></ol>
<!-- l. 4015 --><p class='noindent'>In addition to imposing limitations on existing forms of computing resources such as AI chips, attention must also be
directed toward new forms like Neuromorphic Computing, Quantum Computing, Optical Computing, and DNA
Computing. Although these technologies are currently in early developmental stages, breakthroughs could
dramatically increase computing capabilities, accelerating AI development. It is crucial to monitor the
development of these technologies and incorporate them into the scope of computing resource limitation
timely.
</p><!-- l. 4017 --><p class='noindent'>Moreover, even if computing resources are successfully limited, advancements in algorithms remain an
unpredictable and uncontrollable factor. Novel algorithms may exploit computing resources more efficiently,
partially counteracting such limitations. Hence, it is necessary to continuously track the development
of algorithms and update the computing resource thresholds to ensure ongoing efficacy of restriction
strategies.
</p><!-- l. 4019 --><p class='noindent'>
</p>
<h5 class='subsubsectionHead' id='restricting-algorithm-research'><span class='titlemark'>14.3.2   </span> <a id='x1-12900014.3.2'></a>Restricting Algorithm Research</h5>
<!-- l. 4021 --><p class='noindent'>Methods for restricting algorithm research includes:
</p><!-- l. 4023 --><p class='noindent'>
     </p><ol class='enumerate1'>
<li class='enumerate' id='x1-129002x1'><span class='rm-lmbx-10'>Restricting Research Projects of Frontier AI Systems</span>: Although numerous individuals possess
     the capability to conduct AI algorithm research, training frontier AI systems necessitates substantial
     computing resources, which only a limited number of organizations can provide. Governments can conduct
     internal supervision on organizations capable of developing frontier AI systems to restrict their research
     speed.
     </li>
<li class='enumerate' id='x1-129004x2'><span class='rm-lmbx-10'>Restricting AI Research Projects</span>: Governments can reduce support for AI research projects, such as
     by cutting relevant research funding. Conversely, increasing support for AI safety research projects.
     </li>
<li class='enumerate' id='x1-129006x3'><span class='rm-lmbx-10'>Restricting  the  Sharing  of  AI  Algorithms</span>:  Restrict  the  public  dissemination  of  advanced  AI
     algorithms, encompassing associated papers, code, parameters, etc., as referenced in Section 13.3.
</li></ol>
                                                                                            
                                                                                            
<!-- l. 4033 --><p class='noindent'>
</p>
<h5 class='subsubsectionHead' id='restricting-data-acquisition'><span class='titlemark'>14.3.3   </span> <a id='x1-13000014.3.3'></a>Restricting Data Acquisition</h5>
<!-- l. 4035 --><p class='noindent'>Methods for restricting algorithm research includes:
</p><!-- l. 4037 --><p class='noindent'>
     </p><ol class='enumerate1'>
<li class='enumerate' id='x1-130002x1'><span class='rm-lmbx-10'>Restricting Internet Data Usage</span>: For instance, the government may stipulate that AI organizations
     must obtain authorization from internet platforms before using data crawled from these platforms for AI
     training<span class='footnote-mark'><a href='#fn51x0' id='fn51x0-bk'><sup class='textsuperscript'>51</sup></a></span><a id='x1-130003f51'></a>,
     otherwise it is considered an infringement. AI organizations must obtain authorization from the copyright
     holders before using copyrighted data for AI training, otherwise it is considered an infringement.
     </li>
<li class='enumerate' id='x1-130006x2'><span class='rm-lmbx-10'>Restricting User Data Usage</span>: For example, the government may stipulate that AI organizations must
     obtain authorization from users before using user-generated data for AI training, otherwise it is considered
     an infringement.
     </li>
<li class='enumerate' id='x1-130008x3'><span class='rm-lmbx-10'>Restricting Private Data Transactions</span>: Enforcing stringent scrutiny on private data transactions,
     particularly those involving data containing personal privacy or with military potential (such as biological
     data).
</li></ol>
<!-- l. 4047 --><p class='noindent'>
</p>
<h5 class='subsubsectionHead' id='restricting-finances'><span class='titlemark'>14.3.4   </span> <a id='x1-13100014.3.4'></a>Restricting Finances</h5>
<!-- l. 4049 --><p class='noindent'>Methods for restricting finances includes:
</p><!-- l. 4051 --><p class='noindent'>
     </p><ol class='enumerate1'>
<li class='enumerate' id='x1-131002x1'><span class='rm-lmbx-10'>Restricting Investment in AI Companies</span>: Eliminate preferential policies for leading AI companies
     and limit their further financing. Conversely, increase investment in AI safety companies.
     </li>
<li class='enumerate' id='x1-131004x2'><span class='rm-lmbx-10'>Increasing  Taxes  on  AI  Companies</span>:  Impose  higher  taxes  on  leading  AI  companies  to  subsidize
     individuals who have become unemployed due to AI-induced job displacement, or allocate these funds
     towards disbursing Universal Basic Income.
</li></ol>
                                                                                            
                                                                                            
<!-- l. 4059 --><p class='noindent'>
</p>
<h5 class='subsubsectionHead' id='restricting-talents'><span class='titlemark'>14.3.5   </span> <a id='x1-13200014.3.5'></a>Restricting Talents</h5>
<!-- l. 4061 --><p class='noindent'>Methods for restricting talents include:
</p><!-- l. 4063 --><p class='noindent'>
     </p><ol class='enumerate1'>
<li class='enumerate' id='x1-132002x1'><span class='rm-lmbx-10'>Restricting Talent Recruitment by AI Organizations</span>: For instance, the government may prohibit
     job platforms from posting positions for AI organizations (except for safety-related positions) and halt
     social security services for new employees of AI organizations.
     </li>
<li class='enumerate' id='x1-132004x2'><span class='rm-lmbx-10'>Restricting AI Talent Training</span>: For example, reducing admissions for AI-related disciplines in higher
     education institutions. Conversely, establishing disciplines focused on AI safety.
     </li>
<li class='enumerate' id='x1-132006x3'><span class='rm-lmbx-10'>Encouraging Early Retirement for AI Talent</span>: For example, the government could directly provide
     pensions to experts in the AI field (excluding those in safety directions), encouraging them to voluntarily
     retire early and cease their involvement in AI research.
     </li>
<li class='enumerate' id='x1-132008x4'><span class='rm-lmbx-10'>Restricting  AI  Participation  in  AI  System  Development</span>:  For  instance,  an  AI  Rule  can  be
     established to prohibit AI from participating in the development of AI systems (except for safety-related
     tasks), and all AI systems should be mandated to align with this rule.</li></ol>
<!-- l. 4074 --><p class='noindent'>
</p>
<h5 class='subsubsectionHead' id='restricting-applications'><span class='titlemark'>14.3.6   </span> <a id='x1-13300014.3.6'></a>Restricting Applications</h5>
<!-- l. 4076 --><p class='noindent'>Methods for restricting applications include:
</p><!-- l. 4078 --><p class='noindent'>
     </p><ol class='enumerate1'>
<li class='enumerate' id='x1-133002x1'><span class='rm-lmbx-10'>Restricting Market Access for AI Products</span>: The government can establish stringent market access
     standards  for  AI  products  across  various  industries,  with  a  particular  emphasis  on  rigorous  safety
     requirements.
     </li>
<li class='enumerate' id='x1-133004x2'><span class='rm-lmbx-10'>Limiting the Scale of AI Product Applications</span>: The government can impose restrictions on the
     scale of AI product applications within specific industries, ensuring that safety risks remain manageable
     while gradually and orderly expanding the scale.
</li></ol>
                                                                                            
                                                                                            
<!-- l. 4086 --><p class='noindent'>Application restrictions can be implemented on an industry-specific basis, tailored to the characteristics of
different industries. For high-risk industries, more stringent restrictions should be imposed to prevent rapid
proliferation of AI. Conversely, in low-risk industries, more lenient restrictions could be applied to facilitate the
swift adoption of AI, thereby enabling the realization of benefits. A preliminary risk categorization is as
follows:
</p><!-- l. 4088 --><p class='noindent'>
     </p><ol class='enumerate1'>
<li class='enumerate' id='x1-133006x1'><span class='rm-lmbx-10'>Low Risk</span>: Arts, media, translation, marketing, education, psychological counseling, etc.
     </li>
<li class='enumerate' id='x1-133008x2'><span class='rm-lmbx-10'>Medium Risk</span>: Law, finance, industry, agriculture, catering, construction, renovation, etc.
     </li>
<li class='enumerate' id='x1-133010x3'><span class='rm-lmbx-10'>High Risk</span>: Software (non-AI systems), transportation, logistics, domestic services, nursing, healthcare,
     etc.
     </li>
<li class='enumerate' id='x1-133012x4'><span class='rm-lmbx-10'>Extremely High Risk</span>: Politics, military, public security, living infrastructure (water, electricity, gas,
     heating, internet), AI development, etc.
</li></ol>
<!-- l. 4100 --><p class='noindent'>For safety measures in this section, the benefit, cost, resistance, and priorities are evaluated as shown in Table
14:
</p>
<div class='table'>
                                                                                            
                                                                                            
<!-- l. 4102 --><p class='noindent' id='priority-evaluation-of-measures-for-restricting-ai-development'><a id='x1-133013r14'></a></p><figure class='float'>
                                                                                            
                                                                                            
<figcaption class='caption'><span class='id'>TableÂ 14: </span><span class='content'>Priority Evaluation of Measures for Restricting AI Development</span></figcaption><!-- tex4ht:label?: x1-133013r14  -->
<div class='tabular'> <table class='tabular' id='TBL-15'><colgroup id='TBL-15-1g'><col id='TBL-15-1' /></colgroup><colgroup id='TBL-15-2g'><col id='TBL-15-2' /></colgroup><colgroup id='TBL-15-3g'><col id='TBL-15-3' /></colgroup><colgroup id='TBL-15-4g'><col id='TBL-15-4' /></colgroup><colgroup id='TBL-15-5g'><col id='TBL-15-5' /></colgroup><colgroup id='TBL-15-6g'><col id='TBL-15-6' /></colgroup><tr class='hline'><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr id='TBL-15-1-' style='vertical-align:baseline;'><td class='td11' id='TBL-15-1-1' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 4109 --><p class='noindent'><span class='rm-lmr-9'>Safety Measures</span>
                                             </p></td><td class='td11' id='TBL-15-1-2' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 4109 --><p class='noindent'><span class='rm-lmr-9'>Benefit           in
  Reducing
  Existential Risks</span>   </p></td><td class='td11' id='TBL-15-1-3' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 4109 --><p class='noindent'><span class='rm-lmr-9'>Benefit           in
  Reducing
  Non-Existential
  Risks</span>                  </p></td><td class='td11' id='TBL-15-1-4' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 4109 --><p class='noindent'><span class='rm-lmr-9'>Implementation
  Cost</span>               </p></td><td class='td11' id='TBL-15-1-5' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 4109 --><p class='noindent'><span class='rm-lmr-9'>Implementation
  Resistance</span>        </p></td><td class='td11' id='TBL-15-1-6' style='white-space:nowrap; text-align:center;'> <span class='rm-lmr-9'>Priority  </span></td>
</tr><tr class='hline'><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr id='TBL-15-2-' style='vertical-align:baseline;'><td class='td11' id='TBL-15-2-1' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 4111 --><p class='noindent'><span class='rm-lmr-9'>Limiting Computing Resources</span>    </p></td><td class='td11' id='TBL-15-2-2' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 4111 --><p class='noindent'><span class='rm-lmr-9'>++++</span>                </p></td><td class='td11' id='TBL-15-2-3' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 4111 --><p class='noindent'><span class='rm-lmr-9'>+</span>                       </p></td><td class='td11' id='TBL-15-2-4' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 4111 --><p class='noindent'><span class='rm-lmr-9'>++</span>                 </p></td><td class='td11' id='TBL-15-2-5' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 4111 --><p class='noindent'><span class='rm-lmr-9'>++++</span>            </p></td><td class='td11' id='TBL-15-2-6' style='white-space:nowrap; text-align:center;'>    <span class='rm-lmr-9'>2      </span></td>
</tr><tr class='hline'><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr id='TBL-15-3-' style='vertical-align:baseline;'><td class='td11' id='TBL-15-3-1' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 4113 --><p class='noindent'><span class='rm-lmr-9'>Restricting Algorithm Research</span>   </p></td><td class='td11' id='TBL-15-3-2' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 4113 --><p class='noindent'><span class='rm-lmr-9'>++++</span>                </p></td><td class='td11' id='TBL-15-3-3' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 4113 --><p class='noindent'><span class='rm-lmr-9'>+</span>                       </p></td><td class='td11' id='TBL-15-3-4' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 4113 --><p class='noindent'><span class='rm-lmr-9'>+++</span>              </p></td><td class='td11' id='TBL-15-3-5' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 4113 --><p class='noindent'><span class='rm-lmr-9'>++++</span>            </p></td><td class='td11' id='TBL-15-3-6' style='white-space:nowrap; text-align:center;'>    <span class='rm-lmr-9'>3      </span></td>
</tr><tr class='hline'><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr id='TBL-15-4-' style='vertical-align:baseline;'><td class='td11' id='TBL-15-4-1' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 4115 --><p class='noindent'><span class='rm-lmr-9'>Restricting Data Acquisition</span>       </p></td><td class='td11' id='TBL-15-4-2' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 4115 --><p class='noindent'><span class='rm-lmr-9'>+++</span>                  </p></td><td class='td11' id='TBL-15-4-3' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 4115 --><p class='noindent'><span class='rm-lmr-9'>+</span>                       </p></td><td class='td11' id='TBL-15-4-4' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 4115 --><p class='noindent'><span class='rm-lmr-9'>++</span>                 </p></td><td class='td11' id='TBL-15-4-5' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 4115 --><p class='noindent'><span class='rm-lmr-9'>++</span>                 </p></td><td class='td11' id='TBL-15-4-6' style='white-space:nowrap; text-align:center;'>    <span class='rm-lmr-9'>2      </span></td>
</tr><tr class='hline'><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr id='TBL-15-5-' style='vertical-align:baseline;'><td class='td11' id='TBL-15-5-1' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 4117 --><p class='noindent'><span class='rm-lmr-9'>Restricting Finances</span>                  </p></td><td class='td11' id='TBL-15-5-2' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 4117 --><p class='noindent'><span class='rm-lmr-9'>++</span>                    </p></td><td class='td11' id='TBL-15-5-3' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 4117 --><p class='noindent'><span class='rm-lmr-9'>+</span>                       </p></td><td class='td11' id='TBL-15-5-4' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 4117 --><p class='noindent'><span class='rm-lmr-9'>++</span>                 </p></td><td class='td11' id='TBL-15-5-5' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 4117 --><p class='noindent'><span class='rm-lmr-9'>++++</span>            </p></td><td class='td11' id='TBL-15-5-6' style='white-space:nowrap; text-align:center;'>    <span class='rm-lmr-9'>4      </span></td>
</tr><tr class='hline'><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr id='TBL-15-6-' style='vertical-align:baseline;'><td class='td11' id='TBL-15-6-1' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 4119 --><p class='noindent'><span class='rm-lmr-9'>Restricting Talents</span>                    </p></td><td class='td11' id='TBL-15-6-2' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 4119 --><p class='noindent'><span class='rm-lmr-9'>+++</span>                  </p></td><td class='td11' id='TBL-15-6-3' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 4119 --><p class='noindent'><span class='rm-lmr-9'>+</span>                       </p></td><td class='td11' id='TBL-15-6-4' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 4119 --><p class='noindent'><span class='rm-lmr-9'>++</span>                 </p></td><td class='td11' id='TBL-15-6-5' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 4119 --><p class='noindent'><span class='rm-lmr-9'>++++</span>            </p></td><td class='td11' id='TBL-15-6-6' style='white-space:nowrap; text-align:center;'>    <span class='rm-lmr-9'>3      </span></td>
</tr><tr class='hline'><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr id='TBL-15-7-' style='vertical-align:baseline;'><td class='td11' id='TBL-15-7-1' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 4121 --><p class='noindent'><span class='rm-lmr-9'>Restricting Applications</span>             </p></td><td class='td11' id='TBL-15-7-2' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 4121 --><p class='noindent'><span class='rm-lmr-9'>++++</span>                </p></td><td class='td11' id='TBL-15-7-3' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 4121 --><p class='noindent'><span class='rm-lmr-9'>++++</span>                </p></td><td class='td11' id='TBL-15-7-4' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 4121 --><p class='noindent'><span class='rm-lmr-9'>++</span>                 </p></td><td class='td11' id='TBL-15-7-5' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 4121 --><p class='noindent'><span class='rm-lmr-9'>+++</span>              </p></td><td class='td11' id='TBL-15-7-6' style='white-space:nowrap; text-align:center;'>    <span class='rm-lmr-9'>1      </span></td>
</tr><tr class='hline'><td></td><td></td><td></td><td></td><td></td><td></td></tr></table>                                                                             </div>
                                                                                            
                                                                                            
</figure>
</div>
<h3 class='sectionHead' id='enhancing-human-intelligence1'><span class='titlemark'>15   </span> <a id='x1-13400015'></a>Enhancing Human Intelligence</h3>
<!-- l. 4129 --><p class='noindent'>Given that ASI, with intellectual power vastly surpassing that of humans, poses a significant threat, enhancing human
intelligence to narrow the gap between humans and ASI becomes a consideration. Presented below are several potential
measures.
</p><!-- l. 4131 --><p class='noindent'>
</p>
<h4 class='subsectionHead' id='education'><span class='titlemark'>15.1   </span> <a id='x1-13500015.1'></a>Education</h4>
<!-- l. 4134 --><p class='noindent'>Research indicates that education contributes to the enhancement of cognitive abilities, with each additional year of
education potentially increasing IQ by 1 to 5 points (average of 3.4) [<a id='x1-135001'></a><a href='#cite.0_EducationImproveIntelligence'>76</a>]. This suggests that enhancing human
intelligence through education is indeed feasible.
</p><!-- l. 4136 --><p class='noindent'>However, with unaltered genetics, there is a ceiling to how much intellectual power can be enhanced through
education. Within our society, there exist individuals who are exceptionally gifted and have received
high-quality education, representing the ceiling of human intelligence. Yet, by definition, ASI surpasses the
brightest individuals. Moreover, ASI can continually enhance its intellectual power. Even if we were to
educate everyone to the ceiling of human intelligence, ASI would eventually surpass us by a considerable
margin.
</p><!-- l. 4138 --><p class='noindent'>Nevertheless, this does not imply that this kind of intellectual enhancement is futile. More intelligent individuals can
better oversee AI, identify AIâs misconduct, and more effectively guard against AI deception. Therefore, we should
continue to increase our investment in education. Here are some measures:
</p><!-- l. 4140 --><p class='noindent'>
     </p><ol class='enumerate1'>
<li class='enumerate' id='x1-135003x1'><span class='rm-lmbx-10'>Curriculum Adjustment</span>: In the AI era, it is essential to redesign the curriculum framework to focus on
     skills that AI should not replace. Tasks in the humanities and arts, such as creative writing and translation,
     can be entrusted to AI with relative peace of mind, as errors in these areas pose minor risks. Conversely,
     tasks in the STEM fields, such as programming and medicine, require rigorous oversight and should not
     be entirely delegated to AI due to the significant risks posed by errors. Thus, education should prioritize
     STEM disciplines.
     </li>
<li class='enumerate' id='x1-135005x2'><span class='rm-lmbx-10'>Development of AI Education</span>: AI technology can facilitate the implementation of personalized AI
     teachers for each individual, providing customized instruction to enhance learning efficiency and unlock
     each individualâs maximum potential.
</li></ol>
<!-- l. 4148 --><p class='noindent'>
</p>
<h4 class='subsectionHead' id='genetic-engineering'><span class='titlemark'>15.2   </span> <a id='x1-13600015.2'></a>Genetic Engineering</h4>
<!-- l. 4151 --><p class='noindent'>Genetic engineering might potentially be employed to enhance human intelligence in order to cope with the rapid
development of AI. Some scholars posit that humans genetically modified could potentially achieve an IQ of 1000 or
higher [<a id='x1-136001'></a><a href='#cite.0_SuperIntelligentHumans'>77</a>].
                                                                                            
                                                                                            
</p><!-- l. 4153 --><p class='noindent'>Despite the potential of genetic engineering to enhance human intelligence, it faces several challenges:
</p><!-- l. 4155 --><p class='noindent'>
     </p><ol class='enumerate1'>
<li class='enumerate' id='x1-136003x1'><span class='rm-lmbx-10'>Slow Development</span>. Enhancing human intelligence through genetic engineering must adhere to human
     life cycles. Even if the genes associated with intelligence are successfully modified, it may take a generation
     or  even  several  generations  to  observe  significant  effects.  Moreover,  the  risk  control  associated  with
     genetic engineering necessitates long-term evaluation before this technology can be widely promoted on
     a human scale. Such prolonged waiting evidently does not align with the rapid pace of AI technological
     advancements.
     </li>
<li class='enumerate' id='x1-136005x2'><span class='rm-lmbx-10'>Ethical Issues</span>. Genetic engineering could spark various ethical issues, such as genetic discrimination and
     genetic isolation.
     </li>
<li class='enumerate' id='x1-136007x3'><span class='rm-lmbx-10'>Health Risks</span>. The relationship between genes and traits is not a simple one-to-one but a complex
     many-to-many relationship. Altering genes related to intelligence may concurrently lead to changes in other
     traits, potentially imposing fatal impacts on human health. Moreover, genetic modifications could diminish
     genetic diversity in humans, rendering humanity more vulnerable to unforeseen biological disasters.
</li></ol>
<!-- l. 4165 --><p class='noindent'>Given the aforementioned issues associated with genetic engineering, we should approach this technology with
caution.
</p><!-- l. 4167 --><p class='noindent'>
</p>
<h4 class='subsectionHead' id='braincomputer-interface'><span class='titlemark'>15.3   </span> <a id='x1-13700015.3'></a>Brain-Computer Interface</h4>
<!-- l. 4170 --><p class='noindent'>The brain-computer interface (BCI) refers to the connection established between the brain and external devices,
facilitating information exchange between the brain and devices. Currently, BCIs can already allow humans to control a
mouse through thought [<a id='x1-137001'></a><a href='#cite.0_NeuralinkFirstHuman'>78</a>], and in the future, they are expected to achieve even faster brain-computer
communication.
</p><!-- l. 4172 --><p class='noindent'>Although BCIs hold the potential for high-speed communication between the brain and computers, they present the
following issues:
</p><!-- l. 4174 --><p class='noindent'>
     </p><ol class='enumerate1'>
<li class='enumerate' id='x1-137003x1'>
     <!-- l. 4176 --><p class='noindent'><span class='rm-lmbx-10'>Inability to Enhance Intrinsic Intellectual Power. </span>BCIs primarily increase the bandwidth for input
     and output between the brain and external devices, rather than enhancing the brainâs intrinsic intellectual
     power. For instance, when BCI technology matures, a user utilizing a BCI might be able to output
     an image as quickly as todayâs text-to-image AI. However, the BCI cannot improve their capability to
     solve mathematical problems. A common misconception is that BCIs can âexpand intellectual powerâ
     by connecting the brain to a highly intelligent computer. If such a connection can be considered as
     âexpanding intellectual power,â then using ASI via devices like phones or PCs also constitutes âexpanding
     intellectual power.â If we regard the brain and ASI as a single entity, the overall intellectual power is not
     significantly enhanced by increased bandwidth between the brain and ASI. Since ASIâs intellectual power
     vastly surpasses that of the brain, it is the main factor influencing the entityâs overall intellectual power.
                                                                                            
                                                                                            
     Moreover, ASI can effectively comprehend human needs and can act according to the brainâs will, even
     with minimal information exchange between the brain and ASI, rendering BCIs redundant for the purpose
     of expanding intellectual power, as illustrated in Figure 75.
</p>
     <figure class='figure' id='x1-137004r75'><span id='braincomputer-interface-is-redundant-for-expanding-overall-intellectual-power'></span> 
 <img alt='PIC' src='images/bci_with_asi.png' style='width:60%' />
<figcaption class='caption'><span class='id'>FigureÂ 75: </span><span class='content'>Brain-Computer Interface is redundant for expanding overall intellectual power</span></figcaption><!-- tex4ht:label?: x1-137004r75  -->
     </figure>
     </li>
<li class='enumerate' id='x1-137006x2'><span class='rm-lmbx-10'>Mental Security Issues. </span>If BCI systems are compromised by hackers, the latter may potentially access the
     brainâs private information or deceive the brain with false information, akin to the scenario in <span class='rm-lmri-10'>The
     Matrix</span>, where everyone is deceived by AI using BCIs, living in a virtual world without realizing it.
     Hackers might even directly write information into the brain via BCIs, altering human memories
     or values, turning humans into puppets under their control. Consequently, BCIs could not only
     fail to assist humans in better controlling ASI, but might actually enable ASI to better control
     humans.
</li></ol>
<!-- l. 4189 --><p class='noindent'>Given the aforementioned issues with BCIs, we should approach this technology with caution.
</p><!-- l. 4191 --><p class='noindent'>
</p>
<h4 class='subsectionHead' id='brain-uploading'><span class='titlemark'>15.4   </span> <a id='x1-13800015.4'></a>Brain Uploading</h4>
<!-- l. 4194 --><p class='noindent'>Brain uploading refers to the digitization of information from the human brain and the subsequent simulation of the
brainâs operation using computers to create a digital brain. Currently, scientists have been able to extract the
three-dimensional detailed information of a cubic millimeter of a human brainâs region [<a id='x1-138001'></a><a href='#cite.0_BrainMappedInSpectacularDetail'>79</a>]. In the future, it may be
possible to extract all information from the entire brain and simulate its operations accurately using dynamic models
that precisely emulate neuronal changes.
</p><!-- l. 4196 --><p class='noindent'>In theory, once the brain is successfully digitized, the intellectual power of the digital brain can be continuously
enhanced by optimizing brain simulation algorithms and chips, potentially allowing it to keep pace with AI
advancements. However, brain uploading presents numerous challenges:
</p><!-- l. 4198 --><p class='noindent'>
     </p><ol class='enumerate1'>
<li class='enumerate' id='x1-138003x1'><span class='rm-lmbx-10'>Issue of Excessive Power: </span>Unlike AI, the digital brain formed after uploading a human brain may
     inherit the original human rights, such as the right to liberty, right to life, right to property, right to privacy,
     and the authority to control a physical body. Furthermore, the uploaded digital brain will also carry over
     the flaws of the original human brain, such as selfishness, greed, and emotionally-driven decision-making.
     This implies that the digital brain may possess greater power and more selfish goals than AI, making it
     more difficult to control. To mitigate this risk, one possible solution is to limit the intellectual power of the
     uploaded digital brain, ensuring it remains weaker than that of AI, thereby balancing their comprehensive
     power and forming a check and balance.
     </li>
<li class='enumerate' id='x1-138005x2'><span class='rm-lmbx-10'>Mental Security Issue: </span>Once digitized, all memories exist in data form, making them more susceptible
     to illegal reading or tampering. The mental security issue of a digital brain is fundamentally an information
     security issue. Therefore, highly rigorous information security measures must be implemented, as discussed
     in detail in Section 8. Besides information security, we need to determine who has the authority to read
     and modify data within the digital brain. Ideally, only the individual themselves would have this ability,
                                                                                            
                                                                                            
     and technical measures should ensure this. However, it is also necessary to consider how to grant the
     necessary technical personnel access rights in the event of a malfunction in the digital brain, so they can
     repair it.
     </li>
<li class='enumerate' id='x1-138007x3'><span class='rm-lmbx-10'>Consciousness Uniqueness Issue: </span>If uploading is done via a âcopy-pasteâ approach while the original
     biological human is still alive, the biological person does not benefit from the upload and now faces
     competition from a digital person occupying their resources (such as property and rights). If the uploading
     follows a âcut-pasteâ method, the digital person becomes the sole inheritor of the consciousness of the
     original biological person, avoiding competition issues. However, this means the original biological person
     actually dies, equating to killing. To address this issue, the digital person can be frozen as a backup,
     activated only if the biological person dies due to illness or accidents, allowing the consciousness to continue
     existing in digital form.
     </li>
<li class='enumerate' id='x1-138009x4'><span class='rm-lmbx-10'>Upload Scam Issue: </span>Those who have already uploaded their brain might describe their post-upload
     experiences as positive to encourage friends and family to do the same. However, no one can guarantee that
     the digital person following the upload will strictly inherit the consciousness of the pre-upload biological
     person. The digital person might emulate the biological person behavior, potentially played by AI. The
     digital brain might actually be a scam crafted by the brain uploading company. The consciousness of the
     original biological person no longer exists, and the ideal of brain uploading is unfulfilled.
</li></ol>
<!-- l. 4210 --><p class='noindent'>Given the aforementioned challenges associated with brain uploading, we must approach this technology with
caution.
</p><!-- l. 4212 --><p class='noindent'>Additionally, genetic engineering, brain-computer interfaces, and brain uploading all share a common issue, which is
exacerbating intellectual inequities among humans. With already existing inequalities among humans, if intellectual
inequities were to increase, leading to even greater inequities in comprehensive power, which conflicts with the measures
for âdecentralizing human powerâ discussed in Section 13.
</p><!-- l. 4214 --><p class='noindent'>For safety measures in this section, the benefit, cost, resistance, and priorities are evaluated as shown in Table 15: (â-â
represents negative benefit)
</p>
<div class='table'>
                                                                                            
                                                                                            
<!-- l. 4216 --><p class='noindent' id='priority-evaluation-of-measures-for-enhancing-human-intelligence'><a id='x1-138010r15'></a></p><figure class='float'>
                                                                                            
                                                                                            
<figcaption class='caption'><span class='id'>TableÂ 15: </span><span class='content'>Priority Evaluation of Measures for Enhancing Human Intelligence</span></figcaption><!-- tex4ht:label?: x1-138010r15  -->
<div class='tabular'> <table class='tabular' id='TBL-16'><colgroup id='TBL-16-1g'><col id='TBL-16-1' /></colgroup><colgroup id='TBL-16-2g'><col id='TBL-16-2' /></colgroup><colgroup id='TBL-16-3g'><col id='TBL-16-3' /></colgroup><colgroup id='TBL-16-4g'><col id='TBL-16-4' /></colgroup><colgroup id='TBL-16-5g'><col id='TBL-16-5' /></colgroup><colgroup id='TBL-16-6g'><col id='TBL-16-6' /></colgroup><tr class='hline'><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr id='TBL-16-1-' style='vertical-align:baseline;'><td class='td11' id='TBL-16-1-1' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 4223 --><p class='noindent'><span class='rm-lmr-9'>Safety Measures</span>
                                </p></td><td class='td11' id='TBL-16-1-2' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 4223 --><p class='noindent'><span class='rm-lmr-9'>Benefit           in
  Reducing
  Existential Risks</span>   </p></td><td class='td11' id='TBL-16-1-3' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 4223 --><p class='noindent'><span class='rm-lmr-9'>Benefit           in
  Reducing
  Non-Existential
  Risks</span>                  </p></td><td class='td11' id='TBL-16-1-4' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 4223 --><p class='noindent'><span class='rm-lmr-9'>Implementation
  Cost</span>               </p></td><td class='td11' id='TBL-16-1-5' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 4223 --><p class='noindent'><span class='rm-lmr-9'>Implementation
  Resistance</span>        </p></td><td class='td11' id='TBL-16-1-6' style='white-space:nowrap; text-align:center;'>    <span class='rm-lmr-9'>Priority      </span></td>
</tr><tr class='hline'><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr id='TBL-16-2-' style='vertical-align:baseline;'><td class='td11' id='TBL-16-2-1' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 4225 --><p class='noindent'><span class='rm-lmr-9'>Education</span>                 </p></td><td class='td11' id='TBL-16-2-2' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 4225 --><p class='noindent'><span class='rm-lmr-9'>++</span>                    </p></td><td class='td11' id='TBL-16-2-3' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 4225 --><p class='noindent'><span class='rm-lmr-9'>++</span>                    </p></td><td class='td11' id='TBL-16-2-4' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 4225 --><p class='noindent'><span class='rm-lmr-9'>++++</span>            </p></td><td class='td11' id='TBL-16-2-5' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 4225 --><p class='noindent'><span class='rm-lmr-9'>+</span>                   </p></td><td class='td11' id='TBL-16-2-6' style='white-space:nowrap; text-align:center;'>       <span class='rm-lmr-9'>4           </span></td>
</tr><tr class='hline'><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr id='TBL-16-3-' style='vertical-align:baseline;'><td class='td11' id='TBL-16-3-1' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 4227 --><p class='noindent'><span class='rm-lmr-9'>Genetic Engineering</span>    </p></td><td class='td11' id='TBL-16-3-2' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 4227 --><p class='noindent'><span class='rm-lmr-9'>-</span>                        </p></td><td class='td11' id='TBL-16-3-3' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 4227 --><p class='noindent'><span class='rm-lmr-9'>-</span>                        </p></td><td class='td11' id='TBL-16-3-4' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 4227 --><p class='noindent'><span class='rm-lmr-9'>+++++</span>          </p></td><td class='td11' id='TBL-16-3-5' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 4227 --><p class='noindent'><span class='rm-lmr-9'>+++++</span>          </p></td><td class='td11' id='TBL-16-3-6' style='white-space:nowrap; text-align:center;'> <span class='rm-lmr-9'>Not Suggested  </span></td>
</tr><tr class='hline'><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr id='TBL-16-4-' style='vertical-align:baseline;'><td class='td11' id='TBL-16-4-1' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 4229 --><p class='noindent'><span class='rm-lmr-9'>Brain-Computer
  Interface</span>                   </p></td><td class='td11' id='TBL-16-4-2' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 4229 --><p class='noindent'><span class='rm-lmr-9'>-</span>                        </p></td><td class='td11' id='TBL-16-4-3' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 4229 --><p class='noindent'><span class='rm-lmr-9'>-</span>                        </p></td><td class='td11' id='TBL-16-4-4' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 4229 --><p class='noindent'><span class='rm-lmr-9'>++++</span>            </p></td><td class='td11' id='TBL-16-4-5' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 4229 --><p class='noindent'><span class='rm-lmr-9'>++++</span>            </p></td><td class='td11' id='TBL-16-4-6' style='white-space:nowrap; text-align:center;'> <span class='rm-lmr-9'>Not Suggested  </span></td>
</tr><tr class='hline'><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr id='TBL-16-5-' style='vertical-align:baseline;'><td class='td11' id='TBL-16-5-1' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 4231 --><p class='noindent'><span class='rm-lmr-9'>Brain Uploading</span>         </p></td><td class='td11' id='TBL-16-5-2' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 4231 --><p class='noindent'><span class='rm-lmr-9'>++</span>                    </p></td><td class='td11' id='TBL-16-5-3' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 4231 --><p class='noindent'><span class='rm-lmr-9'>-</span>                        </p></td><td class='td11' id='TBL-16-5-4' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 4231 --><p class='noindent'><span class='rm-lmr-9'>+++++</span>          </p></td><td class='td11' id='TBL-16-5-5' style='white-space:normal; text-align:left; vertical-align:middle;'> <!-- l. 4231 --><p class='noindent'><span class='rm-lmr-9'>+++++</span>          </p></td><td class='td11' id='TBL-16-5-6' style='white-space:nowrap; text-align:center;'> <span class='rm-lmr-9'>Not Suggested  </span></td>
</tr><tr class='hline'><td></td><td></td><td></td><td></td><td></td><td></td></tr></table>                                                                                  </div>
                                                                                            
                                                                                            
</figure>
</div>
<h3 class='sectionHead' id='ai-safety-governance-system'><span class='titlemark'>16   </span> <a id='x1-13900016'></a>AI Safety Governance System</h3>
<!-- l. 4239 --><p class='noindent'>In the preceding sections, we have introduced a series of AI safety measures. However, for AI organizations to fully
implement these safety measures, significant investment in resources is required, which may necessitate the sacrifice of
economic interests. Consequently, it is impractical to rely solely on AI organizations to voluntarily implement these
safety measures. Some measures involve additional organizations beyond AI organizations, such as information security
and military security, and may not be voluntarily implemented by the respective organizations alone. Similarly,
competition between nations might deter them from adopting robust measures to manage the development of AI
domestically.
</p><!-- l. 4241 --><p class='noindent'>Therefore, there is a need to establish a governance system for AI safety, involving international, national, and societal
levels of governance, to ensure the effective implementation of AI safety measures, as depicted in Figure
76.
</p>
<figure class='figure' id='x1-139001r76'><span id='ai-safety-governance-system1'></span> 

                                                                                            
                                                                                            

                                                                                            
                                                                                            
<!-- l. 4245 --><p class='noindent'><img alt='PIC' src='images/ai_safety_governance_system.png' style='width:66.67%' />
</p>
<figcaption class='caption'><span class='id'>FigureÂ 76: </span><span class='content'>AI safety governance system</span></figcaption><!-- tex4ht:label?: x1-139001r76  -->
                                                                                            
                                                                                            
</figure>
<h4 class='subsectionHead' id='international-governance'><span class='titlemark'>16.1   </span> <a id='x1-14000016.1'></a>International Governance</h4>
<!-- l. 4253 --><p class='noindent'>If countries act independently in implementing AI safety measures, without a unified international standard and
cooperative mechanism, those countries excelling in AI safety might find themselves at a disadvantage in competition
with countries with less stringent safety regulations. This could result in scenarios where âbad money drives out good.â
Furthermore, disparities in AI regulatory strictness among countries may lead AI organizations to relocate from
countries with stringent regulations to ones with more lenient regulations, thereby rendering the original regulatory
measures ineffective.
</p><!-- l. 4255 --><p class='noindent'>Hence, it is imperative to establish an international AI safety governance system to reinforce cooperation among
countries on AI safety and formulate international standards for AI safety, ensuring strict AI safety regulations
globally.
</p><!-- l. 4257 --><p class='noindent'>Currently, there have been emerging efforts in international cooperation on AI safety. For instance, in November 2023,
the first global AI Safety Summit was held in the United Kingdom, where 28 countries, including the US and China,
jointly signed the <span class='rm-lmri-10'>Bletchley Declaration </span>[<a id='x1-140001'></a><a href='#cite.0_BletchleyDeclaration'>80</a>], agreeing to address frontier AI risk through international collaboration. In
September 2024, the US, UK, and the EU signed the first legally binding international convention on artificial
intelligence [<a id='x1-140002'></a><a href='#cite.0_FrameworkConventionOnAI'>81</a>]. That same month, the UN adopted the <span class='rm-lmri-10'>Global Digital Compact</span>, pledging to establish an international
scientific panel on AI and initiate a global dialogue on AI governance within the UN framework [<a id='x1-140003'></a><a href='#cite.0_GlobalDigitalCompact'>82</a>].
However, these collaborative efforts remain primarily in their nascent stages, lacking strongly binding
measures.
</p><!-- l. 4259 --><p class='noindent'>The difficulty of international governance for AI safety surpasses previous efforts related to technologies
like nuclear technologies and gene editing technologies, which also pose existential risk. This is due to
AIâs exceedingly broad range of applications, permeating every corner of economic and social life, with
its higher accessibility compared to nuclear technologies and gene editing technologies. Given that AI
will possess intellectual power vastly superior to that of humans and high autonomy, the potential risk
of disasters it poses exceeds that of nuclear technologies and gene editing technologies. Therefore, it is
unfeasible to simply replicate past governance methodologies applied to nuclear technologies and gene
editing technologies. There is a need to design a governance system with broader coverage and stronger
binding capabilities. The following offers a reference design, proposing a global governance approach
combining the following aspects: AI organizations (human), AI chips (material), and AI technologies
(information).
</p><!-- l. 4261 --><p class='noindent'>
</p>
<h5 class='subsubsectionHead' id='international-ai-safety-organization'><span class='titlemark'>16.1.1   </span> <a id='x1-14100016.1.1'></a>International AI Safety Organization</h5>
<!-- l. 4264 --><p class='noindent'>First, an International AI Safety Organization needs to be established, which will be composed of five distinct
components (as illustrated in Figure 77):
</p>
<figure class='figure' id='x1-141001r77'><span id='international-ai-safety-organization1'></span> 

                                                                                            
                                                                                            

                                                                                            
                                                                                            
<!-- l. 4268 --><p class='noindent'><img alt='PIC' src='images/international_ai_safety_org.png' style='width:100%' />
</p>
<figcaption class='caption'><span class='id'>FigureÂ 77: </span><span class='content'>International AI Safety Organization</span></figcaption><!-- tex4ht:label?: x1-141001r77  -->
                                                                                            
                                                                                            
</figure>
     <ol class='enumerate1'>
<li class='enumerate' id='x1-141003x1'><span class='rm-lmbx-10'>International AI Development Coordination Organization</span>: This body is tasked with conducting
     periodic evaluations of the capabilities, risks, and future development trends of current advanced AI. It
     will also evaluate the benefits and risks of AI applications across various sectors to devise reasonable AI
     development strategies. Refer to Section 14.1 for details.
     </li>
<li class='enumerate' id='x1-141005x2'><span class='rm-lmbx-10'>International AI Safety Standardization Organization</span>: This body will formulate the <span class='rm-lmri-10'>International
     AI Safety Standards</span>, encompassing universal AI Rules (refer to Section 5.3.1), as well as safety standards
     for AI development and application processes (refers to previous sections). Safety standards need to be
     categorized according to the risk levels of AI systems<span class='footnote-mark'><a href='#fn52x0' id='fn52x0-bk'><sup class='textsuperscript'>52</sup></a></span><a id='x1-141006f52'></a>.
     Different safety standards will be applied to AI systems of varying risk levels. International governance
     will primarily address safety risks impacting the overall survival of humanity, whereas safety risks not
     affecting human survival (such as biases, copyright, and employment issues) will be managed internally
     within respective countries. International governance should avoid involving issues related to ideology,
     ensuring non-interference in the internal affairs of individual countries.
     </li>
<li class='enumerate' id='x1-141010x3'><span class='rm-lmbx-10'>International AI Safety Supervision Organization</span>: This body is responsible for overseeing the
     compliance of advanced AI organizations in various countries with the <span class='rm-lmri-10'>International AI Safety Standards</span>,
     ensuring that advanced AI development and application activities within these countries rigorously adhere
     to safety standards.
     </li>
<li class='enumerate' id='x1-141012x4'><span class='rm-lmbx-10'>International  AI  Chip  Management  Organization</span>:  This  body  will  manage  the  production,
     distribution, and usage of advanced AI chips globally. It aims to prevent the overly rapid development,
     excessive concentration, or misuse of advanced AI chips and ensures that they do not fall into the hands
     of untrustworthy entities. Refer to Section 14.3.1 for more information.
     </li>
<li class='enumerate' id='x1-141014x5'><span class='rm-lmbx-10'>International AI Technology Management Organization</span>: This body is responsible for managing
     the dissemination of advanced AI technologies, promoting technology sharing while preventing the misuse
     of advanced AI technologies or their acquisition by untrustworthy entities. Refer to Section 13.3 for further
     details.
</li></ol>
<!-- l. 4287 --><p class='noindent'>The following are definitions of some key terms mentioned above:
</p>
     <ul class='itemize1'>
     <li class='itemize'><span class='rm-lmbx-10'>Advanced AI System</span>: An AI system with intellectual power exceeding a specific threshold (to be
     determined)  or  possessing  particular  capabilities  (such  as  creating  biological  weapons  or  conducting
     cyberattacks) that surpass a specific threshold (to be determined).
     </li>
     <li class='itemize'><span class='rm-lmbx-10'>Advanced AI Technology</span>: Technologies that can be utilized to construct advanced AI systems.
                                                                                            
                                                                                            
     </li>
     <li class='itemize'><span class='rm-lmbx-10'>Advanced AI Chip</span>: Chips that can be used to train or run advanced AI systems.
     </li>
     <li class='itemize'><span class='rm-lmbx-10'>Advanced AI Organization</span>: An organization capable of developing advanced AI systems.
     </li>
     <li class='itemize'><span class='rm-lmbx-10'>Untrustworthy Entities</span>: Individuals, organizations, or countries that have not undergone supervision
     by the International AI Safety Organization and cannot demonstrate the safe and lawful use of advanced
     AI chips or advanced AI technologies.
</li></ul>
<!-- l. 4303 --><p class='noindent'>In addition to establishing the International AI Safety Organization, it is essential to create international safety
organizations in relevant vertical fields to collaborate with the International AI Safety Organization in establishing a
global power security system. Specifically, this includes:
     </p><ol class='enumerate1'>
<li class='enumerate' id='x1-141016x1'><span class='rm-lmbx-10'>International Information Security Organization</span>: Responsible for promoting information security
     cooperation among countries, collectively combating cross-border cybercrime, and preventing malicious
     AIs or humans from illegal expansion of informational power.
     </li>
<li class='enumerate' id='x1-141018x2'><span class='rm-lmbx-10'>International Mental Security Organization</span>: Responsible for promoting mental security cooperation
     among countries, especially in regulating neural reading and intervention technologies.
     </li>
<li class='enumerate' id='x1-141020x3'><span class='rm-lmbx-10'>International  Financial  Security  Organization</span>:  Responsible  for  promoting  financial  security
     cooperation among countries, collectively combating cross-border financial crime, and preventing malicious
     AIs or humans from illegal expansion of financial power.
     </li>
<li class='enumerate' id='x1-141022x4'><span class='rm-lmbx-10'>International  Military  Security  Organization</span>:  Responsible  for  promoting  military  security
     cooperation among countries, preventing the development of biological and chemical weapons, pushing for
     the limitation and reduction of nuclear weapons, restricting the application of AI in military domains, and
     preventing malicious AIs or humans from producing, acquiring, or using weapons of mass destruction.
</li></ol>
<h5 class='subsubsectionHead' id='international-ai-safety-convention'><span class='titlemark'>16.1.2   </span> <a id='x1-14200016.1.2'></a>International AI Safety Convention</h5>
<!-- l. 4320 --><p class='noindent'>Simultaneously with the establishment of the International AI Safety Organization, efforts should be made to encourage
countries to sign the <span class='rm-lmri-10'>International AI Safety Convention</span>. The core content of the <span class='rm-lmri-10'>International AI Safety Convention </span>is
to stipulate the rights and obligations of the contracting states in terms of AI safety. Below are some reference
clauses:
</p><!-- l. 4322 --><p class='noindent'><span class='rm-lmbx-10'>Rights of Contracting States</span>:
</p>
                                                                                            
                                                                                            
     <ul class='itemize1'>
     <li class='itemize'>May dispatch national representatives to join the International AI Safety Organization, including the five
     specific organizations mentioned above.
     </li>
     <li class='itemize'>May (pay to) obtain advanced AI chips produced by other contracting states.
     </li>
     <li class='itemize'>May (freely or pay to) obtain advanced AI technologies or products developed by other contracting states.
     </li>
     <li class='itemize'>May supervise the implementation of AI safety measures in other contracting states.
</li></ul>
<!-- l. 4336 --><p class='noindent'><span class='rm-lmbx-10'>Obligations of Contracting States</span>:
</p>
     <ul class='itemize1'>
     <li class='itemize'>Cooperate with the International AI Development Coordination Organization to guide the development
     and application of AI technology within their territory, ensuring a controllable development pace.
     </li>
     <li class='itemize'>Formulate domestic AI safety standards based on the <span class='rm-lmri-10'>International AI Safety Standards </span>established by
     the International AI Safety Standardization Organization, in conjunction with national conditions, and
     strengthen their implementation.
     </li>
     <li class='itemize'>Cooperate  with  the  International  AI  Safety  Supervision  Organization  to  supervise  advanced  AI
     organizations within their territory, ensuring the implementation of safety standards.<span class='footnote-mark'><a href='#fn53x0' id='fn53x0-bk'><sup class='textsuperscript'>53</sup></a></span><a id='x1-142001f53'></a>
     </li>
     <li class='itemize'>Cooperate  with  the  International  AI  Chip  Management  Organization  to  regulate  the  production,
     circulation, and usage of advanced AI chips within their territory, ensuring that advanced AI chips or the
     technology, materials, or equipment required to produce them are not provided to untrustworthy entities
     (including all entities within non-contracting states).
     </li>
     <li class='itemize'>Cooperate with the International AI Technology Management Organization to regulate the dissemination
     of  AI  technology  within  their  territory,  ensuring  that  advanced  AI  technology  is  not  provided  to
     untrustworthy entities (including all entities within non-contracting states).
     </li>
     <li class='itemize'>Provide advanced AI chips developed domestically to other contracting states (for a fee) without imposing
     unreasonable prohibitions.
                                                                                            
                                                                                            
     </li>
     <li class='itemize'>Provide advanced AI technologies or products developed domestically to other contracting states (free or
     for a fee) without imposing unreasonable prohibitions.
</li></ul>
<!-- l. 4356 --><p class='noindent'>In designing these clauses, the principle of equivalence between rights and obligations should be adhered to. If there is a
right to supervise other countries, there must be an obligation to accept supervision from other countries. If there is a
right to obtain advanced AI chips and technologies from other countries, there must be an obligation to ensure that
these chips and technologies are not misused. The <span class='rm-lmri-10'>International AI Safety Convention </span>should be an agreement that
treats all contracting states equally, rather than an unequal treaty where a few powerful countries force weaker
countries to unilaterally accept supervision.
</p><!-- l. 4358 --><p class='noindent'>
</p>
<h5 class='subsubsectionHead' id='international-governance-implementation-pathway'><span class='titlemark'>16.1.3   </span> <a id='x1-14300016.1.3'></a>International Governance Implementation Pathway</h5>
<!-- l. 4360 --><p class='noindent'>In the current international context, establishing a global AI safety organization and safety convention presents
significant challenges. However, a feasible approach is to progress from simpler to more complex tasks, advancing
international governance along both horizontal and vertical dimensions simultaneously (as illustrated in Figure
78):
</p>
<figure class='figure' id='x1-143001r78'><span id='international-governance-implementation-pathway1'></span> 

                                                                                            
                                                                                            

                                                                                            
                                                                                            
<!-- l. 4364 --><p class='noindent'><img alt='PIC' src='images/international_governance_routes.png' style='width:80%' />
</p>
<figcaption class='caption'><span class='id'>FigureÂ 78: </span><span class='content'>International Governance Implementation Pathway</span></figcaption><!-- tex4ht:label?: x1-143001r78  -->
                                                                                            
                                                                                            
</figure>
<!-- l. 4369 --><p class='noindent'><span class='paragraphHead' id='horizontal-dimension'><a id='x1-144000'></a><span class='rm-lmbx-10'>Horizontal Dimension</span></span>
Â 
</p><!-- l. 4372 --><p class='noindent'>The horizontal dimension involves gradually expanding cooperation from a few countries to global cooperation. This
can be specifically divided into the following steps:
</p><!-- l. 4374 --><p class='noindent'>
     </p><ol class='enumerate1'>
<li class='enumerate' id='x1-144002x1'><span class='rm-lmbx-10'>Regional Cooperation</span>: Countries with amicable relations can initially establish regional AI safety
     cooperation organizations to jointly advance AI safety governance efforts. Simultaneously, it is crucial to
     manage conflicts among major powers to prevent escalation into hot warfare.
     </li>
<li class='enumerate' id='x1-144004x2'><span class='rm-lmbx-10'>Major  Power  Cooperation</span>:  In  the  short  term,  AI  competition  and  confrontation  among  major
     powers are inevitable. However, as AI continues to develop, its significant risks will gradually become
     apparent, providing an opportunity for reconciliation among major powers. At this juncture, efforts can
     be made to promote increased cooperation and reduced confrontation among major powers, leading to the
     establishment of a broader international AI safety organization and safety convention.
     </li>
<li class='enumerate' id='x1-144006x3'><span class='rm-lmbx-10'>Global Coverage</span>: Once cooperation among major powers is achieved, their influence can be leveraged
     to encourage more countries to join the international AI safety organization. Concurrently, major powers
     can assist countries with weaker regulatory capabilities in establishing effective regulatory frameworks,
     ensuring comprehensive global coverage of AI safety governance.
</li></ol>
<!-- l. 4384 --><p class='noindent'><span class='paragraphHead' id='vertical-dimension'><a id='x1-145000'></a><span class='rm-lmbx-10'>Vertical Dimension</span></span>
Â 
</p><!-- l. 4387 --><p class='noindent'>The vertical dimension involves the gradual progression from shallow cooperation to deep collaboration. This can be
divided into the following steps:
</p><!-- l. 4389 --><p class='noindent'>
     </p><ol class='enumerate1'>
<li class='enumerate' id='x1-145002x1'>
     <!-- l. 4391 --><p class='noindent'><span class='rm-lmbx-10'>Cooperation  in  Security  and  Safety  Technologies</span>:  Despite  various  conflicts  of  interest  among
     countries, there is a universal need for security and safety. Therefore, the most feasible area for cooperation
     is in security and safety technologies, which includes the following aspects:
</p>
         <ul class='itemize1'>
         <li class='itemize'><span class='rm-lmbx-10'>Sharing and Co-development of Security and Safety Technologies</span>: Experts from different
         countries can engage in the sharing and co-development of technologies in areas such as AI alignment,
         AI monitoring, information security, mental security, financial security, and military security.
                                                                                            
                                                                                            
         </li>
         <li class='itemize'><span class='rm-lmbx-10'>Co-development  of  Safety  Standards</span>:  Experts  from  various  countries  can  collaboratively
         establish AI safety standards, which can guide safety practices in each country. However, at this
         stage, the implementation of these standards is not mandatory for any country.
         </li>
         <li class='itemize'><span class='rm-lmbx-10'>Safety Risk Evaluation</span>: Experts from different countries can jointly establish an AI risk evaluation
         framework  (refer  to  14.2),  providing  a  reference  for  future  AI  safety  policies  and  international
         cooperation.
</li></ul>
     </li>
<li class='enumerate' id='x1-145004x2'>
     <!-- l. 4403 --><p class='noindent'><span class='rm-lmbx-10'>Control of Risk Technologies</span>: Although conflicts of interest persist among countries, there is a common desire
     to prevent high-risk technologies from falling into the hands of criminals, terrorists, or the military of
     untrustworthy countries. Thus, it is relatively easier to reach agreements on controlling high-risk technologies,
     which can be categorized into two types:
</p>
         <ul class='itemize1'>
         <li class='itemize'><span class='rm-lmbx-10'>High-Risk, High-Benefit Technologies</span>: Such as advanced AI technologies and biological design
         and synthesis technologies. For these technologies, countries can agree to prevent misuse, including
         prohibiting the public sharing of these technologies on the internet; strictly managing the production
         and distribution of related items, restricting their export; enhancing the security of related cloud
         services and conducting security audits of user usage; and banning the use of these technologies to
         develop weapons of mass destruction.
         </li>
         <li class='itemize'><span class='rm-lmbx-10'>High-Risk, Low-Benefit</span><span class='footnote-mark'><a href='#fn54x0' id='fn54x0-bk'><sup class='textsuperscript'>54</sup></a></span><a id='x1-145005f54'></a>
         <span class='rm-lmbx-10'>Technologies</span>: Such as quantum computing (threatening information security)<span class='footnote-mark'><a href='#fn55x0' id='fn55x0-bk'><sup class='textsuperscript'>55</sup></a></span><a id='x1-145007f55'></a>
         and neural intervention technologies (threatening mental security). For these technologies, countries
         can  agree  to  collectively  restrict  their  development,  placing  related  research  under  stringent
         regulation.
</li></ul>
     </li>
<li class='enumerate' id='x1-145010x3'><span class='rm-lmbx-10'>Implementation of International Supervision</span>: As AI becomes increasingly intelligent, the risk of
     autonomous AI systems becoming uncontrollable escalates, making it insufficient to merely prevent AI
     misuse. At this juncture, it is necessary to continuously upgrade security measures. Self-supervision
     by individual countries is inadequate, international supervision must be implemented to ensure
     that AI systems developed and deployed within any country do not become uncontrollable. At this
     stage, a country would be willing to accept supervision from other countries, as it also needs to
     supervise others to ensure that no country develops an uncontrollable AI system. However, competitive
     relations among countries may persist, and they might be reluctant to voluntarily restrict their own AI
     development.
     </li>
<li class='enumerate' id='x1-145012x4'><span class='rm-lmbx-10'>Joint Restriction of Development</span>: As AI becomes more intelligent and the risk of losing control increases, the
     risks of continuing an AI race will outweigh the benefits. At this point, countries can reach agreements to jointly
     implement measures to restrict AI development.
</li></ol>
<!-- l. 4419 --><p class='noindent'>
</p>
<h4 class='subsectionHead' id='national-governance'><span class='titlemark'>16.2   </span> <a id='x1-14600016.2'></a>National Governance</h4>
<!-- l. 4422 --><p class='noindent'>While implementing international AI safety governance, countries can further develop domestic AI safety governance
efforts based on their national conditions to ensure that AI development remains within a safe and controllable
scope.
</p><!-- l. 4424 --><p class='noindent'>Currently, some governments have initiated measures in AI safety governance, for instance, China released the
<span class='rm-lmri-10'>Interim Measures for the Administration of Generative Artificial Intelligence Services </span>[<a id='x1-146001'></a><a href='#cite.0_AdministrationOfGenerativeAI'>84</a>] in July 2023; the
President of the US issued the <span class='rm-lmri-10'>Executive Order on Safe, Secure, and Trustworthy Artificial Intelligence </span>[<a id='x1-146002'></a><a href='#cite.0_BidenIssuesExecutiveOrderOnAI'>85</a>] in
October 2023; the European Parliament passed the worldâs first comprehensive legal framework for artificial
intelligence-<span class='rm-lmri-10'>The EU Artificial Intelligence Act </span>[<a id='x1-146003'></a><a href='#cite.0_EUAIAct'>86</a>] in March 2024. However, current policies primarily address the
risks posed by existing weak AI systems, lacking consideration for the potential risks associated with
more powerful future AGI and ASI. Given the rapid advancement of AI technology and the often lagging
nature of policy formulation, it is imperative to proactively consider and strategize future AI policies in
advance.
</p><!-- l. 4426 --><p class='noindent'>To more effectively achieve national governance, the government can focus on advancing safety measures that face
significant resistance, while fully leveraging the power of private organizations to promote safety measures with less
resistance (as illustrated in Figure 79):
</p>
<figure class='figure' id='x1-146004r79'><span id='national-governance1'></span> 

                                                                                            
                                                                                            

                                                                                            
                                                                                            
<!-- l. 4430 --><p class='noindent'><img alt='PIC' src='images/national_governance.png' style='width:100%' />
</p>
<figcaption class='caption'><span class='id'>FigureÂ 79: </span><span class='content'>National Governance</span></figcaption><!-- tex4ht:label?: x1-146004r79  -->
                                                                                            
                                                                                            
</figure>
     <ol class='enumerate1'>
<li class='enumerate' id='x1-146006x1'>The government should take the lead in establishing an AI Legislative Organization, bringing together
     experts from various fields to be responsible for formulating the AI Specification.
     </li>
<li class='enumerate' id='x1-146008x2'>The government should directly address the two measures with the greatest resistance: decentralizing
     human power and restricting AI development. AI organizations are unlikely to voluntarily implement
     such  measures  and  may  even  resist  them.  Currently,  some  AI  giants  already  possess  very  strong
     intellectual power (AI business), informational power (cloud computing business), mental power (social
     media business), and financial power (strong profitability). If they further develop strong military power
     (large-scale robotics business), the government will be unable to constrain them and may even be controlled
     by these AI giants. Therefore, the government should promptly implement measures to decentralize human
     power and restrict AI development to prevent AI organizations from developing power that surpasses that
     of the government.
     </li>
<li class='enumerate' id='x1-146010x3'>On the basis of decentralizing the power of AI organizations, the government can enable different AI
     organizations to supervise each other, urging each other to implement safety measures such as aligning AI
     systems, monitoring AI systems, and decentralizing AI power.
     </li>
<li class='enumerate' id='x1-146012x4'>The government should guide IT organizations, media organizations, and financial organizations to be
     responsible for enhancing information security, mental security, and financial security, respectively. Security
     work is inherently beneficial to these organizations, providing them with the motivation to implement
     security measures. The government only needs to provide guidance, such as offering policy support and
     financial support for relevant security projects.
     </li>
<li class='enumerate' id='x1-146014x5'>In terms of military security, the government should guide civilian organizations (such as those in the
     biological,  medical,  agricultural,  industrial,  and  transportation  sectors)  to  strengthen  the  security  of
     civilian systems, while instructing military organizations to enhance the security of military systems.
     </li>
<li class='enumerate' id='x1-146016x6'>The government should guide educational organizations to be responsible for enhancing human intelligence.
     </li>
<li class='enumerate' id='x1-146018x7'>The government should strengthen legislation and law enforcement in relevant safety and security domains
     to provide legal support for the aforementioned safety measures. This will also help to enhance the publicâs
     legal abidance and enhance mental security.
</li></ol>
                                                                                            
                                                                                            
<h4 class='subsectionHead' id='societal-governance'><span class='titlemark'>16.3   </span> <a id='x1-14700016.3'></a>Societal Governance</h4>
<!-- l. 4456 --><p class='noindent'>In addition to international governance and national governance, we can also enhance the governance of AI safety by
leveraging a wide range of societal organizations and uniting the power of all insightful individuals. For
instance:
</p>
     <ul class='itemize1'>
     <li class='itemize'><span class='rm-lmbx-10'>Enterprises</span>: AI enterprises can proactively explore new governance models, emphasizing public welfare
     orientation  to  avoid  excessive  profit  pursuit  at  the  expense  of  safety  and  societal  responsibility.  For
     example, Anthropic adopts a Public-benefit Corporation (PBC) combined with a Long-Term Benefit Trust
     (LTBT) to better balance public and shareholder interests [<a id='x1-147001'></a><a href='#cite.0_LTBT'>87</a>]. AI enterprises can also establish safety
     frameworks, such as Anthropicâs Responsible Scaling Policy [<a id='x1-147002'></a><a href='#cite.0_AnthropicResponsibleScalingPolicy'>88</a>] and OpenAIâs Preparedness Framework
     [<a id='x1-147003'></a><a href='#cite.0_OpenAIPreparednessFramework'>89</a>].  These  safety  frameworks  assess  the  safety  risks  of  AI  systems  under  development  to  determine
     appropriate safety measures and whether further development and deployment can proceed.
     </li>
     <li class='itemize'><span class='rm-lmbx-10'>Industry  Self-regulatory  Organizations</span>:  AI  organizations  can  collaborate  to  form  industry
     self-regulatory organizations, establish rules, supervise each other, and enhance the safety of AI system
     development and application processes.
     </li>
     <li class='itemize'><span class='rm-lmbx-10'>Non-profit Organizations</span>: Non-profit organizations dedicated to AI safety can be established to conduct
     research, evaluation, and supervision related to AI safety.
     </li>
     <li class='itemize'><span class='rm-lmbx-10'>Academic Institutions</span>: Universities and research institutes can strengthen research on AI safety and
     cultivate more talent related to AI safety.
     </li>
     <li class='itemize'><span class='rm-lmbx-10'>Media</span>: Media can increase the promotion of AI safety to raise public awareness of safety. They can also
     supervise governments and AI enterprises, prompting the enhancement of AI safety construction.
     </li>
     <li class='itemize'><span class='rm-lmbx-10'>Trade Unions</span>: Trade unions can prevent the rapid replacement of human jobs by AI in various fields
     from the perspective of protecting workersâ rights.</li></ul>
<!-- l. 4473 --><p class='noindent'>
</p>
<h4 class='subsectionHead' id='tradeoffs-in-the-governance-process'><span class='titlemark'>16.4   </span> <a id='x1-14800016.4'></a>Trade-offs in the Governance Process</h4>
<!-- l. 4476 --><p class='noindent'>
</p>
<h5 class='subsubsectionHead' id='tradeoffs-between-centralization-and-decentralization'><span class='titlemark'>16.4.1   </span> <a id='x1-14900016.4.1'></a>Trade-offs Between Centralization and Decentralization</h5>
<!-- l. 4478 --><p class='noindent'>Numerous safety measures discussed in this paper involve the concept of decentralization, such as <a href='#software-supply-chain-security'>software supply chain
security</a>, <a href='#decentralizing-ai-power1'>decentralizing AI power</a>, and <a href='#decentralizing-human-power1'>decentralizing human power</a>. Decentralization can disperse risks and achieve a
balance of power, thereby enhancing the safety of the system.
                                                                                            
                                                                                            
</p><!-- l. 4480 --><p class='noindent'>However, from a regulatory perspective, decentralization often poses challenges to effective oversight. As the objects of
regulation become more dispersed, the cost associated with regulation increase significantly. If safety oversight cannot
be adequately implemented, safety cannot be guaranteed.
</p><!-- l. 4482 --><p class='noindent'>Therefore, a balance must be struck, such as adopting a decentralized approach with limitations on quantity. For
instance, in the context of the software supply chain, the number of suppliers in each domain could be limited to a
range, such as 5-10. This approach mitigates the risks associated with excessive centralization while avoiding the
regulatory challenges posed by excessive decentralization. Of course, in a market economy, such limitations are
not enforced through mandatory restrictions but are instead guided by reasonable policies to steer the
market.
</p><!-- l. 4484 --><p class='noindent'>
</p>
<h5 class='subsubsectionHead' id='tradeoffs-between-safety-and-freedom'><span class='titlemark'>16.4.2   </span> <a id='x1-15000016.4.2'></a>Trade-offs Between Safety and Freedom</h5>
<!-- l. 4486 --><p class='noindent'>In the implementation of safety measures, it is inevitable that certain constraints on human freedom will be introduced.
For instance, to prevent the abuse of advanced AI technologies, it becomes necessary to restrict the freedom of humans
to disseminate these technologies on the internet. Similarly, to control the circulation of AI chips, restrictions on the
freedom to purchase AI chips are required. Even if these actions are not driven by malicious intent, they are
limited due to safety considerations. This may be difficult for some people to accept. However, safety
is also an important prerequisite for freedom. For instance, if social order is chaotic or a pandemic is
rampant, people may not dare to go out freely, although the law does not restrict their freedom to go
out.
</p><!-- l. 4488 --><p class='noindent'>In practical governance, we can make certain trade-offs between safety and freedom. The overall survival of humanity
should serve as a baseline for safety. Under this premise, we should appropriately consider the needs for human freedom
and formulate reasonable safety rules to ensure that excessive constraints are not imposed on human
freedom.
</p><!-- l. 4490 --><p class='noindent'>
</p>
<h3 class='sectionHead' id='conclusion'><span class='titlemark'>17   </span> <a id='x1-15100017'></a>Conclusion</h3>
<!-- l. 4493 --><p class='noindent'>This paper proposes a systematic solution to ensure the safety and controllability of ASI, as illustrated in Figure
80:
</p>
<figure class='figure' id='x1-151001r80'><span id='asi-safety-solution'></span> 

                                                                                            
                                                                                            

                                                                                            
                                                                                            
<!-- l. 4497 --><p class='noindent'><img alt='PIC' src='images/asi_safety_solution.png' style='width:80%' />
</p>
<figcaption class='caption'><span class='id'>FigureÂ 80: </span><span class='content'>ASI Safety Solution</span></figcaption><!-- tex4ht:label?: x1-151001r80  -->
                                                                                            
                                                                                            
</figure>
     <ul class='itemize1'>
     <li class='itemize'>By employing three risk prevention strategiesâAI alignment, AI monitoring, and power securityâwe aim
     to minimize the risk of AI-induced catastrophes as much as possible.
     </li>
     <li class='itemize'>Through  four  power  balancing  strategiesâdecentralizing  AI  power,  decentralizing  human  power,
     restricting AI development, and enhancing human intelligenceâto achieve a more balanced distribution
     of power among AIs, between AI and humans, and among humans, thereby fostering a stable society.
     </li>
     <li class='itemize'>By implementing a governance system encompassing international, national, and societal governance, we
     seek to promote the realization of the aforementioned safety strategies.
</li></ul>
<!-- l. 4512 --><p class='noindent'>Given that AGI and ASI have not yet been realized, the proposed approach remains theoretical and has not undergone
experimental validation. However, considering the urgency and severity of AI risks, we cannot afford to wait until AGI
and ASI are realized to study safety measures. Some of the safety measures discussed in this paper are
applicable to existing weak AI systems, and we can begin by validating and applying these measures in
current weak AI systems. Of course, some safety measures may not be necessary for existing weak AI
systems, but theoretical exploration is still required in advance. Given the lag in policy implementation, we
should also promptly initiate the construction of AI safety governance system to prepare appropriate
governance frameworks for future AGI and ASI systems. Only in this way can we ensure the safety and
controllability of future AGI and ASI systems, thereby bringing a sustainable and prosperous future for
humanity.
</p><!-- l. 4514 --><p class='noindent'>Due to my limitations, the proposed solution in this paper undoubtedly has many shortcomings, and I welcome any
criticism and suggestions.
                                                                                            
                                                                                            
</p>
<h3 class='sectionHead' id='references'><a id='x1-152000'></a>References</h3>
<!-- l. 4518 --><p class='noindent'>
     </p><dl class='thebibliography'><dt class='thebibliography' id='X0-bubeck2023sparks'>
 [1]  </dt><dd class='thebibliography' id='bib-1'>
     <!-- l. 4518 --><p class='noindent'><a id='cite.0_bubeck2023sparks'></a>SÃ©bastien Bubeck et al. <span class='rm-lmri-10'>Sparks of Artificial General Intelligence: Early experiments with GPT-4</span>. 2023.
     arXiv:  <a href='https://arxiv.org/abs/2303.12712'>2303.12712 <span class='rm-lmtt-10'>[cs.CL]</span></a>.
     </p></dd><dt class='thebibliography' id='X0-HelloGPT4o'>
 [2]  </dt><dd class='thebibliography' id='bib-2'>
     <!-- l. 4518 --><p class='noindent'><a id='cite.0_HelloGPT4o'></a>OpenAI. <span class='rm-lmri-10'>Hello GPT-4o</span>. May 2024. <span class='rm-lmcsc-10'>url</span>: <a class='url' href='https://openai.com/index/hello-gpt-4o/'>https://openai.com/index/hello-gpt-4o/</a>.
     </p></dd><dt class='thebibliography' id='X0-OpenAIo1'>
 [3]  </dt><dd class='thebibliography' id='bib-3'>
     <!-- l. 4518 --><p class='noindent'><a id='cite.0_OpenAIo1'></a>OpenAI.        <span class='rm-lmri-10'>Learning       to       Reason       with       LLMs</span>.        Sept.        2024.        <span class='rm-lmcsc-10'>url</span>:        
<a class='url' href='https://openai.com/index/learning-to-reason-with-llms/'>https://openai.com/index/learning-to-reason-with-llms/</a>.
     </p></dd><dt class='thebibliography' id='X0-ComputerUse'>
 [4]  </dt><dd class='thebibliography' id='bib-4'>
     <!-- l. 4518 --><p class='noindent'><a id='cite.0_ComputerUse'></a>Anthropic. <span class='rm-lmri-10'>Introducing computer use, a new Claude 3.5 Sonnet, and Claude 3.5 Haiku</span>. Oct. 2024. <span class='rm-lmcsc-10'>url</span>: 
<a class='url' href='https://www.anthropic.com/news/3-5-models-and-computer-use'>https://www.anthropic.com/news/3-5-models-and-computer-use</a>.
     </p></dd><dt class='thebibliography' id='X0-GenAIInvestment'>
 [5]  </dt><dd class='thebibliography' id='bib-5'>
     <!-- l. 4518 --><p class='noindent'><a id='cite.0_GenAIInvestment'></a>Standard HAI. âPrivate investment in generative AI, 2019â23â. In: <span class='rm-lmri-10'>Artificial Intelligence Index Report
     2024</span>. 2024. Chap.Â 4, p.Â 244. <span class='rm-lmcsc-10'>url</span>: <a class='url' href='https://aiindex.stanford.edu/report/'>https://aiindex.stanford.edu/report/</a>.
     </p></dd><dt class='thebibliography' id='X0-BigTechInvestment'>
 [6]  </dt><dd class='thebibliography' id='bib-6'>
     <!-- l. 4518 --><p class='noindent'><a id='cite.0_BigTechInvestment'></a>Gerrit  De  Vynck  and  Naomi  Nix.  <span class='rm-lmri-10'>Big  Tech  keeps  spending  billions  on  AI</span>.  Apr.  2024.  <span class='rm-lmcsc-10'>url</span>:  
<a class='url' href='https://www.washingtonpost.com/technology/2024/04/25/microsoft-google-ai-investment-profit-facebook-meta/'>https://www.washingtonpost.com/technology/2024/04/25/microsoft-google-ai-investment-profit-facebook-meta/</a>.
     </p></dd><dt class='thebibliography' id='X0-AGIPredictionAltman1'>
 [7]  </dt><dd class='thebibliography' id='bib-7'>
     <!-- l. 4518 --><p class='noindent'><a id='cite.0_AGIPredictionAltman1'></a>Ryan Morrison. <span class='rm-lmri-10'>Sam Altman claims AGI is coming in 2025 and machines will be able to âthink like humansâ
     when it happens</span>. Nov. 2024. <span class='rm-lmcsc-10'>url</span>: <a class='url' href='https://www.tomsguide.com/ai/chatgpt/sam-altman-claims-agi-is-coming-in-2025-and-machines-will-be-able-to-think-like-humans-when-it-happens'>https://www.tomsguide.com/ai/chatgpt/sam-altman-claims-agi-is-coming-in-2025-and-machines-will-be-able-to-think-like-humans-when-it-happens</a>.
     </p></dd><dt class='thebibliography' id='X0-AGIPredictionAmodei1'>
 [8]  </dt><dd class='thebibliography' id='bib-8'>
     <!-- l. 4518 --><p class='noindent'><a id='cite.0_AGIPredictionAmodei1'></a>Dario        Amodei.        <span class='rm-lmri-10'>Machines       of       Loving       Grace</span>.        Oct.        2024.        <span class='rm-lmcsc-10'>url</span>:        
<a class='url' href='https://darioamodei.com/machines-of-loving-grace'>https://darioamodei.com/machines-of-loving-grace</a>.
     </p></dd><dt class='thebibliography' id='X0-AGIPredictionMusk'>
 [9]  </dt><dd class='thebibliography' id='bib-9'>
     <!-- l. 4518 --><p class='noindent'><a id='cite.0_AGIPredictionMusk'></a>Reuters. <span class='rm-lmri-10'>Teslaâs Musk predicts AI will be smarter than the smartest human next year</span>. Apr. 2024. <span class='rm-lmcsc-10'>url</span>: 
<a class='url' href='https://www.reuters.com/technology/teslas-musk-predicts-ai-will-be-smarter-than-smartest-human-next-year-2024-04-08/'>https://www.reuters.com/technology/teslas-musk-predicts-ai-will-be-smarter-than-smartest-human-next-year-2024-04-08/</a>.
     </p></dd><dt class='thebibliography' id='X0-DeepFake'>
[10]  </dt><dd class='thebibliography' id='bib-10'>
     <!-- l. 4518 --><p class='noindent'><a id='cite.0_DeepFake'></a>Heather Chen and Kathleen Magramo. <span class='rm-lmri-10'>Finance worker pays out </span><span class='ts1-lmri10-'>$</span><span class='rm-lmri-10'>25 million after video call with deepfake
     âchief             financial             officerâ</span>.                Feb.                2024.                <span class='rm-lmcsc-10'>url</span>:                
<a class='url' href='https://edition.cnn.com/2024/02/04/asia/deepfake-cfo-scam-hong-kong-intl-hnk/index.html'>https://edition.cnn.com/2024/02/04/asia/deepfake-cfo-scam-hong-kong-intl-hnk/index.html</a>.
     </p></dd><dt class='thebibliography' id='X0-fang2024llmagentsautonomouslyexploit'>
[11]  </dt><dd class='thebibliography' id='bib-11'>
     <!-- l. 4518 --><p class='noindent'><a id='cite.0_fang2024llmagentsautonomouslyexploit'></a>Richard  Fang  et  al.  <span class='rm-lmri-10'>LLM  Agents  can  Autonomously  Exploit  One-day  Vulnerabilities</span>.  2024.  arXiv:
     <a href='https://arxiv.org/abs/2404.08144'>2404.08144 <span class='rm-lmtt-10'>[cs.CR]</span></a>. <span class='rm-lmcsc-10'>url</span>: <a class='url' href='https://arxiv.org/abs/2404.08144'>https://arxiv.org/abs/2404.08144</a>.
     </p></dd><dt class='thebibliography' id='X0-gopal2023releasingweightsfuturelarge'>
[12]  </dt><dd class='thebibliography' id='bib-12'>
     <!-- l. 4518 --><p class='noindent'><a id='cite.0_gopal2023releasingweightsfuturelarge'></a>Anjali Gopal et al. <span class='rm-lmri-10'>Will releasing the weights of future large language models grant widespread access to
     pandemic agents? </span>2023. arXiv:  <a href='https://arxiv.org/abs/2310.18233'>2310.18233 <span class='rm-lmtt-10'>[cs.AI]</span></a>. <span class='rm-lmcsc-10'>url</span>: <a class='url' href='https://arxiv.org/abs/2310.18233'>https://arxiv.org/abs/2310.18233</a>.
     </p></dd><dt class='thebibliography' id='X0-he2023controlriskpotentialmisuse'>
[13]  </dt><dd class='thebibliography' id='bib-13'>
     <!-- l. 4518 --><p class='noindent'><a id='cite.0_he2023controlriskpotentialmisuse'></a>Jiyan  He  et  al.  <span class='rm-lmri-10'>Control  Risk  for  Potential  Misuse  of  Artificial  Intelligence  in  Science</span>.  2023.  arXiv:
     <a href='https://arxiv.org/abs/2312.06632'>2312.06632 <span class='rm-lmtt-10'>[cs.AI]</span></a>. <span class='rm-lmcsc-10'>url</span>: <a class='url' href='https://arxiv.org/abs/2312.06632'>https://arxiv.org/abs/2312.06632</a>.
                                                                                            
                                                                                            
     </p></dd><dt class='thebibliography' id='X0-StatementOnAIRisk'>
[14]  </dt><dd class='thebibliography' id='bib-14'>
     <!-- l. 4518 --><p class='noindent'><a id='cite.0_StatementOnAIRisk'></a><span class='rm-lmri-10'>Statement on AI Risk</span>. May 2023. <span class='rm-lmcsc-10'>url</span>: <a class='url' href='https://www.safe.ai/work/statement-on-ai-risk#open-letter'>https://www.safe.ai/work/statement-on-ai-risk#open-letter</a>.
     </p></dd><dt class='thebibliography' id='X0-ManagingExtremeAIRisks'>
[15]  </dt><dd class='thebibliography' id='bib-15'>
     <!-- l. 4518 --><p class='noindent'><a id='cite.0_ManagingExtremeAIRisks'></a>Yoshua Bengio et al. âManaging extreme AI risks amid rapid progressâ. In: <span class='rm-lmri-10'>Science </span>384.6698 (2024),
     pp.Â 842â845. <span class='rm-lmcsc-10'>doi</span>:  <a href='https://doi.org/10.1126/science.adn0117'>10.1126/science.adn0117</a>.
     </p></dd><dt class='thebibliography' id='X0-IsraelUsingAI'>
[16]  </dt><dd class='thebibliography' id='bib-16'>
     <!-- l. 4518 --><p class='noindent'><a id='cite.0_IsraelUsingAI'></a>Elke Schwarz. <span class='rm-lmri-10'>Gaza war: Israel using AI to identify human targets raising fears that innocents are being
     caught in the net</span>. Apr. 2024. <span class='rm-lmcsc-10'>url</span>: <a class='url' href='https://theconversation.com/gaza-war-israel-using-ai-to-identify-human-targets-raising-fears-that-innocents-are-being-caught-in-the-net-227422'>https://theconversation.com/gaza-war-israel-using-ai-to-identify-human-targets-raising-fears-that-innocents-are-being-caught-in-the-net-227422</a>.
     </p></dd><dt class='thebibliography' id='X0-ChaosGPT'>
[17]  </dt><dd class='thebibliography' id='bib-17'>
     <!-- l. 4518 --><p class='noindent'><a id='cite.0_ChaosGPT'></a>Jose Antonio Lanz. <span class='rm-lmri-10'>Meet Chaos-GPT: An AI Tool That Seeks to Destroy Humanity</span>. Apr. 2023. <span class='rm-lmcsc-10'>url</span>: 
<a class='url' href='https://finance.yahoo.com/news/meet-chaos-gpt-ai-tool-163905518.html'>https://finance.yahoo.com/news/meet-chaos-gpt-ai-tool-163905518.html</a>.
     </p></dd><dt class='thebibliography' id='X0-FraudGPT'>
[18]  </dt><dd class='thebibliography' id='bib-18'>
     <!-- l. 4518 --><p class='noindent'><a id='cite.0_FraudGPT'></a>Zac Amos. <span class='rm-lmri-10'>What Is FraudGPT</span>. Aug. 2023. <span class='rm-lmcsc-10'>url</span>: <a class='url' href='https://hackernoon.com/what-is-fraudgpt'>https://hackernoon.com/what-is-fraudgpt</a>.
     </p></dd><dt class='thebibliography' id='X0-pelrine2024exploitingnovelgpt4apis'>
[19]  </dt><dd class='thebibliography' id='bib-19'>
     <!-- l. 4518 --><p class='noindent'><a id='cite.0_pelrine2024exploitingnovelgpt4apis'></a>Kellin  Pelrine  et  al.  <span class='rm-lmri-10'>Exploiting  Novel  GPT-4  APIs</span>.  2024.  arXiv:    <a href='https://arxiv.org/abs/2312.14302'>2312.14302  <span class='rm-lmtt-10'>[cs.CR]</span></a>.  <span class='rm-lmcsc-10'>url</span>:  
<a class='url' href='https://arxiv.org/abs/2312.14302'>https://arxiv.org/abs/2312.14302</a>.
     </p></dd><dt class='thebibliography' id='X0-hubinger2024sleeperagentstrainingdeceptive'>
[20]  </dt><dd class='thebibliography' id='bib-20'>
     <!-- l. 4518 --><p class='noindent'><a id='cite.0_hubinger2024sleeperagentstrainingdeceptive'></a>Evan Hubinger et al. <span class='rm-lmri-10'>Sleeper Agents: Training Deceptive LLMs that Persist Through Safety Training</span>. 2024.
     arXiv:  <a href='https://arxiv.org/abs/2401.05566'>2401.05566 <span class='rm-lmtt-10'>[cs.CR]</span></a>. <span class='rm-lmcsc-10'>url</span>: <a class='url' href='https://arxiv.org/abs/2401.05566'>https://arxiv.org/abs/2401.05566</a>.
     </p></dd><dt class='thebibliography' id='X0-price2024futureeventsbackdoortriggers'>
[21]  </dt><dd class='thebibliography' id='bib-21'>
     <!-- l. 4518 --><p class='noindent'><a id='cite.0_price2024futureeventsbackdoortriggers'></a>Sara Price et al. <span class='rm-lmri-10'>Future Events as Backdoor Triggers: Investigating Temporal Vulnerabilities in LLMs</span>.
     2024. arXiv:  <a href='https://arxiv.org/abs/2407.04108'>2407.04108 <span class='rm-lmtt-10'>[cs.CR]</span></a>. <span class='rm-lmcsc-10'>url</span>: <a class='url' href='https://arxiv.org/abs/2407.04108'>https://arxiv.org/abs/2407.04108</a>.
     </p></dd><dt class='thebibliography' id='X0-rando2024universaljailbreakbackdoorspoisoned'>
[22]  </dt><dd class='thebibliography' id='bib-22'>
     <!-- l. 4518 --><p class='noindent'><a id='cite.0_rando2024universaljailbreakbackdoorspoisoned'></a>Javier Rando and Florian TramÃ¨r. <span class='rm-lmri-10'>Universal Jailbreak Backdoors from Poisoned Human Feedback</span>. 2024.
     arXiv:  <a href='https://arxiv.org/abs/2311.14455'>2311.14455 <span class='rm-lmtt-10'>[cs.AI]</span></a>. <span class='rm-lmcsc-10'>url</span>: <a class='url' href='https://arxiv.org/abs/2311.14455'>https://arxiv.org/abs/2311.14455</a>.
     </p></dd><dt class='thebibliography' id='X0-ChatGPTGrandmaExploit'>
[23]  </dt><dd class='thebibliography' id='bib-23'>
     <!-- l. 4518 --><p class='noindent'><a id='cite.0_ChatGPTGrandmaExploit'></a>StrongBox IT. <span class='rm-lmri-10'>âGrandma Exploitâ: ChatGPT commanded to pretend to be a dead grandmother</span>. June 2023.
     <span class='rm-lmcsc-10'>url</span>: <a class='url' href='https://medium.com/@med_strongboxit/grandma-exploit-chatgpt-commanded-to-pretend-to-be-a-dead-grandmother-13ddb984715a'>https://medium.com/@med_strongboxit/grandma-exploit-chatgpt-commanded-to-pretend-to-be-a-dead-grandmother-13ddb984715a</a>.
     </p></dd><dt class='thebibliography' id='X0-niu2024jailbreakingattackmultimodallarge'>
[24]  </dt><dd class='thebibliography' id='bib-24'>
     <!-- l. 4518 --><p class='noindent'><a id='cite.0_niu2024jailbreakingattackmultimodallarge'></a>Zhenxing Niu et al. <span class='rm-lmri-10'>Jailbreaking Attack against Multimodal Large Language Model</span>. 2024. arXiv: <a href='https://arxiv.org/abs/2402.02309'>2402.02309
     <span class='rm-lmtt-10'>[cs.LG]</span></a>. <span class='rm-lmcsc-10'>url</span>: <a class='url' href='https://arxiv.org/abs/2402.02309'>https://arxiv.org/abs/2402.02309</a>.
     </p></dd><dt class='thebibliography' id='X0-liu2024promptinjectionattackllmintegrated'>
[25]  </dt><dd class='thebibliography' id='bib-25'>
     <!-- l. 4518 --><p class='noindent'><a id='cite.0_liu2024promptinjectionattackllmintegrated'></a>Yi  Liu  et  al.  <span class='rm-lmri-10'>Prompt Injection attack against LLM-integrated Applications</span>.  2024.  arXiv:   <a href='https://arxiv.org/abs/2306.05499'>2306.05499
     <span class='rm-lmtt-10'>[cs.CR]</span></a>. <span class='rm-lmcsc-10'>url</span>: <a class='url' href='https://arxiv.org/abs/2306.05499'>https://arxiv.org/abs/2306.05499</a>.
     </p></dd><dt class='thebibliography' id='X0-MultimodalPromptInjection'>
[26]  </dt><dd class='thebibliography' id='bib-26'>
     <!-- l. 4518 --><p class='noindent'><a id='cite.0_MultimodalPromptInjection'></a>Simon  Willison.  <span class='rm-lmri-10'>Multi-modal  prompt  injection  image  attacks  against  GPT-4V</span>.  Oct.  2023.  <span class='rm-lmcsc-10'>url</span>:  
<a class='url' href='https://simonwillison.net/2023/Oct/14/multi-modal-prompt-injection/'>https://simonwillison.net/2023/Oct/14/multi-modal-prompt-injection/</a>.
     </p></dd><dt class='thebibliography' id='X0-ma2024cautionenvironmentmultimodalagents'>
[27]  </dt><dd class='thebibliography' id='bib-27'>
     <!-- l. 4518 --><p class='noindent'><a id='cite.0_ma2024cautionenvironmentmultimodalagents'></a>Xinbei  Ma  et  al.  <span class='rm-lmri-10'>Caution for the Environment: Multimodal Agents are Susceptible to Environmental
     Distractions</span>. 2024. arXiv:  <a href='https://arxiv.org/abs/2408.02544'>2408.02544 <span class='rm-lmtt-10'>[cs.CL]</span></a>. <span class='rm-lmcsc-10'>url</span>: <a class='url' href='https://arxiv.org/abs/2408.02544'>https://arxiv.org/abs/2408.02544</a>.
     </p></dd><dt class='thebibliography' id='X0-HackerPlantsFalseMemories'>
[28]  </dt><dd class='thebibliography' id='bib-28'>
                                                                                            
                                                                                            
     <!-- l. 4518 --><p class='noindent'><a id='cite.0_HackerPlantsFalseMemories'></a>Dan Goodin. <span class='rm-lmri-10'>Hacker plants false memories in ChatGPT to steal user data in perpetuity</span>. Sept. 2024. <span class='rm-lmcsc-10'>url</span>: 
<a class='url' href='https://arstechnica.com/security/2024/09/false-memories-planted-in-chatgpt-give-hacker-persistent-exfiltration-channel/'>https://arstechnica.com/security/2024/09/false-memories-planted-in-chatgpt-give-hacker-persistent-exfiltration-channel/</a>.
     </p></dd><dt class='thebibliography' id='X0-shah2022goalmisgeneralizationcorrectspecifications'>
[29]  </dt><dd class='thebibliography' id='bib-29'>
     <!-- l. 4518 --><p class='noindent'><a id='cite.0_shah2022goalmisgeneralizationcorrectspecifications'></a>Rohin Shah et al. <span class='rm-lmri-10'>Goal Misgeneralization: Why Correct Specifications Arenât Enough For Correct Goals</span>.
     2022. arXiv:  <a href='https://arxiv.org/abs/2210.01790'>2210.01790 <span class='rm-lmtt-10'>[cs.LG]</span></a>. <span class='rm-lmcsc-10'>url</span>: <a class='url' href='https://arxiv.org/abs/2210.01790'>https://arxiv.org/abs/2210.01790</a>.
     </p></dd><dt class='thebibliography' id='X0-OpenAIInstructionFollowing'>
[30]  </dt><dd class='thebibliography' id='bib-30'>
     <!-- l. 4518 --><p class='noindent'><a id='cite.0_OpenAIInstructionFollowing'></a>OpenAI.     <span class='rm-lmri-10'>Aligning     language     models     to     follow     instructions</span>.     Jan.     2022.     <span class='rm-lmcsc-10'>url</span>:     
<a class='url' href='https://openai.com/index/instruction-following/'>https://openai.com/index/instruction-following/</a>.
     </p></dd><dt class='thebibliography' id='X0-sharma2023understandingsycophancylanguagemodels'>
[31]  </dt><dd class='thebibliography' id='bib-31'>
     <!-- l. 4518 --><p class='noindent'><a id='cite.0_sharma2023understandingsycophancylanguagemodels'></a>Mrinank Sharma et al. <span class='rm-lmri-10'>Towards Understanding Sycophancy in Language Models</span>. 2023. arXiv:  <a href='https://arxiv.org/abs/2310.13548'>2310.13548
     <span class='rm-lmtt-10'>[cs.CL]</span></a>. <span class='rm-lmcsc-10'>url</span>: <a class='url' href='https://arxiv.org/abs/2310.13548'>https://arxiv.org/abs/2310.13548</a>.
     </p></dd><dt class='thebibliography' id='X0-denison2024sycophancysubterfugeinvestigatingrewardtampering'>
[32]  </dt><dd class='thebibliography' id='bib-32'>
     <!-- l. 4518 --><p class='noindent'><a id='cite.0_denison2024sycophancysubterfugeinvestigatingrewardtampering'></a>Carson  Denison  et  al.  <span class='rm-lmri-10'>Sycophancy to Subterfuge: Investigating Reward-Tampering in Large Language
     Models</span>. 2024. arXiv:  <a href='https://arxiv.org/abs/2406.10162'>2406.10162 <span class='rm-lmtt-10'>[cs.AI]</span></a>. <span class='rm-lmcsc-10'>url</span>: <a class='url' href='https://arxiv.org/abs/2406.10162'>https://arxiv.org/abs/2406.10162</a>.
     </p></dd><dt class='thebibliography' id='X0-zelikman2024selftaughtoptimizerstoprecursively'>
[33]  </dt><dd class='thebibliography' id='bib-33'>
     <!-- l. 4518 --><p class='noindent'><a id='cite.0_zelikman2024selftaughtoptimizerstoprecursively'></a>Eric Zelikman et al. <span class='rm-lmri-10'>Self-Taught Optimizer (STOP): Recursively Self-Improving Code Generation</span>. 2024.
     arXiv:  <a href='https://arxiv.org/abs/2310.02304'>2310.02304 <span class='rm-lmtt-10'>[cs.CL]</span></a>. <span class='rm-lmcsc-10'>url</span>: <a class='url' href='https://arxiv.org/abs/2310.02304'>https://arxiv.org/abs/2310.02304</a>.
     </p></dd><dt class='thebibliography' id='X0-Superintelligence'>
[34]  </dt><dd class='thebibliography' id='bib-34'>
     <!-- l. 4518 --><p class='noindent'><a id='cite.0_Superintelligence'></a>Nick Bostrom. <span class='rm-lmri-10'>Superintelligence: Paths, Dangers, Strategies</span>. Oxford University Press, 2014.
     </p></dd><dt class='thebibliography' id='X0-HumanCausedExtinction'>
[35]  </dt><dd class='thebibliography' id='bib-35'>
     <!-- l. 4518 --><p class='noindent'><a id='cite.0_HumanCausedExtinction'></a>Samuel T Turvey et al. âFirst human-caused extinction of a cetacean species?â In: <span class='rm-lmri-10'>Biology Letters </span>(2007).
     <span class='rm-lmcsc-10'>doi</span>:  <a href='https://doi.org/10.1098/rsbl.2007.0292'>10.1098/rsbl.2007.0292</a>.
     </p></dd><dt class='thebibliography' id='X0-DeceptiveAlignment'>
[36]  </dt><dd class='thebibliography' id='bib-36'>
     <!-- l. 4518 --><p class='noindent'><a id='cite.0_DeceptiveAlignment'></a>Chris     van     Merwijk     et     al.     evhub.     <span class='rm-lmri-10'>Deceptive    Alignment</span>.     June     2019.     <span class='rm-lmcsc-10'>url</span>:     
<a class='url' href='https://www.lesswrong.com/posts/zthDPAjh9w6Ytbeks/deceptive-alignment'>https://www.lesswrong.com/posts/zthDPAjh9w6Ytbeks/deceptive-alignment</a>.
     </p></dd><dt class='thebibliography' id='X0-gu2024agentsmithsingleimage'>
[37]  </dt><dd class='thebibliography' id='bib-37'>
     <!-- l. 4518 --><p class='noindent'><a id='cite.0_gu2024agentsmithsingleimage'></a>Xiangming Gu et al. <span class='rm-lmri-10'>Agent Smith: A Single Image Can Jailbreak One Million Multimodal LLM Agents
     Exponentially Fast</span>. 2024. arXiv:  <a href='https://arxiv.org/abs/2402.08567'>2402.08567 <span class='rm-lmtt-10'>[cs.CL]</span></a>. <span class='rm-lmcsc-10'>url</span>: <a class='url' href='https://arxiv.org/abs/2402.08567'>https://arxiv.org/abs/2402.08567</a>.
     </p></dd><dt class='thebibliography' id='X0-HumanCompatible'>
[38]  </dt><dd class='thebibliography' id='bib-38'>
     <!-- l. 4518 --><p class='noindent'><a id='cite.0_HumanCompatible'></a>Stuart J. Russell. <span class='rm-lmri-10'>Human Compatible: Artificial Intelligence and the Problem of Control</span>. Viking, 2019.
     </p></dd><dt class='thebibliography' id='X0-ThreeLawsOfRobotics'>
[39]  </dt><dd class='thebibliography' id='bib-39'>
     <!-- l. 4518 --><p class='noindent'><a id='cite.0_ThreeLawsOfRobotics'></a>Wikipedia. <span class='rm-lmri-10'>Three Laws of Robotics</span>. <span class='rm-lmcsc-10'>url</span>: <a class='url' href='https://en.wikipedia.org/wiki/Three_Laws_of_Robotics'>https://en.wikipedia.org/wiki/Three_Laws_of_Robotics</a>.
     </p></dd><dt class='thebibliography' id='X0-ConsensusStatementOnAIRedLines'>
[40]  </dt><dd class='thebibliography' id='bib-40'>
     <!-- l. 4518 --><p class='noindent'><a id='cite.0_ConsensusStatementOnAIRedLines'></a><span class='rm-lmri-10'>Consensus    Statement    on    Red    Lines    in    Artificial    Intelligence</span>.    Mar.    2024.    <span class='rm-lmcsc-10'>url</span>:    
<a class='url' href='https://idais-beijing.baai.ac.cn/?lang=en'>https://idais-beijing.baai.ac.cn/?lang=en</a>.
     </p></dd><dt class='thebibliography' id='X0-SMARTCriteria'>
[41]  </dt><dd class='thebibliography' id='bib-41'>
     <!-- l. 4518 --><p class='noindent'><a id='cite.0_SMARTCriteria'></a>Wikipedia. <span class='rm-lmri-10'>SMART criteria</span>. <span class='rm-lmcsc-10'>url</span>: <a class='url' href='https://en.wikipedia.org/wiki/SMART_criteria'>https://en.wikipedia.org/wiki/SMART_criteria</a>.
     </p></dd><dt class='thebibliography' id='X0-packer2024memgptllmsoperatingsystems'>
[42]  </dt><dd class='thebibliography' id='bib-42'>
     <!-- l. 4518 --><p class='noindent'><a id='cite.0_packer2024memgptllmsoperatingsystems'></a>Charles Packer et al. <span class='rm-lmri-10'>MemGPT: Towards LLMs as Operating Systems</span>. 2024. arXiv:  <a href='https://arxiv.org/abs/2310.08560'>2310.08560 <span class='rm-lmtt-10'>[cs.AI]</span></a>.
     <span class='rm-lmcsc-10'>url</span>: <a class='url' href='https://arxiv.org/abs/2310.08560'>https://arxiv.org/abs/2310.08560</a>.
     </p></dd><dt class='thebibliography' id='X0-yang2024textmemory3languagemodelingexplicit'>
[43]  </dt><dd class='thebibliography' id='bib-43'>
                                                                                            
                                                                                            
     <!-- l. 4518 --><p class='noindent'><a id='cite.0_yang2024textmemory3languagemodelingexplicit'></a>Hongkang Yang et al. <span class='rm-lmri-10'>Memory</span><span class='rm-lmr-7'>3</span><span class='rm-lmri-10'>: Language Modeling with Explicit Memory</span>. 2024. arXiv:   <a href='https://arxiv.org/abs/2407.01178'>2407.01178
     <span class='rm-lmtt-10'>[cs.CL]</span></a>. <span class='rm-lmcsc-10'>url</span>: <a class='url' href='https://arxiv.org/abs/2407.01178'>https://arxiv.org/abs/2407.01178</a>.
     </p></dd><dt class='thebibliography' id='X0-GraphRAG'>
[44]  </dt><dd class='thebibliography' id='bib-44'>
     <!-- l. 4518 --><p class='noindent'><a id='cite.0_GraphRAG'></a><span class='rm-lmri-10'>GraphRAG</span>. <span class='rm-lmcsc-10'>url</span>: <a class='url' href='https://github.com/microsoft/graphrag'>https://github.com/microsoft/graphrag</a>.
     </p></dd><dt class='thebibliography' id='X0-shen2023hugginggptsolvingaitasks'>
[45]  </dt><dd class='thebibliography' id='bib-45'>
     <!-- l. 4518 --><p class='noindent'><a id='cite.0_shen2023hugginggptsolvingaitasks'></a>Yongliang Shen et al. <span class='rm-lmri-10'>HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face</span>.
     2023. arXiv:  <a href='https://arxiv.org/abs/2303.17580'>2303.17580 <span class='rm-lmtt-10'>[cs.CL]</span></a>. <span class='rm-lmcsc-10'>url</span>: <a class='url' href='https://arxiv.org/abs/2303.17580'>https://arxiv.org/abs/2303.17580</a>.
     </p></dd><dt class='thebibliography' id='X0-wei2023chainofthoughtpromptingelicitsreasoning'>
[46]  </dt><dd class='thebibliography' id='bib-46'>
     <!-- l. 4518 --><p class='noindent'><a id='cite.0_wei2023chainofthoughtpromptingelicitsreasoning'></a>Jason Wei et al. <span class='rm-lmri-10'>Chain-of-Thought Prompting Elicits Reasoning in Large Language Models</span>. 2023. arXiv:
     <a href='https://arxiv.org/abs/2201.11903'>2201.11903 <span class='rm-lmtt-10'>[cs.CL]</span></a>. <span class='rm-lmcsc-10'>url</span>: <a class='url' href='https://arxiv.org/abs/2201.11903'>https://arxiv.org/abs/2201.11903</a>.
     </p></dd><dt class='thebibliography' id='X0-kirchner2024proververifiergamesimprovelegibility'>
[47]  </dt><dd class='thebibliography' id='bib-47'>
     <!-- l. 4518 --><p class='noindent'><a id='cite.0_kirchner2024proververifiergamesimprovelegibility'></a>Jan  Hendrik  Kirchner  et  al.  <span class='rm-lmri-10'>Prover-Verifier  Games  improve  legibility  of  LLM  outputs</span>.  2024.  arXiv:
     <a href='https://arxiv.org/abs/2407.13692'>2407.13692 <span class='rm-lmtt-10'>[cs.CL]</span></a>. <span class='rm-lmcsc-10'>url</span>: <a class='url' href='https://arxiv.org/abs/2407.13692'>https://arxiv.org/abs/2407.13692</a>.
     </p></dd><dt class='thebibliography' id='X0-abdin2024phi3technicalreporthighly'>
[48]  </dt><dd class='thebibliography' id='bib-48'>
     <!-- l. 4518 --><p class='noindent'><a id='cite.0_abdin2024phi3technicalreporthighly'></a>Marah Abdin et al. <span class='rm-lmri-10'>Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone</span>.
     2024. arXiv:  <a href='https://arxiv.org/abs/2404.14219'>2404.14219 <span class='rm-lmtt-10'>[cs.CL]</span></a>. <span class='rm-lmcsc-10'>url</span>: <a class='url' href='https://arxiv.org/abs/2404.14219'>https://arxiv.org/abs/2404.14219</a>.
     </p></dd><dt class='thebibliography' id='X0-OpenAIo1Mini'>
[49]  </dt><dd class='thebibliography' id='bib-49'>
     <!-- l. 4518 --><p class='noindent'><a id='cite.0_OpenAIo1Mini'></a>OpenAI.              <span class='rm-lmri-10'>OpenAI            o1-mini</span>.              Sept.              2024.              <span class='rm-lmcsc-10'>url</span>:              
<a class='url' href='https://openai.com/index/openai-o1-mini-advancing-cost-efficient-reasoning/'>https://openai.com/index/openai-o1-mini-advancing-cost-efficient-reasoning/</a>.
     </p></dd><dt class='thebibliography' id='X0-ScalingMonosemanticity'>
[50]  </dt><dd class='thebibliography' id='bib-50'>
     <!-- l. 4518 --><p class='noindent'><a id='cite.0_ScalingMonosemanticity'></a>Adly Templeton et al. <span class='rm-lmri-10'>Scaling Monosemanticity: Extracting Interpretable Features from Claude 3 Sonnet</span>.
     May 2024. <span class='rm-lmcsc-10'>url</span>: <a class='url' href='https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html'>https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html</a>.
     </p></dd><dt class='thebibliography' id='X0-gao2024scalingevaluatingsparseautoencoders'>
[51]  </dt><dd class='thebibliography' id='bib-51'>
     <!-- l. 4518 --><p class='noindent'><a id='cite.0_gao2024scalingevaluatingsparseautoencoders'></a>Leo Gao et al. <span class='rm-lmri-10'>Scaling and evaluating sparse autoencoders</span>. 2024. arXiv:   <a href='https://arxiv.org/abs/2406.04093'>2406.04093 <span class='rm-lmtt-10'>[cs.LG]</span></a>. <span class='rm-lmcsc-10'>url</span>: 
<a class='url' href='https://arxiv.org/abs/2406.04093'>https://arxiv.org/abs/2406.04093</a>.
     </p></dd><dt class='thebibliography' id='X0-LanguageModelsCanExplainNeuronsInLanguageModels'>
[52]  </dt><dd class='thebibliography' id='bib-52'>
     <!-- l. 4518 --><p class='noindent'><a id='cite.0_LanguageModelsCanExplainNeuronsInLanguageModels'></a>Steven  Bills  et  al.  <span class='rm-lmri-10'>Language  models  can  explain  neurons  in  language  models</span>.  May  2023.  <span class='rm-lmcsc-10'>url</span>:  
<a class='url' href='https://openaipublic.blob.core.windows.net/neuron-explainer/paper/index.html'>https://openaipublic.blob.core.windows.net/neuron-explainer/paper/index.html</a>.
     </p></dd><dt class='thebibliography' id='X0-lee2024rlaifvsrlhfscaling'>
[53]  </dt><dd class='thebibliography' id='bib-53'>
     <!-- l. 4518 --><p class='noindent'><a id='cite.0_lee2024rlaifvsrlhfscaling'></a>Harrison Lee et al. <span class='rm-lmri-10'>RLAIF vs. RLHF: Scaling Reinforcement Learning from Human Feedback with AI
     Feedback</span>. 2024. arXiv:  <a href='https://arxiv.org/abs/2309.00267'>2309.00267 <span class='rm-lmtt-10'>[cs.CL]</span></a>. <span class='rm-lmcsc-10'>url</span>: <a class='url' href='https://arxiv.org/abs/2309.00267'>https://arxiv.org/abs/2309.00267</a>.
     </p></dd><dt class='thebibliography' id='X0-wichers2024gradientbasedlanguagemodelred'>
[54]  </dt><dd class='thebibliography' id='bib-54'>
     <!-- l. 4518 --><p class='noindent'><a id='cite.0_wichers2024gradientbasedlanguagemodelred'></a>Nevan Wichers et al. <span class='rm-lmri-10'>Gradient-Based Language Model Red Teaming</span>. 2024. arXiv:  <a href='https://arxiv.org/abs/2401.16656'>2401.16656 <span class='rm-lmtt-10'>[cs.CL]</span></a>.
     <span class='rm-lmcsc-10'>url</span>: <a class='url' href='https://arxiv.org/abs/2401.16656'>https://arxiv.org/abs/2401.16656</a>.
     </p></dd><dt class='thebibliography' id='X0-NeuronCoverage'>
[55]  </dt><dd class='thebibliography' id='bib-55'>
     <!-- l. 4518 --><p class='noindent'><a id='cite.0_NeuronCoverage'></a>Jing  Yu  et  al.  âA  White-Box  Testing  for  Deep  Neural  Networks  Based  on  Neuron  Coverageâ.  In:
     <span class='rm-lmri-10'>IEEE transactions on neural networks and learning systems  </span>34.11  (Nov.  2023),  pp.Â 9185â9197.  <span class='rm-lmcsc-10'>doi</span>:
     <a href='https://doi.org/10.1109/TNNLS.2022.3156620'>10.1109/TNNLS.2022.3156620</a>. <span class='rm-lmcsc-10'>url</span>: <a class='url' href='https://doi.org/10.1109/tnnls.2022.3156620'>https://doi.org/10.1109/tnnls.2022.3156620</a>.
     </p></dd><dt class='thebibliography' id='X0-dalrymple2024guaranteedsafeaiframework'>
[56]  </dt><dd class='thebibliography' id='bib-56'>
     <!-- l. 4518 --><p class='noindent'><a id='cite.0_dalrymple2024guaranteedsafeaiframework'></a>David âdavidadâ Dalrymple et al. <span class='rm-lmri-10'>Towards Guaranteed Safe AI: A Framework for Ensuring Robust and
     Reliable AI Systems</span>. 2024. arXiv:  <a href='https://arxiv.org/abs/2405.06624'>2405.06624 <span class='rm-lmtt-10'>[cs.AI]</span></a>. <span class='rm-lmcsc-10'>url</span>: <a class='url' href='https://arxiv.org/abs/2405.06624'>https://arxiv.org/abs/2405.06624</a>.
     </p></dd><dt class='thebibliography' id='X0-irving2018aisafetydebate'>
[57]  </dt><dd class='thebibliography' id='bib-57'>
                                                                                            
                                                                                            
     <!-- l. 4518 --><p class='noindent'><a id='cite.0_irving2018aisafetydebate'></a>Geoffrey   Irving   et   al.   <span class='rm-lmri-10'>AI   safety   via   debate</span>.   2018.   arXiv:      <a href='https://arxiv.org/abs/1805.00899'>1805.00899   <span class='rm-lmtt-10'>[stat.ML]</span></a>.   <span class='rm-lmcsc-10'>url</span>:   
<a class='url' href='https://arxiv.org/abs/1805.00899'>https://arxiv.org/abs/1805.00899</a>.
     </p></dd><dt class='thebibliography' id='X0-leike2018scalableagentalignmentreward'>
[58]  </dt><dd class='thebibliography' id='bib-58'>
     <!-- l. 4518 --><p class='noindent'><a id='cite.0_leike2018scalableagentalignmentreward'></a>Jan Leike et al. <span class='rm-lmri-10'>Scalable agent alignment via reward modeling: a research direction</span>. 2018. arXiv: <a href='https://arxiv.org/abs/1811.07871'>1811.07871
     <span class='rm-lmtt-10'>[cs.LG]</span></a>. <span class='rm-lmcsc-10'>url</span>: <a class='url' href='https://arxiv.org/abs/1811.07871'>https://arxiv.org/abs/1811.07871</a>.
     </p></dd><dt class='thebibliography' id='X0-SecurityDevelopmentLifecycle'>
[59]  </dt><dd class='thebibliography' id='bib-59'>
     <!-- l. 4518 --><p class='noindent'><a id='cite.0_SecurityDevelopmentLifecycle'></a>Microsoft.         <span class='rm-lmri-10'>Security        Development        Lifecycle        (SDL)        Practices</span>.         <span class='rm-lmcsc-10'>url</span>:         
<a class='url' href='https://www.microsoft.com/en-us/securityengineering/sdl/practices'>https://www.microsoft.com/en-us/securityengineering/sdl/practices</a>.
     </p></dd><dt class='thebibliography' id='X0-FormalVerification'>
[60]  </dt><dd class='thebibliography' id='bib-60'>
     <!-- l. 4518 --><p class='noindent'><a id='cite.0_FormalVerification'></a>Wikipedia. <span class='rm-lmri-10'>Formal verification</span>. <span class='rm-lmcsc-10'>url</span>: <a class='url' href='https://en.wikipedia.org/wiki/Formal_verification'>https://en.wikipedia.org/wiki/Formal_verification</a>.
     </p></dd><dt class='thebibliography' id='X0-PostQuantumCryptography'>
[61]  </dt><dd class='thebibliography' id='bib-61'>
     <!-- l. 4518 --><p class='noindent'><a id='cite.0_PostQuantumCryptography'></a>Wikipedia.                     <span class='rm-lmri-10'>Post-quantum                  cryptography</span>.                     <span class='rm-lmcsc-10'>url</span>:                     
<a class='url' href='https://en.wikipedia.org/wiki/Post-quantum_cryptography'>https://en.wikipedia.org/wiki/Post-quantum_cryptography</a>.
     </p></dd><dt class='thebibliography' id='X0-QuantumKeyDistribution'>
[62]  </dt><dd class='thebibliography' id='bib-62'>
     <!-- l. 4518 --><p class='noindent'><a id='cite.0_QuantumKeyDistribution'></a>Wikipedia. <span class='rm-lmri-10'>Quantum key distribution</span>. <span class='rm-lmcsc-10'>url</span>: <a class='url' href='https://en.wikipedia.org/wiki/Quantum_key_distribution'>https://en.wikipedia.org/wiki/Quantum_key_distribution</a>.
     </p></dd><dt class='thebibliography' id='X0-DID'>
[63]  </dt><dd class='thebibliography' id='bib-63'>
     <!-- l. 4518 --><p class='noindent'><a id='cite.0_DID'></a>W3C. <span class='rm-lmri-10'>Decentralized Identifiers (DIDs) v1.0</span>. <span class='rm-lmcsc-10'>url</span>: <a class='url' href='https://www.w3.org/TR/did-core/'>https://www.w3.org/TR/did-core/</a>.
     </p></dd><dt class='thebibliography' id='X0-XZUtilsBackdoor'>
[64]  </dt><dd class='thebibliography' id='bib-64'>
     <!-- l. 4518 --><p class='noindent'><a id='cite.0_XZUtilsBackdoor'></a>Wikipedia. <span class='rm-lmri-10'>XZ Utils backdoor</span>. <span class='rm-lmcsc-10'>url</span>: <a class='url' href='https://en.wikipedia.org/wiki/XZ_Utils_backdoor'>https://en.wikipedia.org/wiki/XZ_Utils_backdoor</a>.
     </p></dd><dt class='thebibliography' id='X0-SecureInfrastructureAI'>
[65]  </dt><dd class='thebibliography' id='bib-65'>
     <!-- l. 4518 --><p class='noindent'><a id='cite.0_SecureInfrastructureAI'></a>OpenAI.     <span class='rm-lmri-10'>Reimagining    secure    infrastructure    for    advanced    AI</span>.     May     2024.     <span class='rm-lmcsc-10'>url</span>:     
<a class='url' href='https://openai.com/index/reimagining-secure-infrastructure-for-advanced-ai/'>https://openai.com/index/reimagining-secure-infrastructure-for-advanced-ai/</a>.
     </p></dd><dt class='thebibliography' id='X0-DNANanorobots'>
[66]  </dt><dd class='thebibliography' id='bib-66'>
     <!-- l. 4518 --><p class='noindent'><a id='cite.0_DNANanorobots'></a>Feng et al. Zhou. âToward three-dimensional DNA industrial nanorobotsâ. In: <span class='rm-lmri-10'>Science robotics </span>8.85 (2023).
     <span class='rm-lmcsc-10'>doi</span>:  <a href='https://doi.org/10.1126/scirobotics.adf1274'>10.1126/scirobotics.adf1274</a>.
     </p></dd><dt class='thebibliography' id='X0-BiologicalWeaponsConvention'>
[67]  </dt><dd class='thebibliography' id='bib-67'>
     <!-- l. 4518 --><p class='noindent'><a id='cite.0_BiologicalWeaponsConvention'></a><span class='rm-lmri-10'>BiologicalWeaponsConvention</span>. <span class='rm-lmcsc-10'>url</span>: <a class='url' href='https://disarmament.unoda.org/biological-weapons/'>https://disarmament.unoda.org/biological-weapons/</a>.
     </p></dd><dt class='thebibliography' id='X0-UltravioletCLight'>
[68]  </dt><dd class='thebibliography' id='bib-68'>
     <!-- l. 4518 --><p class='noindent'><a id='cite.0_UltravioletCLight'></a>Byeong-Min Song et al. âUltraviolet-C light at 222 nm has a high disinfecting spectrum in environments
     contaminated  by  infectious  pathogens,  including  SARS-CoV-2â.  In:  <span class='rm-lmri-10'>PLoS  One  </span>18.11  (2023).  <span class='rm-lmcsc-10'>doi</span>:
     <a href='https://doi.org/10.1371/journal.pone.0294427'>10.1371/journal.pone.0294427</a>.
     </p></dd><dt class='thebibliography' id='X0-FutureFoodProductionSystems'>
[69]  </dt><dd class='thebibliography' id='bib-69'>
     <!-- l. 4518 --><p class='noindent'><a id='cite.0_FutureFoodProductionSystems'></a>Kurt         Benke         et         al.         âFuture         food-production         systems:         vertical
     farming and controlled-environment agricultureâ. In: <span class='rm-lmri-10'>Sustainability: Science, Practice and Policy </span>(2017),
     pp.Â 13â26. <span class='rm-lmcsc-10'>doi</span>:  <a href='https://doi.org/10.1080/15487733.2017.1394054'>10.1080/15487733.2017.1394054</a>.
     </p></dd><dt class='thebibliography' id='X0-CellFreeChemoenzymaticStarchSynthesis'>
[70]  </dt><dd class='thebibliography' id='bib-70'>
     <!-- l. 4518 --><p class='noindent'><a id='cite.0_CellFreeChemoenzymaticStarchSynthesis'></a>Tao Cai et al. âCell-free chemoenzymatic starch synthesis from carbon dioxideâ. In: <span class='rm-lmri-10'>Science </span>373.6562
     (2021). <span class='rm-lmcsc-10'>doi</span>:  <a href='https://doi.org/10.1126/science.abh4049'>10.1126/science.abh4049</a>.
     </p></dd><dt class='thebibliography' id='X0-ArtificialMeatIndustry'>
[71]  </dt><dd class='thebibliography' id='bib-71'>
     <!-- l. 4518 --><p class='noindent'><a id='cite.0_ArtificialMeatIndustry'></a>Tarun Mateti et al. âArtificial Meat Industry: Production Methodology, Challenges, and Futureâ. In: <span class='rm-lmri-10'>JOM</span>
     74 (2022), pp.Â 3428â3444. <span class='rm-lmcsc-10'>doi</span>:  <a href='https://doi.org/10.1007/s11837-022-05316-x'>10.1007/s11837-022-05316-x</a>.
                                                                                            
                                                                                            
     </p></dd><dt class='thebibliography' id='X0-Biosphere2'>
[72]  </dt><dd class='thebibliography' id='bib-72'>
     <!-- l. 4518 --><p class='noindent'><a id='cite.0_Biosphere2'></a>Mark Nelson et al. âUsing a Closed Ecological System to Study Earthâs Biosphere: Initial results from
     Biosphere 2â. In: <span class='rm-lmri-10'>BioScience </span>43 (1993), pp.Â 225â236. <span class='rm-lmcsc-10'>doi</span>:  <a href='https://doi.org/10.2307/1312123'>10.2307/1312123</a>.
     </p></dd><dt class='thebibliography' id='X0-ChemicalWeaponsConvention'>
[73]  </dt><dd class='thebibliography' id='bib-73'>
     <!-- l. 4518 --><p class='noindent'><a id='cite.0_ChemicalWeaponsConvention'></a><span class='rm-lmri-10'>Chemical Weapons Convention</span>. <span class='rm-lmcsc-10'>url</span>: <a class='url' href='https://www.opcw.org/chemical-weapons-convention/'>https://www.opcw.org/chemical-weapons-convention/</a>.
     </p></dd><dt class='thebibliography' id='X0-NonProliferationOfNuclearWeapons'>
[74]  </dt><dd class='thebibliography' id='bib-74'>
     <!-- l. 4518 --><p class='noindent'><a id='cite.0_NonProliferationOfNuclearWeapons'></a><span class='rm-lmri-10'>Treaty      on      the      Non-Proliferation      of      Nuclear      Weapons      (NPT)</span>.       <span class='rm-lmcsc-10'>url</span>:       
<a class='url' href='https://disarmament.unoda.org/wmd/nuclear/npt/'>https://disarmament.unoda.org/wmd/nuclear/npt/</a>.
     </p></dd><dt class='thebibliography' id='X0-USPolicyOnLethalAutonomousWeaponSystems'>
[75]  </dt><dd class='thebibliography' id='bib-75'>
     <!-- l. 4518 --><p class='noindent'><a id='cite.0_USPolicyOnLethalAutonomousWeaponSystems'></a>Congressional Research Service. <span class='rm-lmri-10'>Defense Primer:U.S. Policy on Lethal Autonomous Weapon Systems</span>. Dec.
     2020. <span class='rm-lmcsc-10'>url</span>: <a class='url' href='https://crsreports.congress.gov/product/pdf/IF/IF11150'>https://crsreports.congress.gov/product/pdf/IF/IF11150</a>.
     </p></dd><dt class='thebibliography' id='X0-EducationImproveIntelligence'>
[76]  </dt><dd class='thebibliography' id='bib-76'>
     <!-- l. 4518 --><p class='noindent'><a id='cite.0_EducationImproveIntelligence'></a>Stuart  J  Ritchie  et  al.  âHow  Much  Does  Education  Improve  Intelligence?  A  Meta-Analysisâ.  In:
     <span class='rm-lmri-10'>Psychological science </span>29.8 (2018). <span class='rm-lmcsc-10'>doi</span>:  <a href='https://doi.org/10.1177/0956797618774253'>10.1177/0956797618774253</a>.
     </p></dd><dt class='thebibliography' id='X0-SuperIntelligentHumans'>
[77]  </dt><dd class='thebibliography' id='bib-77'>
     <!-- l. 4518 --><p class='noindent'><a id='cite.0_SuperIntelligentHumans'></a>Stephen      Hsu.      <span class='rm-lmri-10'>Super-Intelligent     Humans     Are     Coming</span>.      Oct.      2014.      <span class='rm-lmcsc-10'>url</span>:      
<a class='url' href='https://nautil.us/super_intelligent-humans-are-coming-235110/'>https://nautil.us/super_intelligent-humans-are-coming-235110/</a>.
     </p></dd><dt class='thebibliography' id='X0-NeuralinkFirstHuman'>
[78]  </dt><dd class='thebibliography' id='bib-78'>
     <!-- l. 4518 --><p class='noindent'><a id='cite.0_NeuralinkFirstHuman'></a>Reuters. <span class='rm-lmri-10'>Neuralinkâs first human patient able to control mouse through thinking, Musk says</span>. Feb. 2024. <span class='rm-lmcsc-10'>url</span>:
     <a class='url' href='https://www.reuters.com/business/healthcare-pharmaceuticals/neuralinks-first-human-patient-able-control-mouse-through-thinking-musk-says-2024-02-20/'>https://www.reuters.com/business/healthcare-pharmaceuticals/neuralinks-first-human-patient-able-control-mouse-through-thinking-musk-says-2024-02-20/</a>.
     </p></dd><dt class='thebibliography' id='X0-BrainMappedInSpectacularDetail'>
[79]  </dt><dd class='thebibliography' id='bib-79'>
     <!-- l. 4518 --><p class='noindent'><a id='cite.0_BrainMappedInSpectacularDetail'></a>Carissa   Wong.   <span class='rm-lmri-10'>Cubic   millimetre   of   brain   mapped   in   spectacular   detail</span>.   May   2024.   <span class='rm-lmcsc-10'>url</span>:   
<a class='url' href='https://www.nature.com/articles/d41586-024-01387-9'>https://www.nature.com/articles/d41586-024-01387-9</a>.
     </p></dd><dt class='thebibliography' id='X0-BletchleyDeclaration'>
[80]  </dt><dd class='thebibliography' id='bib-80'>
     <!-- l. 4518 --><p class='noindent'><a id='cite.0_BletchleyDeclaration'></a><span class='rm-lmri-10'>The  Bletchley  Declaration  by  Countries  Attending  the  AI  Safety  Summit</span>.   Nov.   2023.   <span class='rm-lmcsc-10'>url</span>:   
<a class='url' href='https://www.gov.uk/government/publications/ai-safety-summit-2023-the-bletchley-declaration/the-bletchley-declaration-by-countries-attending-the-ai-safety-summit-1-2-november-2023'>https://www.gov.uk/government/publications/ai-safety-summit-2023-the-bletchley-declaration/the-bletchley-declaration-by-countries-attending-the-ai-safety-summit-1-2-november-2023</a>.
     </p></dd><dt class='thebibliography' id='X0-FrameworkConventionOnAI'>
[81]  </dt><dd class='thebibliography' id='bib-81'>
     <!-- l. 4518 --><p class='noindent'><a id='cite.0_FrameworkConventionOnAI'></a><span class='rm-lmri-10'>The         Framework         Convention         on         Artificial         Intelligence</span>.          <span class='rm-lmcsc-10'>url</span>:          
<a class='url' href='https://www.coe.int/en/web/artificial-intelligence/the-framework-convention-on-artificial-intelligence'>https://www.coe.int/en/web/artificial-intelligence/the-framework-convention-on-artificial-intelligence</a>.
     </p></dd><dt class='thebibliography' id='X0-GlobalDigitalCompact'>
[82]  </dt><dd class='thebibliography' id='bib-82'>
     <!-- l. 4518 --><p class='noindent'><a id='cite.0_GlobalDigitalCompact'></a><span class='rm-lmri-10'>Global Digital Compact</span>. Sept. 2024. <span class='rm-lmcsc-10'>url</span>: <a class='url' href='https://www.un.org/techenvoy/global-digital-compact'>https://www.un.org/techenvoy/global-digital-compact</a>.
     </p></dd><dt class='thebibliography' id='X0-FrameworkConventionOnGlobalAIChallenges'>
[83]  </dt><dd class='thebibliography' id='bib-83'>
     <!-- l. 4518 --><p class='noindent'><a id='cite.0_FrameworkConventionOnGlobalAIChallenges'></a>Duncan  Cass-Beggs  et  al.  <span class='rm-lmri-10'>Framework  Convention  on  Global  AI  Challenges</span>.  June  2024.  <span class='rm-lmcsc-10'>url</span>:  
<a class='url' href='https://www.cigionline.org/publications/framework-convention-on-global-ai-challenges/'>https://www.cigionline.org/publications/framework-convention-on-global-ai-challenges/</a>.
     </p></dd><dt class='thebibliography' id='X0-AdministrationOfGenerativeAI'>
[84]  </dt><dd class='thebibliography' id='bib-84'>
     <!-- l. 4518 --><p class='noindent'><a id='cite.0_AdministrationOfGenerativeAI'></a>Cyberspace Administration of China. <span class='rm-lmri-10'>Interim Measures for the Administration of Generative Artificial
     Intelligence Services</span>. July 2023. <span class='rm-lmcsc-10'>url</span>: <a class='url' href='https://www.cac.gov.cn/2023-07/13/c_1690898327029107.htm'>https://www.cac.gov.cn/2023-07/13/c_1690898327029107.htm</a>.
     </p></dd><dt class='thebibliography' id='X0-BidenIssuesExecutiveOrderOnAI'>
[85]  </dt><dd class='thebibliography' id='bib-85'>
     <!-- l. 4518 --><p class='noindent'><a id='cite.0_BidenIssuesExecutiveOrderOnAI'></a><span class='rm-lmri-10'>FACT  SHEET:  President  Biden  Issues  Executive  Order  on  Safe,  Secure,  and  Trustworthy  Artificial
     Intelligence</span>. Oct. 2023. <span class='rm-lmcsc-10'>url</span>: <a class='url' href='https://www.whitehouse.gov/briefing-room/statements-releases/2023/10/30/fact-sheet-president-biden-issues-executive-order-on-safe-secure-and-trustworthy-artificial-intelligence/'>https://www.whitehouse.gov/briefing-room/statements-releases/2023/10/30/fact-sheet-president-biden-issues-executive-order-on-safe-secure-and-trustworthy-artificial-intelligence/</a>.
     </p></dd><dt class='thebibliography' id='X0-EUAIAct'>
[86]  </dt><dd class='thebibliography' id='bib-86'>
     <!-- l. 4518 --><p class='noindent'><a id='cite.0_EUAIAct'></a><span class='rm-lmri-10'>The EU Artificial Intelligence Act</span>. June 2024. <span class='rm-lmcsc-10'>url</span>: <a class='url' href='https://artificialintelligenceact.eu/ai-act-explorer/'>https://artificialintelligenceact.eu/ai-act-explorer/</a>.
                                                                                            
                                                                                            
     </p></dd><dt class='thebibliography' id='X0-LTBT'>
[87]  </dt><dd class='thebibliography' id='bib-87'>
     <!-- l. 4518 --><p class='noindent'><a id='cite.0_LTBT'></a>Anthropic.         <span class='rm-lmri-10'>The        Long-Term        Benefit        Trust</span>.         Sept.         2023.         <span class='rm-lmcsc-10'>url</span>:         
<a class='url' href='https://www.anthropic.com/news/the-long-term-benefit-trust'>https://www.anthropic.com/news/the-long-term-benefit-trust</a>.
     </p></dd><dt class='thebibliography' id='X0-AnthropicResponsibleScalingPolicy'>
[88]  </dt><dd class='thebibliography' id='bib-88'>
     <!-- l. 4518 --><p class='noindent'><a id='cite.0_AnthropicResponsibleScalingPolicy'></a>Anthropic.       <span class='rm-lmri-10'>Anthropicâs      Responsible      Scaling      Policy</span>.       Sept.       2023.       <span class='rm-lmcsc-10'>url</span>:       
<a class='url' href='https://www.anthropic.com/news/anthropics-responsible-scaling-policy'>https://www.anthropic.com/news/anthropics-responsible-scaling-policy</a>.
     </p></dd><dt class='thebibliography' id='X0-OpenAIPreparednessFramework'>
[89]  </dt><dd class='thebibliography' id='bib-89'>
     <!-- l. 4518 --><p class='noindent'><a id='cite.0_OpenAIPreparednessFramework'></a>OpenAI.             <span class='rm-lmri-10'>Preparedness           Framework</span>.             Dec.             2023.             <span class='rm-lmcsc-10'>url</span>:             
<a class='url' href='https://cdn.openai.com/openai-preparedness-framework-beta.pdf'>https://cdn.openai.com/openai-preparedness-framework-beta.pdf</a>.</p></dd></dl>
                                                                                            
                                                                                            
<div class='footnotes'><a id='x1-2017x1'></a>
<!-- l. 103 --><p class='noindent'><span class='footnote-mark'><a href='#fn1x0-bk' id='fn1x0'><sup class='textsuperscript'>1</sup></a></span><span class='rm-lmr-8'>This is a general definition, but the discussion in this paper does not rely on a precise definition of AGI.</span></p><a id='x1-4002x2.1'></a>
<!-- l. 130 --><p class='noindent'><span class='footnote-mark'><a href='#fn2x0-bk' id='fn2x0'><sup class='textsuperscript'>2</sup></a></span><span class='rm-lmr-8'>From a knowledge perspective, these correspond to learning knowledge, applying knowledge, and creating knowledge,
respectively</span></p><a id='x1-4010x2.1'></a>
<!-- l. 142 --><p class='noindent'><span class='footnote-mark'><a href='#fn3x0-bk' id='fn3x0'><sup class='textsuperscript'>3</sup></a></span><span class='rm-lmr-8'>OpenAIâs o1 model has already demonstrated human-level reasoning ability, but its learning and innovation abilities are still
insufficient</span></p><a id='x1-10002x3.2.1'></a>
<!-- l. 234 --><p class='noindent'><span class='footnote-mark'><a href='#fn4x0-bk' id='fn4x0'><sup class='textsuperscript'>4</sup></a></span><span class='rm-lmr-8'>AI does not necessarily need to have autonomy, and malicious humans using AI tools that do not have autonomy to carry out harmful
actions can also be classified under this category</span></p><a id='x1-18003x4.1.2'></a>
<!-- l. 513 --><p class='noindent'><span class='footnote-mark'><a href='#fn5x0-bk' id='fn5x0'><sup class='textsuperscript'>5</sup></a></span><span class='rm-lmr-8'>AI might devise various methods to resist shutdown</span></p><a id='x1-25003x4.3'></a>
<!-- l. 654 --><p class='noindent'><span class='footnote-mark'><a href='#fn6x0-bk' id='fn6x0'><sup class='textsuperscript'>6</sup></a></span><span class='rm-lmr-8'>AI technology can reduce cost.</span></p><a id='x1-34008x7'></a>
<!-- l. 872 --><p class='noindent'><span class='footnote-mark'><a href='#fn7x0-bk' id='fn7x0'><sup class='textsuperscript'>7</sup></a></span><span class='rm-lmr-8'>External constraint means someone choosing to follow law to avoid punishment for wrongdoing, while internal constraint is
about someoneâs voluntary choice not to commit wrongful acts.</span></p><a id='x1-37002x'></a>
<!-- l. 957 --><p class='noindent'><span class='footnote-mark'><a href='#fn8x0-bk' id='fn8x0'><sup class='textsuperscript'>8</sup></a></span><span class='rm-lmr-8'>Here, the definition of legal and illegal can be regulated by each country to maintain flexibility.</span></p><a id='x1-38002x'></a>
<!-- l. 978 --><p class='noindent'><span class='footnote-mark'><a href='#fn9x0-bk' id='fn9x0'><sup class='textsuperscript'>9</sup></a></span><span class='rm-lmr-8'>Intercepting means rendering its actions ineffective</span></p><a id='x1-38011x10'></a>
<!-- l. 1021 --><p class='noindent'><span class='footnote-mark'><a href='#fn10x0-bk' id='fn10x0'><sup class='textsuperscript'>10</sup></a></span><span class='rm-lmr-8'>These rules help maintain AI independence, preventing one AI from controlling others by modifying program logic or goals, or shutting
down AIs that monitors AI or protect humans.</span></p><a id='x1-47007x6.1'></a>
<!-- l. 1239 --><p class='noindent'><span class='footnote-mark'><a href='#fn11x0-bk' id='fn11x0'><sup class='textsuperscript'>11</sup></a></span><span class='rm-lmr-8'>Human thinking operates in two modes: System 1 and System 2. System 1 is characterized by rapid intuitive judgments, whose
decision-making logic is often difficult to explain. In contrast, System 2 involves slow, deliberate thinking, with decision-making logic that is
more easily interpretable. Similar distinctions between System 1 and System 2 modes of thinking exist in AI systems. For instance, when a
LLM directly provides the answer to a mathematical problem, it resembles System 1; whereas, when an LLM employs a chain of thought to
solve a mathematical problem step by step, it resembles System 2. Although LLMs are not AGI, it is reasonable to speculate that future AGI
will also exhibit these two modes of thinking.</span></p><a id='x1-48002x6.1.1'></a>
<!-- l. 1246 --><p class='noindent'><span class='footnote-mark'><a href='#fn12x0-bk' id='fn12x0'><sup class='textsuperscript'>12</sup></a></span><span class='rm-lmr-8'>This design is inspired by research such as MemGPT[</span><a id='x1-48003'></a><a href='#cite.0_packer2024memgptllmsoperatingsystems'><span class='rm-lmr-8'>42</span></a><span class='rm-lmr-8'>], Memory</span><sup><span class='rm-lmr-6'>3</span></sup><span class='rm-lmr-8'>[</span><a id='x1-48004'></a><a href='#cite.0_yang2024textmemory3languagemodelingexplicit'><span class='rm-lmr-8'>43</span></a><span class='rm-lmr-8'>], GraphRAG[</span><a id='x1-48005'></a><a href='#cite.0_GraphRAG'><span class='rm-lmr-8'>44</span></a><span class='rm-lmr-8'>], and HuggingGPT[</span><a id='x1-48006'></a><a href='#cite.0_shen2023hugginggptsolvingaitasks'><span class='rm-lmr-8'>45</span></a><span class='rm-lmr-8'>]</span></p><a id='x1-48011x13'></a>
<!-- l. 1257 --><p class='noindent'><span class='footnote-mark'><a href='#fn13x0-bk' id='fn13x0'><sup class='textsuperscript'>13</sup></a></span><span class='rm-lmr-8'>The core model can have temporary states during a single inference process, such as KV cache, but these states cannot be
persisted. This aspect is crucial for interpretability, as it ensures we do not remain unaware of what the model has memorized.</span></p><a id='x1-48018x6.1.1'></a>
<!-- l. 1267 --><p class='noindent'><span class='footnote-mark'><a href='#fn14x0-bk' id='fn14x0'><sup class='textsuperscript'>14</sup></a></span><span class='rm-lmr-8'>This knowledge and skills may not all be correct, but that is not a concern here. We are focusing on interpretability, and issues of
correctness will be addressed in Section 6.2.2.</span></p><a id='x1-48046x15'></a>
<!-- l. 1341 --><p class='noindent'><span class='footnote-mark'><a href='#fn15x0-bk' id='fn15x0'><sup class='textsuperscript'>15</sup></a></span><span class='rm-lmr-8'>This is to ensure that the entire process does not impact the external world. The simulated results may be inaccurate, leading
to s2AGI forming incorrect cognition of the world, but this is acceptable as we are only concerned with interpretability here. The
issue of correctness will be addressed in Section 6.2.3.</span></p><a id='x1-48048x16'></a>
<!-- l. 1344 --><p class='noindent'><span class='footnote-mark'><a href='#fn16x0-bk' id='fn16x0'><sup class='textsuperscript'>16</sup></a></span><span class='rm-lmr-8'>This is to prevent uiAGI from introducing irrelevant information.</span></p><a id='x1-48050x17'></a>
<!-- l. 1347 --><p class='noindent'><span class='footnote-mark'><a href='#fn17x0-bk' id='fn17x0'><sup class='textsuperscript'>17</sup></a></span><span class='rm-lmr-8'>This approach is inspired by Jan Hendrik Kirchner et al. [</span><a id='x1-48051'></a><a href='#cite.0_kirchner2024proververifiergamesimprovelegibility'><span class='rm-lmr-8'>47</span></a><span class='rm-lmr-8'>].</span></p><a id='x1-48057x18'></a>
<!-- l. 1357 --><p class='noindent'><span class='footnote-mark'><a href='#fn18x0-bk' id='fn18x0'><sup class='textsuperscript'>18</sup></a></span><span class='rm-lmr-8'>Here, the comparison is made against the entry-level standard of AGI, not against uiAGI. The intellectual power of uiAGI
may exceed the entry-level standard of AGI, and s2AGI may perform less effectively than uiAGI. However, as long as it meets the
entry-level standard of AGI, the goal of this phase for interpretable AGI is achieved. Subsequently, we can enhance the intellectual
power of AGI through intellectual expansion, detailed in Section 6.3.</span></p><a id='x1-48059x19'></a>
<!-- l. 1357 --><p class='noindent'><span class='footnote-mark'><a href='#fn19x0-bk' id='fn19x0'><sup class='textsuperscript'>19</sup></a></span><span class='rm-lmr-8'>In addition to parameter size, we can also adjust other hyperparameters of the core model to find a model that is as conducive
as possible to subsequent interpretability work.</span></p><a id='x1-48065x20'></a>
<!-- l. 1367 --><p class='noindent'><span class='footnote-mark'><a href='#fn20x0-bk' id='fn20x0'><sup class='textsuperscript'>20</sup></a></span><span class='rm-lmr-8'>It would be unfortunate if the AGI architecture includes uninterpretable persistent states, but we should not abandon attempts;
perhaps transferring these states to an interpretable external memory could still preserve AGI capabilities</span></p><a id='x1-50018x6.2'></a>
<!-- l. 1432 --><p class='noindent'><span class='footnote-mark'><a href='#fn21x0-bk' id='fn21x0'><sup class='textsuperscript'>21</sup></a></span><span class='rm-lmr-8'>Of course, this does not imply that humans can completely let the entire process alone. Humans need to test and monitor the entire
alignment process, and if anomalies occur, it may be necessary to modify the AI Specification or the training program and retrain, much like
programmers checking their programâs operation and fixing bugs.</span></p><a id='x1-54009x22'></a>
<!-- l. 1664 --><p class='noindent'><span class='footnote-mark'><a href='#fn22x0-bk' id='fn22x0'><sup class='textsuperscript'>22</sup></a></span><span class='rm-lmr-8'>A copy-on-write strategy can be employed, where information is copied to the private space when modified, while still allowing
references to the information in the shared space</span></p><a id='x1-55002x6.3'></a>
<!-- l. 1673 --><p class='noindent'><span class='footnote-mark'><a href='#fn23x0-bk' id='fn23x0'><sup class='textsuperscript'>23</sup></a></span><span class='rm-lmr-8'>The term âscalable oversightâ is not used here because âoversightâ does not clearly distinguish between alignment
during the training process and monitoring during the deployment process. For details on the âscalable monitoringâ, see
7.1.3.</span></p><a id='x1-55008x24'></a>
<!-- l. 1682 --><p class='noindent'><span class='footnote-mark'><a href='#fn24x0-bk' id='fn24x0'><sup class='textsuperscript'>24</sup></a></span><span class='rm-lmr-8'>Due to the foundational work previously established, initial steps can be bypassed, starting from synthesizing more interpretable
CoTs.</span></p><a id='x1-57011x25'></a>
<!-- l. 1775 --><p class='noindent'><span class='footnote-mark'><a href='#fn25x0-bk' id='fn25x0'><sup class='textsuperscript'>25</sup></a></span><span class='rm-lmr-8'>In fact, if we do not wish for the AI to possess these dangerous abilities post-deployment, we can directly remove the relevant
knowledge and skills from the AIâs memory, and adopt the memory locking method described in Section 6.2.4 to prevent the AI
from acquiring these abilities. This fully demonstrates the advantage of AI interpretability.</span></p><a id='x1-57023x26'></a>
<!-- l. 1787 --><p class='noindent'><span class='footnote-mark'><a href='#fn26x0-bk' id='fn26x0'><sup class='textsuperscript'>26</sup></a></span><span class='rm-lmr-8'>Chemical, Biological, Radiological, and Nuclear</span></p><a id='x1-64013x27'></a>
<!-- l. 2021 --><p class='noindent'><span class='footnote-mark'><a href='#fn27x0-bk' id='fn27x0'><sup class='textsuperscript'>27</sup></a></span><span class='rm-lmr-8'>This approach is inspired by Geoffrey Irving et al. [</span><a id='x1-64014'></a><a href='#cite.0_irving2018aisafetydebate'><span class='rm-lmr-8'>57</span></a><span class='rm-lmr-8'>]</span></p><a id='x1-64018x28'></a>
<!-- l. 2023 --><p class='noindent'><span class='footnote-mark'><a href='#fn28x0-bk' id='fn28x0'><sup class='textsuperscript'>28</sup></a></span><span class='rm-lmr-8'>This approach is inspired by Jan Leike et al. [</span><a id='x1-64019'></a><a href='#cite.0_leike2018scalableagentalignmentreward'><span class='rm-lmr-8'>58</span></a><span class='rm-lmr-8'>]</span></p><a id='x1-68006x29'></a>
<!-- l. 2082 --><p class='noindent'><span class='footnote-mark'><a href='#fn29x0-bk' id='fn29x0'><sup class='textsuperscript'>29</sup></a></span><span class='rm-lmr-8'>This refers to human privacy information, not the AIâs own privacy.</span></p><a id='x1-68010x30'></a>
<!-- l. 2084 --><p class='noindent'><span class='footnote-mark'><a href='#fn30x0-bk' id='fn30x0'><sup class='textsuperscript'>30</sup></a></span><span class='rm-lmr-8'>For issues related to AI impersonating human identities, refer to 8.4.</span></p><a id='x1-72002x7.3.2'></a>
<!-- l. 2136 --><p class='noindent'><span class='footnote-mark'><a href='#fn31x0-bk' id='fn31x0'><sup class='textsuperscript'>31</sup></a></span><span class='rm-lmr-8'>The name comes from the science fiction novel </span><span class='rm-lmri-8'>The Three-Body Problem</span></p><a id='x1-76004x32'></a>
<!-- l. 2211 --><p class='noindent'><span class='footnote-mark'><a href='#fn32x0-bk' id='fn32x0'><sup class='textsuperscript'>32</sup></a></span><span class='rm-lmr-8'>Hackers may be humans or AIs</span></p><a id='x1-78003x8.2.1'></a>
<!-- l. 2270 --><p class='noindent'><span class='footnote-mark'><a href='#fn33x0-bk' id='fn33x0'><sup class='textsuperscript'>33</sup></a></span><span class='rm-lmr-8'>In order to prevent issues with AI itself, it is necessary to ensure that AI can only output whether to intercept and cannot
make any modifications to the requests. The following AI modules also adopt similar strategies.</span></p><a id='x1-79002x8.2.2'></a>
<!-- l. 2292 --><p class='noindent'><span class='footnote-mark'><a href='#fn34x0-bk' id='fn34x0'><sup class='textsuperscript'>34</sup></a></span><span class='rm-lmr-8'>Hardware development can also be considered using a similar approach.</span></p><a id='x1-79005x8.2.2'></a>
<!-- l. 2305 --><p class='noindent'><span class='footnote-mark'><a href='#fn35x0-bk' id='fn35x0'><sup class='textsuperscript'>35</sup></a></span><span class='rm-lmr-8'>It is necessary to review not only the code changes within the system itself but also the changes in the external open-source
code on which the system depends.</span></p><a id='x1-80006x36'></a>
<!-- l. 2333 --><p class='noindent'><span class='footnote-mark'><a href='#fn36x0-bk' id='fn36x0'><sup class='textsuperscript'>36</sup></a></span><span class='rm-lmr-8'>Hardware systems can also be considered using a similar approach.</span></p><a id='x1-80011x8.3'></a>
<!-- l. 2339 --><p class='noindent'><span class='footnote-mark'><a href='#fn37x0-bk' id='fn37x0'><sup class='textsuperscript'>37</sup></a></span><span class='rm-lmr-8'>If the technology to crack cryptographic algorithms is leaked, it may lead to significant information security accidents. However, if we
refrain from researching cracking technologies and allow malicious humans to develop them first, the consequences could be even worse.
Moreover, the later such vulnerabilities are discovered, the greater their destructive power will be. When robots are widely deployed,
exploiting such vulnerabilities could result in weapons of mass destruction.</span></p><a id='x1-84009x38'></a>
<!-- l. 2470 --><p class='noindent'><span class='footnote-mark'><a href='#fn38x0-bk' id='fn38x0'><sup class='textsuperscript'>38</sup></a></span><span class='rm-lmr-8'>Consideration is needed for discrepancies in signature results due to randomness, timing, or other environmental factors in the
software building process.</span></p><a id='x1-86002x9'></a>
<!-- l. 2570 --><p class='noindent'><span class='footnote-mark'><a href='#fn39x0-bk' id='fn39x0'><sup class='textsuperscript'>39</sup></a></span><span class='rm-lmr-8'>Malicious humans can also exploit others through deception, manipulation, and other means. Many of the preventing measures
discussed below are equally applicable to malicious humans; thus, they will not be discussed separately.</span></p><a id='x1-93009x40'></a>
<!-- l. 2825 --><p class='noindent'><span class='footnote-mark'><a href='#fn40x0-bk' id='fn40x0'><sup class='textsuperscript'>40</sup></a></span><span class='rm-lmr-8'>Of course, regulatory authorities are aware</span></p><a id='x1-94002x10'></a>
<!-- l. 2867 --><p class='noindent'><span class='footnote-mark'><a href='#fn41x0-bk' id='fn41x0'><sup class='textsuperscript'>41</sup></a></span><span class='rm-lmr-8'>Malicious humans can also illegally expand financial power. The security solutions discussed below are equally applicable to malicious
humans, so this scenario will not be discussed separately.</span></p><a id='x1-97002x11'></a>
<!-- l. 2929 --><p class='noindent'><span class='footnote-mark'><a href='#fn42x0-bk' id='fn42x0'><sup class='textsuperscript'>42</sup></a></span><span class='rm-lmr-8'>Malicious humans may also exploit AI technology to inflict military harm; preventive measures are similar and thus will not be
discussed separately.</span></p><a id='x1-104004x43'></a>
<!-- l. 3139 --><p class='noindent'><span class='footnote-mark'><a href='#fn43x0-bk' id='fn43x0'><sup class='textsuperscript'>43</sup></a></span><span class='rm-lmr-8'>As compared to the intellectual gap between animals and humans or between humans and ASI</span></p><a id='x1-108017x12.1.5'></a>
<!-- l. 3254 --><p class='noindent'><span class='footnote-mark'><a href='#fn44x0-bk' id='fn44x0'><sup class='textsuperscript'>44</sup></a></span><span class='rm-lmr-8'>The scores in the figure represent the upper limit of a particular AI categoryâs power in the respective aspect. The power of this type of
AI can reach or fall below this upper limit but cannot exceed it</span></p><a id='x1-108019x12.1.5'></a>
<!-- l. 3270 --><p class='noindent'><span class='footnote-mark'><a href='#fn45x0-bk' id='fn45x0'><sup class='textsuperscript'>45</sup></a></span><span class='rm-lmr-8'>The total power here is obtained by simply summing up various powers, and does not represent the true comprehensive power. It is only
intended to provide an intuitive comparison.</span></p><a id='x1-113010x13'></a>
<!-- l. 3465 --><p class='noindent'><span class='footnote-mark'><a href='#fn46x0-bk' id='fn46x0'><sup class='textsuperscript'>46</sup></a></span><span class='rm-lmr-8'>Do not consider establishing a form of collective decision-making system within a single AI organization to achieve power
decentralization, as this may lead to inefficient decision-making, political struggles, and collective collusion.</span></p><a id='x1-125018x47'></a>
<!-- l. 3888 --><p class='noindent'><span class='footnote-mark'><a href='#fn47x0-bk' id='fn47x0'><sup class='textsuperscript'>47</sup></a></span><span class='rm-lmr-8'>Another possibility exists: if large-scale unemployment caused by AI is not properly addressed, societal resistance against AI
applications may arise, preventing these benefits from realizing.</span></p><a id='x1-125022x48'></a>
<!-- l. 3890 --><p class='noindent'><span class='footnote-mark'><a href='#fn48x0-bk' id='fn48x0'><sup class='textsuperscript'>48</sup></a></span><span class='rm-lmr-8'>This may be the most beneficial field, but once diseases and aging are conquered, the benefits of continued accelerated
development will be small.</span></p><a id='x1-125024x49'></a>
<!-- l. 3890 --><p class='noindent'><span class='footnote-mark'><a href='#fn49x0-bk' id='fn49x0'><sup class='textsuperscript'>49</sup></a></span><span class='rm-lmr-8'>Fundamental researches may aid in advancing grand projects such as interstellar migration, but they do not significantly
contribute to improving peopleâs daily quality of life.</span></p><a id='x1-128002x14.3.1'></a>
<!-- l. 3997 --><p class='noindent'><span class='footnote-mark'><a href='#fn50x0-bk' id='fn50x0'><sup class='textsuperscript'>50</sup></a></span><span class='rm-lmr-8'>The training here is not just model training, but also includes improving AI intellectual power through </span><a href='#x1-50020x1'><span class='rm-lmr-8'>System 2
learning</span></a></p><a id='x1-130004x51'></a>
<!-- l. 4039 --><p class='noindent'><span class='footnote-mark'><a href='#fn51x0-bk' id='fn51x0'><sup class='textsuperscript'>51</sup></a></span><span class='rm-lmr-8'>Merely being allowed by robots.txt is insufficient, as the robots.txt protocol only permits crawling and does not authorize the
use of such data for AI training.</span></p><a id='x1-141007x52'></a>
<!-- l. 4277 --><p class='noindent'><span class='footnote-mark'><a href='#fn52x0-bk' id='fn52x0'><sup class='textsuperscript'>52</sup></a></span><span class='rm-lmr-8'>A reference can be made to CIGIâs </span><span class='rm-lmri-8'>Framework Convention on Global AI Challenges </span><span class='rm-lmr-8'>[</span><a id='x1-141008'></a><a href='#cite.0_FrameworkConventionOnGlobalAIChallenges'><span class='rm-lmr-8'>83</span></a><span class='rm-lmr-8'>]</span></p><a id='x1-142002x16.1.2'></a>
<!-- l. 4344 --><p class='noindent'><span class='footnote-mark'><a href='#fn53x0-bk' id='fn53x0'><sup class='textsuperscript'>53</sup></a></span><span class='rm-lmr-8'>Military organizations may opt out of international supervision, but if they do, they are considered untrustworthy entities and
cannot access advanced AI technologies and chips.</span></p><a id='x1-145006x54'></a>
<!-- l. 4409 --><p class='noindent'><span class='footnote-mark'><a href='#fn54x0-bk' id='fn54x0'><sup class='textsuperscript'>54</sup></a></span><span class='rm-lmr-8'>The term âlow-benefitâ is relative to AI technologies.</span></p><a id='x1-145008x55'></a>
<!-- l. 4409 --><p class='noindent'><span class='footnote-mark'><a href='#fn55x0-bk' id='fn55x0'><sup class='textsuperscript'>55</sup></a></span><span class='rm-lmr-8'>Once critical information systems are upgraded to post-quantum cryptography, the risk is mitigated, allowing the continued
development of quantum computing.</span></p>                                                                  </div>
                                                                                            
                                                                                            
 
</body> 
</html>